{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"De novo peptide sequencing with InstaNovo The official code repository for InstaNovo. This repo contains the code for training and inference of InstaNovo and InstaNovo+. InstaNovo is a transformer neural network with the ability to translate fragment ion peaks into the sequence of amino acids that make up the studied peptide(s). InstaNovo+, inspired by human intuition, is a multinomial diffusion model that further improves performance by iterative refinement of predicted sequences. Links: bioRxiv: https://www.biorxiv.org/content/10.1101/2023.08.30.555055v2 Developed by: InstaDeep The Department of Biotechnology and Biomedicine - Technical University of Denmark Usage Installation To use InstaNovo, we need to install the module via pip : pip install instanovo It is recommended to install InstaNovo in a fresh environment, such as Conda or PyEnv. For example, if you have conda / miniconda installed: conda env create -f environment.yml conda activate instanovo Note: InstaNovo is built for Python >= 3.8 Training To train auto-regressive InstaNovo: usage: python -m instanovo.transformer.train train_path valid_path [ -h ] [ --config CONFIG ] [ --n_gpu N_GPU ] [ --n_workers N_WORKERS ] required arguments: train_path Training data path valid_path Validation data path optional arguments: --config CONFIG file in configs folder --n_workers N_WORKERS Note: data is expected to be saved as Polars .ipc format. See section on data conversion. To update the InstaNovo model config, modify the config file under configs/instanovo/base.yaml Prediction To evaluate InstaNovo: usage: python -m instanovo.transformer.predict data_path model_path [ -h ] [ --denovo ] [ --config CONFIG ] [ --subset SUBSET ] [ --knapsack_path KNAPSACK_PATH ] [ --n_workers N_WORKERS ] required arguments: data_path Evaluation data path model_path Model checkpoint path optional arguments: --denovo evaluate in de novo mode, will not try to compute metrics --output_path OUTPUT_PATH Save predictions to a csv file ( required in de novo mode ) --subset SUBSET portion of set to evaluate --knapsack_path KNAPSACK_PATH path to pre-computed knapsack --n_workers N_WORKERS Using your own datasets To use your own datasets, you simply need to tabulate your data in either Pandas or Polars with the following schema: The dataset is tabular, where each row corresponds to a labelled MS2 spectra. sequence (string) [Optional] \\ The target peptide sequence excluding post-translational modifications modified_sequence (string) \\ The target peptide sequence including post-translational modifications precursor_mz (float64) \\ The mass-to-charge of the precursor (from MS1) charge (int64) \\ The charge of the precursor (from MS1) mz_array (list[float64]) \\ The mass-to-charge values of the MS2 spectrum mz_array (list[float32]) \\ The intensity values of the MS2 spectrum For example, the DataFrame for the nine species benchmark dataset (introduced in Tran et al. 2017 ) looks as follows: sequence modified_sequence precursor_mz precursor_charge mz_array intensity_array 0 GRVEGMEAR GRVEGMEAR 335.502 3 [102.05527 104.052956 113.07079 ...] [ 767.38837 2324.8787 598.8512 ...] 1 IGEYK IGEYK 305.165 2 [107.07023 110.071236 111.11693 ...] [ 1055.4957 2251.3171 35508.96 ...] 2 GVSREEIQR GVSREEIQR 358.528 3 [103.039444 109.59844 112.08704 ...] [801.19995 460.65268 808.3431 ...] 3 SSYHADEQVNEASK SSYHADEQVNEASK 522.234 3 [101.07095 102.0552 110.07163 ...] [ 989.45154 2332.653 1170.6191 ...] 4 DTFNTSSTSNSTSSSSSNSK DTFNTSSTSN(+.98)STSSSSSNSK 676.282 3 [119.82458 120.08073 120.2038 ...] [ 487.86942 4806.1377 516.8846 ...] For de novo prediction, the modified_sequence column is not required. We also provide a conversion script for converting to Polars IPC binary ( .ipc ): usage: python -m instanovo.utils.convert_to_ipc source target [ -h ] [ --source_type { mgf,mzml,csv }] [ --max_charge MAX_CHARGE ] [ --verbose ] positional arguments: source source file or folder target target ipc file to be saved optional arguments: -h, --help show this help message and exit --source_type { mgf,mzml,csv } type of input data --max_charge MAX_CHARGE maximum charge to filter out Note: we currently only support mzml , mgf and csv conversions. If you want to use InstaNovo for evaluating metrics, you will need to manually set the modified_sequence column after conversion. Roadmap This code repo is currently under construction. ToDo: Add data preprocessing pipeline Multi-GPU support License Code is licensed under the Apache License, Version 2.0 (see LICENSE ) The model checkpoints are licensed under Creative Commons Non-Commercial ( CC BY-NC-SA 4.0 ) BibTeX entry and citation info @article { eloff_kalogeropoulos_2023_instanovo , title = {De novo peptide sequencing with InstaNovo: Accurate, database-free peptide identification for large scale proteomics experiments} , author = {Kevin Eloff and Konstantinos Kalogeropoulos and Oliver Morell and Amandla Mabona and Jakob Berg Jespersen and Wesley Williams and Sam van Beljouw and Marcin Skwark and Andreas Hougaard Laustsen and Stan J. J. Brouns and Anne Ljungars and Erwin Marten Schoof and Jeroen Van Goey and Ulrich auf dem Keller and Karim Beguir and Nicolas Lopez Carranza and Timothy Patrick Jenkins} , year = {2023} , doi = {10.1101/2023.08.30.555055} , publisher = {Cold Spring Harbor Laboratory} , URL = {https://www.biorxiv.org/content/10.1101/2023.08.30.555055v2} , journal = {bioRxiv} }","title":"Home"},{"location":"#de-novo-peptide-sequencing-with-instanovo","text":"The official code repository for InstaNovo. This repo contains the code for training and inference of InstaNovo and InstaNovo+. InstaNovo is a transformer neural network with the ability to translate fragment ion peaks into the sequence of amino acids that make up the studied peptide(s). InstaNovo+, inspired by human intuition, is a multinomial diffusion model that further improves performance by iterative refinement of predicted sequences. Links: bioRxiv: https://www.biorxiv.org/content/10.1101/2023.08.30.555055v2 Developed by: InstaDeep The Department of Biotechnology and Biomedicine - Technical University of Denmark","title":"De novo peptide sequencing with InstaNovo"},{"location":"#usage","text":"","title":"Usage"},{"location":"#installation","text":"To use InstaNovo, we need to install the module via pip : pip install instanovo It is recommended to install InstaNovo in a fresh environment, such as Conda or PyEnv. For example, if you have conda / miniconda installed: conda env create -f environment.yml conda activate instanovo Note: InstaNovo is built for Python >= 3.8","title":"Installation"},{"location":"#training","text":"To train auto-regressive InstaNovo: usage: python -m instanovo.transformer.train train_path valid_path [ -h ] [ --config CONFIG ] [ --n_gpu N_GPU ] [ --n_workers N_WORKERS ] required arguments: train_path Training data path valid_path Validation data path optional arguments: --config CONFIG file in configs folder --n_workers N_WORKERS Note: data is expected to be saved as Polars .ipc format. See section on data conversion. To update the InstaNovo model config, modify the config file under configs/instanovo/base.yaml","title":"Training"},{"location":"#prediction","text":"To evaluate InstaNovo: usage: python -m instanovo.transformer.predict data_path model_path [ -h ] [ --denovo ] [ --config CONFIG ] [ --subset SUBSET ] [ --knapsack_path KNAPSACK_PATH ] [ --n_workers N_WORKERS ] required arguments: data_path Evaluation data path model_path Model checkpoint path optional arguments: --denovo evaluate in de novo mode, will not try to compute metrics --output_path OUTPUT_PATH Save predictions to a csv file ( required in de novo mode ) --subset SUBSET portion of set to evaluate --knapsack_path KNAPSACK_PATH path to pre-computed knapsack --n_workers N_WORKERS","title":"Prediction"},{"location":"#using-your-own-datasets","text":"To use your own datasets, you simply need to tabulate your data in either Pandas or Polars with the following schema: The dataset is tabular, where each row corresponds to a labelled MS2 spectra. sequence (string) [Optional] \\ The target peptide sequence excluding post-translational modifications modified_sequence (string) \\ The target peptide sequence including post-translational modifications precursor_mz (float64) \\ The mass-to-charge of the precursor (from MS1) charge (int64) \\ The charge of the precursor (from MS1) mz_array (list[float64]) \\ The mass-to-charge values of the MS2 spectrum mz_array (list[float32]) \\ The intensity values of the MS2 spectrum For example, the DataFrame for the nine species benchmark dataset (introduced in Tran et al. 2017 ) looks as follows: sequence modified_sequence precursor_mz precursor_charge mz_array intensity_array 0 GRVEGMEAR GRVEGMEAR 335.502 3 [102.05527 104.052956 113.07079 ...] [ 767.38837 2324.8787 598.8512 ...] 1 IGEYK IGEYK 305.165 2 [107.07023 110.071236 111.11693 ...] [ 1055.4957 2251.3171 35508.96 ...] 2 GVSREEIQR GVSREEIQR 358.528 3 [103.039444 109.59844 112.08704 ...] [801.19995 460.65268 808.3431 ...] 3 SSYHADEQVNEASK SSYHADEQVNEASK 522.234 3 [101.07095 102.0552 110.07163 ...] [ 989.45154 2332.653 1170.6191 ...] 4 DTFNTSSTSNSTSSSSSNSK DTFNTSSTSN(+.98)STSSSSSNSK 676.282 3 [119.82458 120.08073 120.2038 ...] [ 487.86942 4806.1377 516.8846 ...] For de novo prediction, the modified_sequence column is not required. We also provide a conversion script for converting to Polars IPC binary ( .ipc ): usage: python -m instanovo.utils.convert_to_ipc source target [ -h ] [ --source_type { mgf,mzml,csv }] [ --max_charge MAX_CHARGE ] [ --verbose ] positional arguments: source source file or folder target target ipc file to be saved optional arguments: -h, --help show this help message and exit --source_type { mgf,mzml,csv } type of input data --max_charge MAX_CHARGE maximum charge to filter out Note: we currently only support mzml , mgf and csv conversions. If you want to use InstaNovo for evaluating metrics, you will need to manually set the modified_sequence column after conversion.","title":"Using your own datasets"},{"location":"#roadmap","text":"This code repo is currently under construction. ToDo: Add data preprocessing pipeline Multi-GPU support","title":"Roadmap"},{"location":"#license","text":"Code is licensed under the Apache License, Version 2.0 (see LICENSE ) The model checkpoints are licensed under Creative Commons Non-Commercial ( CC BY-NC-SA 4.0 )","title":"License"},{"location":"#bibtex-entry-and-citation-info","text":"@article { eloff_kalogeropoulos_2023_instanovo , title = {De novo peptide sequencing with InstaNovo: Accurate, database-free peptide identification for large scale proteomics experiments} , author = {Kevin Eloff and Konstantinos Kalogeropoulos and Oliver Morell and Amandla Mabona and Jakob Berg Jespersen and Wesley Williams and Sam van Beljouw and Marcin Skwark and Andreas Hougaard Laustsen and Stan J. J. Brouns and Anne Ljungars and Erwin Marten Schoof and Jeroen Van Goey and Ulrich auf dem Keller and Karim Beguir and Nicolas Lopez Carranza and Timothy Patrick Jenkins} , year = {2023} , doi = {10.1101/2023.08.30.555055} , publisher = {Cold Spring Harbor Laboratory} , URL = {https://www.biorxiv.org/content/10.1101/2023.08.30.555055v2} , journal = {bioRxiv} }","title":"BibTeX entry and citation info"},{"location":"LICENSE/","text":"Apache License Version 2.0, January 2004 < http://www.apache.org/licenses/ > Terms and Conditions for use, reproduction, and distribution 1. Definitions \u201cLicense\u201d shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \u201cLicensor\u201d shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \u201cLegal Entity\u201d shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \u201ccontrol\u201d means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \u201cYou\u201d (or \u201cYour\u201d) shall mean an individual or Legal Entity exercising permissions granted by this License. \u201cSource\u201d form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \u201cObject\u201d form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \u201cWork\u201d shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \u201cDerivative Works\u201d shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \u201cContribution\u201d shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \u201csubmitted\u201d means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \u201cNot a Contribution.\u201d \u201cContributor\u201d shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. 2. Grant of Copyright License Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. 3. Grant of Patent License Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. 4. Redistribution You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \u201cNOTICE\u201d text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. 5. Submission of Contributions Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. 6. Trademarks This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. 7. Disclaimer of Warranty Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \u201cAS IS\u201d BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. 8. Limitation of Liability In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. 9. Accepting Warranty or Additional Liability While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets [] replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \u201cprinted page\u201d as the copyright notice for easier identification within third-party archives. Copyright [yyyy] [name of copyright owner] Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"LICENSE/#apache-license","text":"Version 2.0, January 2004 < http://www.apache.org/licenses/ >","title":"Apache License"},{"location":"LICENSE/#terms-and-conditions-for-use-reproduction-and-distribution","text":"","title":"Terms and Conditions for use, reproduction, and distribution"},{"location":"LICENSE/#1-definitions","text":"\u201cLicense\u201d shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \u201cLicensor\u201d shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \u201cLegal Entity\u201d shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \u201ccontrol\u201d means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \u201cYou\u201d (or \u201cYour\u201d) shall mean an individual or Legal Entity exercising permissions granted by this License. \u201cSource\u201d form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \u201cObject\u201d form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \u201cWork\u201d shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \u201cDerivative Works\u201d shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \u201cContribution\u201d shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \u201csubmitted\u201d means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \u201cNot a Contribution.\u201d \u201cContributor\u201d shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work.","title":"1. Definitions"},{"location":"LICENSE/#2-grant-of-copyright-license","text":"Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form.","title":"2. Grant of Copyright License"},{"location":"LICENSE/#3-grant-of-patent-license","text":"Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed.","title":"3. Grant of Patent License"},{"location":"LICENSE/#4-redistribution","text":"You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \u201cNOTICE\u201d text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License.","title":"4. Redistribution"},{"location":"LICENSE/#5-submission-of-contributions","text":"Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions.","title":"5. Submission of Contributions"},{"location":"LICENSE/#6-trademarks","text":"This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file.","title":"6. Trademarks"},{"location":"LICENSE/#7-disclaimer-of-warranty","text":"Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \u201cAS IS\u201d BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License.","title":"7. Disclaimer of Warranty"},{"location":"LICENSE/#8-limitation-of-liability","text":"In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages.","title":"8. Limitation of Liability"},{"location":"LICENSE/#9-accepting-warranty-or-additional-liability","text":"While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS","title":"9. Accepting Warranty or Additional Liability"},{"location":"LICENSE/#appendix-how-to-apply-the-apache-license-to-your-work","text":"To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets [] replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \u201cprinted page\u201d as the copyright notice for easier identification within third-party archives. Copyright [yyyy] [name of copyright owner] Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"APPENDIX: How to apply the Apache License to your work"},{"location":"reference/","text":"","title":"Index"},{"location":"reference/SUMMARY/","text":"instanovo constants diffusion config dataset layers model multinomial_diffusion predict inference beam_search diffusion interfaces knapsack knapsack_beam_search transformer dataset decoding layers model predict train utils convert_to_ipc metrics residues","title":"Code reference"},{"location":"reference/constants/","text":"PrecursorDimension Bases: Enum Names corresponding to indices in the precursor tensor.","title":"Constants"},{"location":"reference/constants/#instanovo.constants.PrecursorDimension","text":"Bases: Enum Names corresponding to indices in the precursor tensor.","title":"PrecursorDimension"},{"location":"reference/diffusion/","text":"","title":"Index"},{"location":"reference/diffusion/config/","text":"MassSpectrumModelConfig dataclass Bases: ModelConfig Configuration for a MassSpectrumTransfusion model.","title":"Config"},{"location":"reference/diffusion/config/#instanovo.diffusion.config.MassSpectrumModelConfig","text":"Bases: ModelConfig Configuration for a MassSpectrumTransfusion model.","title":"MassSpectrumModelConfig"},{"location":"reference/diffusion/dataset/","text":"AnnotatedPolarsSpectrumDataset ( data_frame , peptides ) Bases: PolarsSpectrumDataset A dataset with a Polars index that includes peptides from an aligned list. Source code in instanovo/diffusion/dataset.py 69 70 71 def __init__ ( self , data_frame : polars . DataFrame , peptides : list [ str ]) -> None : super () . __init__ ( data_frame ) self . peptides = peptides AnnotatedSpectrumBatch Bases: NamedTuple Represents a batch of annotated spectrum data. Attributes: Name Type Description spectra FloatTensor The tensor containing the spectra data. spectra_padding_mask BoolTensor A boolean tensor indicating the padding positions in the spectra tensor. precursors FloatTensor The tensor containing precursor mass information. peptides LongTensor The tensor containing peptide sequence information. peptide_padding_mask BoolTensor A boolean tensor indicating the padding positions in the peptides tensor. PolarsSpectrumDataset ( data_frame ) Bases: Dataset An Polars data frame index wrapper for depthcharge / casanovo datasets. Source code in instanovo/diffusion/dataset.py 48 49 def __init__ ( self , data_frame : polars . DataFrame ) -> None : self . data = data_frame SpectrumBatch Bases: NamedTuple Represents a batch of spectrum data without annotations. Attributes: Name Type Description spectra FloatTensor The tensor containing the spectra data. spectra_padding_mask BoolTensor A boolean tensor indicating the padding positions in the spectra tensor. precursors FloatTensor The tensor containing precursor mass information. collate_batches ( residues , max_length , time_steps , annotated ) Get batch collation function for given residue set, maximum length and time steps. The returned function combines spectra and precursor information for a batch into torch tensors. It also maps the residues in a peptide to their indices in residues , pads or truncates them all to max_length and returns this as a torch tensor. Parameters: Name Type Description Default residues ResidueSet The residues in the vocabulary together with their masses and index map. required max_length int The maximum peptide sequence length. All sequences are padded to this length. required time_steps int The number of diffusion time steps. required Returns: Type Description Callable [[ list [ tuple [ FloatTensor , float , int , str ]]], SpectrumBatch | AnnotatedSpectrumBatch ] Callable[ [list[tuple[torch.FloatTensor, float, int, str]]], SpectrumBatch | AnnotatedSpectrumBatch]: The function that combines examples into a batch given the parameters above. Source code in instanovo/diffusion/dataset.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def collate_batches ( residues : ResidueSet , max_length : int , time_steps : int , annotated : bool ) -> Callable [ [ list [ tuple [ torch . FloatTensor , float , int , str ]]], SpectrumBatch | AnnotatedSpectrumBatch ]: \"\"\"Get batch collation function for given residue set, maximum length and time steps. The returned function combines spectra and precursor information for a batch into `torch` tensors. It also maps the residues in a peptide to their indices in `residues`, pads or truncates them all to `max_length` and returns this as a `torch` tensor. Args: residues (ResidueSet): The residues in the vocabulary together with their masses and index map. max_length (int): The maximum peptide sequence length. All sequences are padded to this length. time_steps (int): The number of diffusion time steps. Returns: Callable[ [list[tuple[torch.FloatTensor, float, int, str]]], SpectrumBatch | AnnotatedSpectrumBatch]: The function that combines examples into a batch given the parameters above. \"\"\" def fn ( batch : list [ tuple [ torch . Tensor , float , int , str ]] ) -> SpectrumBatch | AnnotatedSpectrumBatch : if annotated : spectra , precursor_mz , precursor_charge , peptides = list ( zip ( * batch )) else : spectra , precursor_mz , precursor_charge = list ( zip ( * batch )) spectra = torch . nn . utils . rnn . pad_sequence ( spectra , batch_first = True ) spectra_padding_mask = spectra [:, :, 0 ] == 0.0 precursor_mz = torch . tensor ( precursor_mz ) precursor_charge = torch . FloatTensor ( precursor_charge ) precursor_masses = ( precursor_mz - PROTON_MASS_AMU ) * precursor_charge precursors = torch . stack ([ precursor_masses , precursor_charge , precursor_mz ], - 1 ) . float () if annotated : peptides = [ sequence if isinstance ( sequence , str ) else \"$\" for sequence in peptides ] peptides = [ sequence if len ( sequence ) > 0 else \"$\" for sequence in peptides ] peptides = torch . stack ( [ residues . encode ( residues . tokenize ( sequence )[: max_length ], pad_length = max_length ) for sequence in peptides ] ) peptide_padding_mask = peptides == residues . pad_index return AnnotatedSpectrumBatch ( spectra , spectra_padding_mask , precursors , peptides , peptide_padding_mask ) else : return SpectrumBatch ( spectra , spectra_padding_mask , precursors ) return fn","title":"Dataset"},{"location":"reference/diffusion/dataset/#instanovo.diffusion.dataset.AnnotatedPolarsSpectrumDataset","text":"Bases: PolarsSpectrumDataset A dataset with a Polars index that includes peptides from an aligned list. Source code in instanovo/diffusion/dataset.py 69 70 71 def __init__ ( self , data_frame : polars . DataFrame , peptides : list [ str ]) -> None : super () . __init__ ( data_frame ) self . peptides = peptides","title":"AnnotatedPolarsSpectrumDataset"},{"location":"reference/diffusion/dataset/#instanovo.diffusion.dataset.AnnotatedSpectrumBatch","text":"Bases: NamedTuple Represents a batch of annotated spectrum data. Attributes: Name Type Description spectra FloatTensor The tensor containing the spectra data. spectra_padding_mask BoolTensor A boolean tensor indicating the padding positions in the spectra tensor. precursors FloatTensor The tensor containing precursor mass information. peptides LongTensor The tensor containing peptide sequence information. peptide_padding_mask BoolTensor A boolean tensor indicating the padding positions in the peptides tensor.","title":"AnnotatedSpectrumBatch"},{"location":"reference/diffusion/dataset/#instanovo.diffusion.dataset.PolarsSpectrumDataset","text":"Bases: Dataset An Polars data frame index wrapper for depthcharge / casanovo datasets. Source code in instanovo/diffusion/dataset.py 48 49 def __init__ ( self , data_frame : polars . DataFrame ) -> None : self . data = data_frame","title":"PolarsSpectrumDataset"},{"location":"reference/diffusion/dataset/#instanovo.diffusion.dataset.SpectrumBatch","text":"Bases: NamedTuple Represents a batch of spectrum data without annotations. Attributes: Name Type Description spectra FloatTensor The tensor containing the spectra data. spectra_padding_mask BoolTensor A boolean tensor indicating the padding positions in the spectra tensor. precursors FloatTensor The tensor containing precursor mass information.","title":"SpectrumBatch"},{"location":"reference/diffusion/dataset/#instanovo.diffusion.dataset.collate_batches","text":"Get batch collation function for given residue set, maximum length and time steps. The returned function combines spectra and precursor information for a batch into torch tensors. It also maps the residues in a peptide to their indices in residues , pads or truncates them all to max_length and returns this as a torch tensor. Parameters: Name Type Description Default residues ResidueSet The residues in the vocabulary together with their masses and index map. required max_length int The maximum peptide sequence length. All sequences are padded to this length. required time_steps int The number of diffusion time steps. required Returns: Type Description Callable [[ list [ tuple [ FloatTensor , float , int , str ]]], SpectrumBatch | AnnotatedSpectrumBatch ] Callable[ [list[tuple[torch.FloatTensor, float, int, str]]], SpectrumBatch | AnnotatedSpectrumBatch]: The function that combines examples into a batch given the parameters above. Source code in instanovo/diffusion/dataset.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def collate_batches ( residues : ResidueSet , max_length : int , time_steps : int , annotated : bool ) -> Callable [ [ list [ tuple [ torch . FloatTensor , float , int , str ]]], SpectrumBatch | AnnotatedSpectrumBatch ]: \"\"\"Get batch collation function for given residue set, maximum length and time steps. The returned function combines spectra and precursor information for a batch into `torch` tensors. It also maps the residues in a peptide to their indices in `residues`, pads or truncates them all to `max_length` and returns this as a `torch` tensor. Args: residues (ResidueSet): The residues in the vocabulary together with their masses and index map. max_length (int): The maximum peptide sequence length. All sequences are padded to this length. time_steps (int): The number of diffusion time steps. Returns: Callable[ [list[tuple[torch.FloatTensor, float, int, str]]], SpectrumBatch | AnnotatedSpectrumBatch]: The function that combines examples into a batch given the parameters above. \"\"\" def fn ( batch : list [ tuple [ torch . Tensor , float , int , str ]] ) -> SpectrumBatch | AnnotatedSpectrumBatch : if annotated : spectra , precursor_mz , precursor_charge , peptides = list ( zip ( * batch )) else : spectra , precursor_mz , precursor_charge = list ( zip ( * batch )) spectra = torch . nn . utils . rnn . pad_sequence ( spectra , batch_first = True ) spectra_padding_mask = spectra [:, :, 0 ] == 0.0 precursor_mz = torch . tensor ( precursor_mz ) precursor_charge = torch . FloatTensor ( precursor_charge ) precursor_masses = ( precursor_mz - PROTON_MASS_AMU ) * precursor_charge precursors = torch . stack ([ precursor_masses , precursor_charge , precursor_mz ], - 1 ) . float () if annotated : peptides = [ sequence if isinstance ( sequence , str ) else \"$\" for sequence in peptides ] peptides = [ sequence if len ( sequence ) > 0 else \"$\" for sequence in peptides ] peptides = torch . stack ( [ residues . encode ( residues . tokenize ( sequence )[: max_length ], pad_length = max_length ) for sequence in peptides ] ) peptide_padding_mask = peptides == residues . pad_index return AnnotatedSpectrumBatch ( spectra , spectra_padding_mask , precursors , peptides , peptide_padding_mask ) else : return SpectrumBatch ( spectra , spectra_padding_mask , precursors ) return fn","title":"collate_batches()"},{"location":"reference/diffusion/layers/","text":"CustomPeakEncoder ( dim_model , dim_intensity = None , min_wavelength = 0.001 , max_wavelength = 10000 , partial_encode = 1.0 ) Bases: MassEncoder Encode m/z values in a mass spectrum using sine and cosine waves. Parameters dim_model : int The number of features to output. dim_intensity : int, optional The number of features to use for intensity. The remaining features will be used to encode the m/z values. min_wavelength : float, optional The minimum wavelength to use. max_wavelength : float, optional The maximum wavelength to use. Initialize the MzEncoder. Source code in instanovo/diffusion/layers.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def __init__ ( self , dim_model : int , dim_intensity : int | None = None , min_wavelength : float = 0.001 , max_wavelength : float = 10000 , partial_encode : float = 1.0 , ): \"\"\"Initialize the MzEncoder.\"\"\" self . dim_intensity = dim_intensity self . dim_model = dim_model self . dim_mz = int ( dim_model * partial_encode ) self . partial_encode = partial_encode if self . dim_intensity is not None : self . dim_mz -= self . dim_intensity super () . __init__ ( dim_model = self . dim_mz , min_wavelength = min_wavelength , max_wavelength = max_wavelength , ) self . int_encoder : nn . Module if self . dim_intensity is None : self . int_encoder = torch . nn . Linear ( 1 , dim_model , bias = False ) else : self . int_encoder = MassEncoder ( dim_model = dim_intensity , min_wavelength = 0 , max_wavelength = 1 , ) forward ( x , mass , precursor_mass ) Encode m/z values and intensities. Note that we expect intensities to fall within the interval [0, 1]. Parameters x : torch.Tensor of shape (n_spectra, n_peaks, 2) The spectra to embed. Axis 0 represents a mass spectrum, axis 1 contains the peaks in the mass spectrum, and axis 2 is essentially a 2-tuple specifying the m/z-intensity pair for each peak. These should be zero-padded, such that all of the spectra in the batch are the same length. mass : optional torch.Tensor of shape (n_spectra, ) The mass of the sequence decoded so far precursor_mass : optional torch.Tensor of shape (n_spectra, ) The mass of the parent ion Returns torch.Tensor of shape (n_spectr, n_peaks, dim_model) The encoded features for the mass spectra. Source code in instanovo/diffusion/layers.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def forward ( self , x : torch . Tensor , mass : torch . Tensor , precursor_mass : torch . Tensor ) -> torch . Tensor : \"\"\"Encode m/z values and intensities. Note that we expect intensities to fall within the interval [0, 1]. Parameters ---------- x : torch.Tensor of shape (n_spectra, n_peaks, 2) The spectra to embed. Axis 0 represents a mass spectrum, axis 1 contains the peaks in the mass spectrum, and axis 2 is essentially a 2-tuple specifying the m/z-intensity pair for each peak. These should be zero-padded, such that all of the spectra in the batch are the same length. mass : optional torch.Tensor of shape (n_spectra, ) The mass of the sequence decoded so far precursor_mass : optional torch.Tensor of shape (n_spectra, ) The mass of the parent ion Returns ------- torch.Tensor of shape (n_spectr, n_peaks, dim_model) The encoded features for the mass spectra. \"\"\" m_over_z = x [:, :, [ 0 ]] encoded = torch . zeros ( ( x . shape [ 0 ], x . shape [ 1 ], self . dim_model ), device = x . device , dtype = x . dtype ) encoded [:, :, : self . dim_mz ] = super () . forward ( m_over_z ) if mass is not None : encoded [:, :, : self . dim_mz ] += super () . forward ( m_over_z - mass [:, :, None ]) if precursor_mass is not None : encoded [:, :, : self . dim_mz ] += super () . forward ( precursor_mass [:, None , None ] - m_over_z ) if self . dim_intensity is None : intensity = self . int_encoder ( x [:, :, [ 1 ]]) return encoded + intensity intensity = self . int_encoder ( x [:, :, [ 1 ]]) return torch . cat ([ encoded , intensity ], dim = 2 ) CustomSpectrumEncoder ( dim_model = 128 , n_head = 8 , dim_feedforward = 1024 , n_layers = 1 , dropout = 0.0 , peak_encoder = True , dim_intensity = None , mass_encoding = 'linear' ) Bases: SpectrumEncoder A Transformer encoder for input mass spectra. Parameters dim_model : int, optional The latent dimensionality to represent peaks in the mass spectrum. n_head : int, optional The number of attention heads in each layer. dim_model must be divisible by n_head . dim_feedforward : int, optional The dimensionality of the fully connected layers in the Transformer layers of the model. n_layers : int, optional The number of Transformer layers. dropout : float, optional The dropout probability for all layers. peak_encoder : bool, optional Use positional encodings m/z values of each peak. dim_intensity: int or None, optional The number of features to use for encoding peak intensity. The remaining ( dim_model - dim_intensity ) are reserved for encoding the m/z value. Initialize a CustomSpectrumEncoder. Source code in instanovo/diffusion/layers.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def __init__ ( self , dim_model : int = 128 , n_head : int = 8 , dim_feedforward : int = 1024 , n_layers : int = 1 , dropout : float = 0.0 , peak_encoder : bool = True , dim_intensity : int | None = None , mass_encoding : str = \"linear\" , ): \"\"\"Initialize a CustomSpectrumEncoder.\"\"\" super () . __init__ ( dim_model , n_head , dim_feedforward , n_layers , dropout , peak_encoder , dim_intensity ) if peak_encoder and mass_encoding == \"casanovo\" : self . peak_encoder = CustomPeakEncoder ( dim_model , dim_intensity = dim_intensity ) self . linear_encoder = False else : self . peak_encoder = torch . nn . Linear ( 2 , dim_model ) self . linear_encoder = True forward ( spectra , spectra_padding_mask = None , mass = None , precursor_mass = None ) The forward pass. Parameters spectra : torch.Tensor of shape (n_spectra, n_peaks, 2) The spectra to embed. Axis 0 represents a mass spectrum, axis 1 contains the peaks in the mass spectrum, and axis 2 is essentially a 2-tuple specifying the m/z-intensity pair for each peak. These should be zero-padded, such that all of the spectra in the batch are the same length. mass : torch.Tensor of shape (n_spectra, ) The mass of the sequence decoded so far precursor_mass : torch.Tensor of shape (n_spectra, ) The mass of the parent ion Returns latent : torch.Tensor of shape (n_spectra, n_peaks + 1, dim_model) The latent representations for the spectrum and each of its peaks. mem_mask : torch.Tensor The memory mask specifying which elements were padding in X. Source code in instanovo/diffusion/layers.py 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def forward ( self , spectra : torch . Tensor , spectra_padding_mask : torch . Tensor | None = None , mass : torch . Tensor | None = None , precursor_mass : torch . Tensor | None = None , ) -> tuple [ torch . Tensor , torch . Tensor ]: \"\"\"The forward pass. Parameters ---------- spectra : torch.Tensor of shape (n_spectra, n_peaks, 2) The spectra to embed. Axis 0 represents a mass spectrum, axis 1 contains the peaks in the mass spectrum, and axis 2 is essentially a 2-tuple specifying the m/z-intensity pair for each peak. These should be zero-padded, such that all of the spectra in the batch are the same length. mass : torch.Tensor of shape (n_spectra, ) The mass of the sequence decoded so far precursor_mass : torch.Tensor of shape (n_spectra, ) The mass of the parent ion Returns ------- latent : torch.Tensor of shape (n_spectra, n_peaks + 1, dim_model) The latent representations for the spectrum and each of its peaks. mem_mask : torch.Tensor The memory mask specifying which elements were padding in X. \"\"\" zeros = ~ spectra . sum ( dim = 2 ) . bool () if spectra_padding_mask is not None : mask = spectra_padding_mask mask = torch . cat ( [ torch . tensor ([[ False ]] * spectra . shape [ 0 ]) . type_as ( zeros ), mask ], dim = 1 ) else : mask = [ torch . tensor ([[ False ]] * spectra . shape [ 0 ]) . type_as ( zeros ), zeros , ] mask = torch . cat ( mask , dim = 1 ) if not self . linear_encoder : peaks = self . peak_encoder ( spectra , mass , precursor_mass ) else : peaks = self . peak_encoder ( spectra ) # Add the spectrum representation to each input: latent_spectra = self . latent_spectrum . expand ( peaks . shape [ 0 ], - 1 , - 1 ) peaks = torch . cat ([ latent_spectra , peaks ], dim = 1 ) return self . transformer_encoder ( peaks , src_key_padding_mask = mask ), mask LocalisedEncoderLayer ( d_model , nhead , dim_feedforward = 2048 , dropout = 0.1 , activation = F . relu , layer_norm_eps = 1e-05 , batch_first = False , norm_first = False , mass_encoding = 'linear' , device = None , dtype = None ) Bases: TransformerEncoderLayer Layer in a localised transformer. Source code in instanovo/diffusion/layers.py 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 def __init__ ( self , d_model : int , nhead : int , dim_feedforward : int = 2048 , dropout : float = 0.1 , activation : str | Callable [[ Tensor ], Tensor ] = F . relu , layer_norm_eps : float = 1e-5 , batch_first : bool = False , norm_first : bool = False , mass_encoding : str = \"linear\" , device : str | None = None , dtype : torch . dtype | None = None , ) -> None : super () . __init__ ( d_model , nhead , dim_feedforward , dropout , activation , layer_norm_eps , batch_first , norm_first , device , dtype , ) self . pos_enc = None self . mass_encoding = mass_encoding forward ( src , mass = None , src_mask = None , src_key_padding_mask = None ) Compute localised transformer encoding for one layer. Parameters: Name Type Description Default src Tensor The source tensor. required mass Tensor | None Masses of the batch. Defaults to None. None src_mask Tensor | None The source self-attention mask. Defaults to None. None src_key_padding_mask Tensor | None The source padding mask. Defaults to None. None Returns: Name Type Description Tensor Tensor The encoding representation for one layer. Source code in instanovo/diffusion/layers.py 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 def forward ( self , src : Tensor , mass : Tensor | None = None , src_mask : Tensor | None = None , src_key_padding_mask : Tensor | None = None , ) -> Tensor : \"\"\"Compute localised transformer encoding for one layer. Args: src (Tensor): The source tensor. mass (Tensor | None, optional): Masses of the batch. Defaults to None. src_mask (Tensor | None, optional): The source self-attention mask. Defaults to None. src_key_padding_mask (Tensor | None, optional): The source padding mask. Defaults to None. Returns: Tensor: The encoding representation for one layer. \"\"\" x = src x = self . norm1 ( x + self . _sa_block ( x , mass , src_mask , src_key_padding_mask )) x = self . norm2 ( x + self . _ff_block ( x )) return x LocalisedEncoding ( d_model , casanovo_style = False , window_size = 100 , min_wavelength = 0.001 , device = None ) Bases: Module LocalisedEncoding module. Custom localised positional encoder. Parameters: Name Type Description Default d_model int required window_size int Defaults to 100. 100 min_wavelength float Defaults to 0.001. 0.001 Source code in instanovo/diffusion/layers.py 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 def __init__ ( self , d_model : int , casanovo_style : bool = False , window_size : int = 100 , min_wavelength : float = 0.001 , device : str | None = None , ) -> None : \"\"\"Custom localised positional encoder. Args: d_model (int): window_size (int, optional): Defaults to 100. min_wavelength (float, optional): Defaults to 0.001. \"\"\" super () . __init__ () self . min_wavelength = min_wavelength self . window_size = window_size self . d_model = d_model max_len = int (( window_size + 1 ) * 2 / min_wavelength ) # +2 to be inclusive of window bounds logging . info ( f \"Pre-computing localised encoding matrix on device= { 'cpu' if not device else device } , \\ total { ( window_size * d_model * 4 ) / min_wavelength / ( 1024 ** 2 ) : .2f } MB\" ) position = torch . arange ( max_len , device = device ) . unsqueeze ( 1 ) - max_len // 2 div_term = torch . exp ( torch . arange ( 0 , d_model , 2 , device = device ) * ( - math . log ( 10000.0 ) / d_model ) ) pe = torch . zeros ( max_len , d_model , device = device ) if casanovo_style : pe [:, : pe . shape [ 1 ] // 2 ] = torch . sin ( position * div_term , device = device ) pe [:, pe . shape [ 1 ] // 2 :] = torch . cos ( position * div_term , device = device ) else : pe [:, 0 :: 2 ] = torch . sin ( position * div_term ) pe [:, 1 :: 2 ] = torch . cos ( position * div_term ) logging . info ( f \"Pre-computing complete\" ) self . register_buffer ( \"pe\" , pe ) forward ( mass ) Defines the computation performed at every call. Parameters: Name Type Description Default mass Tensor shape [batch_size, seq_len, 1] required Returns: Type Description Tensor Tensor Source code in instanovo/diffusion/layers.py 529 530 531 532 533 534 535 536 537 538 539 540 541 def forward ( self , mass : Tensor ) -> Tensor : \"\"\"Defines the computation performed at every call. Args: mass (Tensor): shape [batch_size, seq_len, 1] Returns: Tensor \"\"\" mass = mass . clamp ( - self . window_size , self . window_size ) mass_idx = ((( mass + 1 ) + self . window_size ) / self . min_wavelength ) . to ( torch . long ) return self . pe [ mass_idx ] LocalisedSpectrumEncoder ( dim_model = 128 , n_head = 8 , dim_feedforward = 1024 , n_layers = 1 , dropout = 0 , peak_encoder = True , dim_intensity = None , window_size = 400 , device = None , mass_encoding = 'linear' ) Bases: Module A Transformer encoder for input mass spectra. Parameters dim_model : int, optional The latent dimensionality to represent peaks in the mass spectrum. n_head : int, optional The number of attention heads in each layer. dim_model must be divisible by n_head . dim_feedforward : int, optional The dimensionality of the fully connected layers in the Transformer layers of the model. n_layers : int, optional The number of Transformer layers. dropout : float, optional The dropout probability for all layers. peak_encoder : bool, optional Use positional encodings m/z values of each peak. dim_intensity: int or None, optional The number of features to use for encoding peak intensity. The remaining ( dim_model - dim_intensity ) are reserved for encoding the m/z value. Initialize a SpectrumEncoder. Source code in instanovo/diffusion/layers.py 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 def __init__ ( self , dim_model : int = 128 , n_head : int = 8 , dim_feedforward : int = 1024 , n_layers : int = 1 , dropout : int = 0 , peak_encoder : bool = True , dim_intensity : int | None = None , window_size : int = 400 , device : str | None = None , mass_encoding : str = \"linear\" , ): \"\"\"Initialize a SpectrumEncoder.\"\"\" super () . __init__ () self . latent_spectrum = torch . nn . Parameter ( torch . randn ( 1 , 1 , dim_model )) if peak_encoder and mass_encoding == \"casanovo\" : self . peak_encoder = CustomPeakEncoder ( dim_model , dim_intensity = dim_intensity , partial_encode = 0.5 , ) else : self . peak_encoder = torch . nn . Linear ( 2 , dim_model ) # The Transformer layers: layer = LocalisedEncoderLayer ( d_model = dim_model , nhead = n_head , dim_feedforward = dim_feedforward , batch_first = True , dropout = dropout , mass_encoding = mass_encoding , ) self . transformer_encoder = LocalisedTransformerEncoder ( layer , num_layers = n_layers , ) # Only add positional encoding to first layer! if mass_encoding == \"casanovo\" : self . transformer_encoder . layers [ 0 ] . pos_enc = LocalisedEncoding ( d_model = dim_model // 2 , window_size = window_size , device = device ) elif mass_encoding == \"linear\" : self . transformer_encoder . layers [ 0 ] . pos_enc = nn . Sequential ( nn . Linear ( 3 , dim_model // 2 ), ) device : str property The current device for the model. forward ( spectra , spectra_padding_mask = None ) The forward pass. Parameters spectra : torch.Tensor of shape (n_spectra, n_peaks, 2) The spectra to embed. Axis 0 represents a mass spectrum, axis 1 contains the peaks in the mass spectrum, and axis 2 is essentially a 2-tuple specifying the m/z-intensity pair for each peak. These should be zero-padded, such that all of the spectra in the batch are the same length. Returns latent : torch.Tensor of shape (n_spectra, n_peaks + 1, dim_model) The latent representations for the spectrum and each of its peaks. mem_mask : torch.Tensor The memory mask specifying which elements were padding in X. Source code in instanovo/diffusion/layers.py 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 def forward ( self , spectra : torch . Tensor , spectra_padding_mask : torch . Tensor | None = None ) -> torch . Tensor : \"\"\"The forward pass. Parameters ---------- spectra : torch.Tensor of shape (n_spectra, n_peaks, 2) The spectra to embed. Axis 0 represents a mass spectrum, axis 1 contains the peaks in the mass spectrum, and axis 2 is essentially a 2-tuple specifying the m/z-intensity pair for each peak. These should be zero-padded, such that all of the spectra in the batch are the same length. Returns ------- latent : torch.Tensor of shape (n_spectra, n_peaks + 1, dim_model) The latent representations for the spectrum and each of its peaks. mem_mask : torch.Tensor The memory mask specifying which elements were padding in X. \"\"\" zeros = ~ spectra . sum ( dim = 2 ) . bool () if spectra_padding_mask is not None : mask = spectra_padding_mask else : mask = [ torch . tensor ([[ False ]] * spectra . shape [ 0 ]) . type_as ( zeros ), zeros , ] mask = torch . cat ( mask , dim = 1 ) peaks = self . peak_encoder ( spectra , None , None ) m_over_z = spectra [:, :, 0 ] # Add the spectrum representation to each input: latent_spectra = self . latent_spectrum . expand ( peaks . shape [ 0 ], - 1 , - 1 ) latent_mz = torch . zeros (( m_over_z . shape [ 0 ], 1 ), device = spectra . device , dtype = m_over_z . dtype ) peaks = torch . cat ([ latent_spectra , peaks ], dim = 1 ) m_over_z = torch . cat ([ latent_mz , m_over_z ], dim = 1 ) return self . transformer_encoder ( peaks , mass = m_over_z , src_key_padding_mask = mask ), mask LocalisedTransformerEncoder Bases: TransformerEncoder Localised transformer encoder. forward ( src , mass = None , mask = None , src_key_padding_mask = None ) Compute representations using localised transformer. Parameters: Name Type Description Default src Tensor The source tensor. required mass Tensor | None Masses of the batch. Defaults to None. None mask Tensor | None The self-attention mask. Defaults to None. None src_key_padding_mask Tensor | None The padding mask for th source sequence. Defaults to None. None Returns: Name Type Description Tensor Tensor The encoding representation. Source code in instanovo/diffusion/layers.py 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 def forward ( self , src : Tensor , mass : Tensor | None = None , mask : Tensor | None = None , src_key_padding_mask : Tensor | None = None , ) -> Tensor : \"\"\"Compute representations using localised transformer. Args: src (Tensor): The source tensor. mass (Tensor | None, optional): Masses of the batch. Defaults to None. mask (Tensor | None, optional): The self-attention mask. Defaults to None. src_key_padding_mask (Tensor | None, optional): The padding mask for th source sequence. Defaults to None. Returns: Tensor: The encoding representation. \"\"\" src_key_padding_mask_for_layers = src_key_padding_mask output = self . layers [ 0 ]( src , mass = mass , src_mask = mask , src_key_padding_mask = src_key_padding_mask_for_layers ) for mod in self . layers [ 1 :]: output = mod ( output , mass = None , src_mask = mask , src_key_padding_mask = src_key_padding_mask_for_layers , ) return output","title":"Layers"},{"location":"reference/diffusion/layers/#instanovo.diffusion.layers.CustomPeakEncoder","text":"Bases: MassEncoder Encode m/z values in a mass spectrum using sine and cosine waves.","title":"CustomPeakEncoder"},{"location":"reference/diffusion/layers/#instanovo.diffusion.layers.CustomPeakEncoder--parameters","text":"dim_model : int The number of features to output. dim_intensity : int, optional The number of features to use for intensity. The remaining features will be used to encode the m/z values. min_wavelength : float, optional The minimum wavelength to use. max_wavelength : float, optional The maximum wavelength to use. Initialize the MzEncoder. Source code in instanovo/diffusion/layers.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def __init__ ( self , dim_model : int , dim_intensity : int | None = None , min_wavelength : float = 0.001 , max_wavelength : float = 10000 , partial_encode : float = 1.0 , ): \"\"\"Initialize the MzEncoder.\"\"\" self . dim_intensity = dim_intensity self . dim_model = dim_model self . dim_mz = int ( dim_model * partial_encode ) self . partial_encode = partial_encode if self . dim_intensity is not None : self . dim_mz -= self . dim_intensity super () . __init__ ( dim_model = self . dim_mz , min_wavelength = min_wavelength , max_wavelength = max_wavelength , ) self . int_encoder : nn . Module if self . dim_intensity is None : self . int_encoder = torch . nn . Linear ( 1 , dim_model , bias = False ) else : self . int_encoder = MassEncoder ( dim_model = dim_intensity , min_wavelength = 0 , max_wavelength = 1 , )","title":"Parameters"},{"location":"reference/diffusion/layers/#instanovo.diffusion.layers.CustomPeakEncoder.forward","text":"Encode m/z values and intensities. Note that we expect intensities to fall within the interval [0, 1].","title":"forward()"},{"location":"reference/diffusion/layers/#instanovo.diffusion.layers.CustomPeakEncoder.forward--parameters","text":"x : torch.Tensor of shape (n_spectra, n_peaks, 2) The spectra to embed. Axis 0 represents a mass spectrum, axis 1 contains the peaks in the mass spectrum, and axis 2 is essentially a 2-tuple specifying the m/z-intensity pair for each peak. These should be zero-padded, such that all of the spectra in the batch are the same length. mass : optional torch.Tensor of shape (n_spectra, ) The mass of the sequence decoded so far precursor_mass : optional torch.Tensor of shape (n_spectra, ) The mass of the parent ion","title":"Parameters"},{"location":"reference/diffusion/layers/#instanovo.diffusion.layers.CustomPeakEncoder.forward--returns","text":"torch.Tensor of shape (n_spectr, n_peaks, dim_model) The encoded features for the mass spectra. Source code in instanovo/diffusion/layers.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def forward ( self , x : torch . Tensor , mass : torch . Tensor , precursor_mass : torch . Tensor ) -> torch . Tensor : \"\"\"Encode m/z values and intensities. Note that we expect intensities to fall within the interval [0, 1]. Parameters ---------- x : torch.Tensor of shape (n_spectra, n_peaks, 2) The spectra to embed. Axis 0 represents a mass spectrum, axis 1 contains the peaks in the mass spectrum, and axis 2 is essentially a 2-tuple specifying the m/z-intensity pair for each peak. These should be zero-padded, such that all of the spectra in the batch are the same length. mass : optional torch.Tensor of shape (n_spectra, ) The mass of the sequence decoded so far precursor_mass : optional torch.Tensor of shape (n_spectra, ) The mass of the parent ion Returns ------- torch.Tensor of shape (n_spectr, n_peaks, dim_model) The encoded features for the mass spectra. \"\"\" m_over_z = x [:, :, [ 0 ]] encoded = torch . zeros ( ( x . shape [ 0 ], x . shape [ 1 ], self . dim_model ), device = x . device , dtype = x . dtype ) encoded [:, :, : self . dim_mz ] = super () . forward ( m_over_z ) if mass is not None : encoded [:, :, : self . dim_mz ] += super () . forward ( m_over_z - mass [:, :, None ]) if precursor_mass is not None : encoded [:, :, : self . dim_mz ] += super () . forward ( precursor_mass [:, None , None ] - m_over_z ) if self . dim_intensity is None : intensity = self . int_encoder ( x [:, :, [ 1 ]]) return encoded + intensity intensity = self . int_encoder ( x [:, :, [ 1 ]]) return torch . cat ([ encoded , intensity ], dim = 2 )","title":"Returns"},{"location":"reference/diffusion/layers/#instanovo.diffusion.layers.CustomSpectrumEncoder","text":"Bases: SpectrumEncoder A Transformer encoder for input mass spectra.","title":"CustomSpectrumEncoder"},{"location":"reference/diffusion/layers/#instanovo.diffusion.layers.CustomSpectrumEncoder--parameters","text":"dim_model : int, optional The latent dimensionality to represent peaks in the mass spectrum. n_head : int, optional The number of attention heads in each layer. dim_model must be divisible by n_head . dim_feedforward : int, optional The dimensionality of the fully connected layers in the Transformer layers of the model. n_layers : int, optional The number of Transformer layers. dropout : float, optional The dropout probability for all layers. peak_encoder : bool, optional Use positional encodings m/z values of each peak. dim_intensity: int or None, optional The number of features to use for encoding peak intensity. The remaining ( dim_model - dim_intensity ) are reserved for encoding the m/z value. Initialize a CustomSpectrumEncoder. Source code in instanovo/diffusion/layers.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def __init__ ( self , dim_model : int = 128 , n_head : int = 8 , dim_feedforward : int = 1024 , n_layers : int = 1 , dropout : float = 0.0 , peak_encoder : bool = True , dim_intensity : int | None = None , mass_encoding : str = \"linear\" , ): \"\"\"Initialize a CustomSpectrumEncoder.\"\"\" super () . __init__ ( dim_model , n_head , dim_feedforward , n_layers , dropout , peak_encoder , dim_intensity ) if peak_encoder and mass_encoding == \"casanovo\" : self . peak_encoder = CustomPeakEncoder ( dim_model , dim_intensity = dim_intensity ) self . linear_encoder = False else : self . peak_encoder = torch . nn . Linear ( 2 , dim_model ) self . linear_encoder = True","title":"Parameters"},{"location":"reference/diffusion/layers/#instanovo.diffusion.layers.CustomSpectrumEncoder.forward","text":"The forward pass.","title":"forward()"},{"location":"reference/diffusion/layers/#instanovo.diffusion.layers.CustomSpectrumEncoder.forward--parameters","text":"spectra : torch.Tensor of shape (n_spectra, n_peaks, 2) The spectra to embed. Axis 0 represents a mass spectrum, axis 1 contains the peaks in the mass spectrum, and axis 2 is essentially a 2-tuple specifying the m/z-intensity pair for each peak. These should be zero-padded, such that all of the spectra in the batch are the same length. mass : torch.Tensor of shape (n_spectra, ) The mass of the sequence decoded so far precursor_mass : torch.Tensor of shape (n_spectra, ) The mass of the parent ion","title":"Parameters"},{"location":"reference/diffusion/layers/#instanovo.diffusion.layers.CustomSpectrumEncoder.forward--returns","text":"latent : torch.Tensor of shape (n_spectra, n_peaks + 1, dim_model) The latent representations for the spectrum and each of its peaks. mem_mask : torch.Tensor The memory mask specifying which elements were padding in X. Source code in instanovo/diffusion/layers.py 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def forward ( self , spectra : torch . Tensor , spectra_padding_mask : torch . Tensor | None = None , mass : torch . Tensor | None = None , precursor_mass : torch . Tensor | None = None , ) -> tuple [ torch . Tensor , torch . Tensor ]: \"\"\"The forward pass. Parameters ---------- spectra : torch.Tensor of shape (n_spectra, n_peaks, 2) The spectra to embed. Axis 0 represents a mass spectrum, axis 1 contains the peaks in the mass spectrum, and axis 2 is essentially a 2-tuple specifying the m/z-intensity pair for each peak. These should be zero-padded, such that all of the spectra in the batch are the same length. mass : torch.Tensor of shape (n_spectra, ) The mass of the sequence decoded so far precursor_mass : torch.Tensor of shape (n_spectra, ) The mass of the parent ion Returns ------- latent : torch.Tensor of shape (n_spectra, n_peaks + 1, dim_model) The latent representations for the spectrum and each of its peaks. mem_mask : torch.Tensor The memory mask specifying which elements were padding in X. \"\"\" zeros = ~ spectra . sum ( dim = 2 ) . bool () if spectra_padding_mask is not None : mask = spectra_padding_mask mask = torch . cat ( [ torch . tensor ([[ False ]] * spectra . shape [ 0 ]) . type_as ( zeros ), mask ], dim = 1 ) else : mask = [ torch . tensor ([[ False ]] * spectra . shape [ 0 ]) . type_as ( zeros ), zeros , ] mask = torch . cat ( mask , dim = 1 ) if not self . linear_encoder : peaks = self . peak_encoder ( spectra , mass , precursor_mass ) else : peaks = self . peak_encoder ( spectra ) # Add the spectrum representation to each input: latent_spectra = self . latent_spectrum . expand ( peaks . shape [ 0 ], - 1 , - 1 ) peaks = torch . cat ([ latent_spectra , peaks ], dim = 1 ) return self . transformer_encoder ( peaks , src_key_padding_mask = mask ), mask","title":"Returns"},{"location":"reference/diffusion/layers/#instanovo.diffusion.layers.LocalisedEncoderLayer","text":"Bases: TransformerEncoderLayer Layer in a localised transformer. Source code in instanovo/diffusion/layers.py 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 def __init__ ( self , d_model : int , nhead : int , dim_feedforward : int = 2048 , dropout : float = 0.1 , activation : str | Callable [[ Tensor ], Tensor ] = F . relu , layer_norm_eps : float = 1e-5 , batch_first : bool = False , norm_first : bool = False , mass_encoding : str = \"linear\" , device : str | None = None , dtype : torch . dtype | None = None , ) -> None : super () . __init__ ( d_model , nhead , dim_feedforward , dropout , activation , layer_norm_eps , batch_first , norm_first , device , dtype , ) self . pos_enc = None self . mass_encoding = mass_encoding","title":"LocalisedEncoderLayer"},{"location":"reference/diffusion/layers/#instanovo.diffusion.layers.LocalisedEncoderLayer.forward","text":"Compute localised transformer encoding for one layer. Parameters: Name Type Description Default src Tensor The source tensor. required mass Tensor | None Masses of the batch. Defaults to None. None src_mask Tensor | None The source self-attention mask. Defaults to None. None src_key_padding_mask Tensor | None The source padding mask. Defaults to None. None Returns: Name Type Description Tensor Tensor The encoding representation for one layer. Source code in instanovo/diffusion/layers.py 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 def forward ( self , src : Tensor , mass : Tensor | None = None , src_mask : Tensor | None = None , src_key_padding_mask : Tensor | None = None , ) -> Tensor : \"\"\"Compute localised transformer encoding for one layer. Args: src (Tensor): The source tensor. mass (Tensor | None, optional): Masses of the batch. Defaults to None. src_mask (Tensor | None, optional): The source self-attention mask. Defaults to None. src_key_padding_mask (Tensor | None, optional): The source padding mask. Defaults to None. Returns: Tensor: The encoding representation for one layer. \"\"\" x = src x = self . norm1 ( x + self . _sa_block ( x , mass , src_mask , src_key_padding_mask )) x = self . norm2 ( x + self . _ff_block ( x )) return x","title":"forward()"},{"location":"reference/diffusion/layers/#instanovo.diffusion.layers.LocalisedEncoding","text":"Bases: Module LocalisedEncoding module. Custom localised positional encoder. Parameters: Name Type Description Default d_model int required window_size int Defaults to 100. 100 min_wavelength float Defaults to 0.001. 0.001 Source code in instanovo/diffusion/layers.py 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 def __init__ ( self , d_model : int , casanovo_style : bool = False , window_size : int = 100 , min_wavelength : float = 0.001 , device : str | None = None , ) -> None : \"\"\"Custom localised positional encoder. Args: d_model (int): window_size (int, optional): Defaults to 100. min_wavelength (float, optional): Defaults to 0.001. \"\"\" super () . __init__ () self . min_wavelength = min_wavelength self . window_size = window_size self . d_model = d_model max_len = int (( window_size + 1 ) * 2 / min_wavelength ) # +2 to be inclusive of window bounds logging . info ( f \"Pre-computing localised encoding matrix on device= { 'cpu' if not device else device } , \\ total { ( window_size * d_model * 4 ) / min_wavelength / ( 1024 ** 2 ) : .2f } MB\" ) position = torch . arange ( max_len , device = device ) . unsqueeze ( 1 ) - max_len // 2 div_term = torch . exp ( torch . arange ( 0 , d_model , 2 , device = device ) * ( - math . log ( 10000.0 ) / d_model ) ) pe = torch . zeros ( max_len , d_model , device = device ) if casanovo_style : pe [:, : pe . shape [ 1 ] // 2 ] = torch . sin ( position * div_term , device = device ) pe [:, pe . shape [ 1 ] // 2 :] = torch . cos ( position * div_term , device = device ) else : pe [:, 0 :: 2 ] = torch . sin ( position * div_term ) pe [:, 1 :: 2 ] = torch . cos ( position * div_term ) logging . info ( f \"Pre-computing complete\" ) self . register_buffer ( \"pe\" , pe )","title":"LocalisedEncoding"},{"location":"reference/diffusion/layers/#instanovo.diffusion.layers.LocalisedEncoding.forward","text":"Defines the computation performed at every call. Parameters: Name Type Description Default mass Tensor shape [batch_size, seq_len, 1] required Returns: Type Description Tensor Tensor Source code in instanovo/diffusion/layers.py 529 530 531 532 533 534 535 536 537 538 539 540 541 def forward ( self , mass : Tensor ) -> Tensor : \"\"\"Defines the computation performed at every call. Args: mass (Tensor): shape [batch_size, seq_len, 1] Returns: Tensor \"\"\" mass = mass . clamp ( - self . window_size , self . window_size ) mass_idx = ((( mass + 1 ) + self . window_size ) / self . min_wavelength ) . to ( torch . long ) return self . pe [ mass_idx ]","title":"forward()"},{"location":"reference/diffusion/layers/#instanovo.diffusion.layers.LocalisedSpectrumEncoder","text":"Bases: Module A Transformer encoder for input mass spectra.","title":"LocalisedSpectrumEncoder"},{"location":"reference/diffusion/layers/#instanovo.diffusion.layers.LocalisedSpectrumEncoder--parameters","text":"dim_model : int, optional The latent dimensionality to represent peaks in the mass spectrum. n_head : int, optional The number of attention heads in each layer. dim_model must be divisible by n_head . dim_feedforward : int, optional The dimensionality of the fully connected layers in the Transformer layers of the model. n_layers : int, optional The number of Transformer layers. dropout : float, optional The dropout probability for all layers. peak_encoder : bool, optional Use positional encodings m/z values of each peak. dim_intensity: int or None, optional The number of features to use for encoding peak intensity. The remaining ( dim_model - dim_intensity ) are reserved for encoding the m/z value. Initialize a SpectrumEncoder. Source code in instanovo/diffusion/layers.py 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 def __init__ ( self , dim_model : int = 128 , n_head : int = 8 , dim_feedforward : int = 1024 , n_layers : int = 1 , dropout : int = 0 , peak_encoder : bool = True , dim_intensity : int | None = None , window_size : int = 400 , device : str | None = None , mass_encoding : str = \"linear\" , ): \"\"\"Initialize a SpectrumEncoder.\"\"\" super () . __init__ () self . latent_spectrum = torch . nn . Parameter ( torch . randn ( 1 , 1 , dim_model )) if peak_encoder and mass_encoding == \"casanovo\" : self . peak_encoder = CustomPeakEncoder ( dim_model , dim_intensity = dim_intensity , partial_encode = 0.5 , ) else : self . peak_encoder = torch . nn . Linear ( 2 , dim_model ) # The Transformer layers: layer = LocalisedEncoderLayer ( d_model = dim_model , nhead = n_head , dim_feedforward = dim_feedforward , batch_first = True , dropout = dropout , mass_encoding = mass_encoding , ) self . transformer_encoder = LocalisedTransformerEncoder ( layer , num_layers = n_layers , ) # Only add positional encoding to first layer! if mass_encoding == \"casanovo\" : self . transformer_encoder . layers [ 0 ] . pos_enc = LocalisedEncoding ( d_model = dim_model // 2 , window_size = window_size , device = device ) elif mass_encoding == \"linear\" : self . transformer_encoder . layers [ 0 ] . pos_enc = nn . Sequential ( nn . Linear ( 3 , dim_model // 2 ), )","title":"Parameters"},{"location":"reference/diffusion/layers/#instanovo.diffusion.layers.LocalisedSpectrumEncoder.device","text":"The current device for the model.","title":"device"},{"location":"reference/diffusion/layers/#instanovo.diffusion.layers.LocalisedSpectrumEncoder.forward","text":"The forward pass.","title":"forward()"},{"location":"reference/diffusion/layers/#instanovo.diffusion.layers.LocalisedSpectrumEncoder.forward--parameters","text":"spectra : torch.Tensor of shape (n_spectra, n_peaks, 2) The spectra to embed. Axis 0 represents a mass spectrum, axis 1 contains the peaks in the mass spectrum, and axis 2 is essentially a 2-tuple specifying the m/z-intensity pair for each peak. These should be zero-padded, such that all of the spectra in the batch are the same length.","title":"Parameters"},{"location":"reference/diffusion/layers/#instanovo.diffusion.layers.LocalisedSpectrumEncoder.forward--returns","text":"latent : torch.Tensor of shape (n_spectra, n_peaks + 1, dim_model) The latent representations for the spectrum and each of its peaks. mem_mask : torch.Tensor The memory mask specifying which elements were padding in X. Source code in instanovo/diffusion/layers.py 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 def forward ( self , spectra : torch . Tensor , spectra_padding_mask : torch . Tensor | None = None ) -> torch . Tensor : \"\"\"The forward pass. Parameters ---------- spectra : torch.Tensor of shape (n_spectra, n_peaks, 2) The spectra to embed. Axis 0 represents a mass spectrum, axis 1 contains the peaks in the mass spectrum, and axis 2 is essentially a 2-tuple specifying the m/z-intensity pair for each peak. These should be zero-padded, such that all of the spectra in the batch are the same length. Returns ------- latent : torch.Tensor of shape (n_spectra, n_peaks + 1, dim_model) The latent representations for the spectrum and each of its peaks. mem_mask : torch.Tensor The memory mask specifying which elements were padding in X. \"\"\" zeros = ~ spectra . sum ( dim = 2 ) . bool () if spectra_padding_mask is not None : mask = spectra_padding_mask else : mask = [ torch . tensor ([[ False ]] * spectra . shape [ 0 ]) . type_as ( zeros ), zeros , ] mask = torch . cat ( mask , dim = 1 ) peaks = self . peak_encoder ( spectra , None , None ) m_over_z = spectra [:, :, 0 ] # Add the spectrum representation to each input: latent_spectra = self . latent_spectrum . expand ( peaks . shape [ 0 ], - 1 , - 1 ) latent_mz = torch . zeros (( m_over_z . shape [ 0 ], 1 ), device = spectra . device , dtype = m_over_z . dtype ) peaks = torch . cat ([ latent_spectra , peaks ], dim = 1 ) m_over_z = torch . cat ([ latent_mz , m_over_z ], dim = 1 ) return self . transformer_encoder ( peaks , mass = m_over_z , src_key_padding_mask = mask ), mask","title":"Returns"},{"location":"reference/diffusion/layers/#instanovo.diffusion.layers.LocalisedTransformerEncoder","text":"Bases: TransformerEncoder Localised transformer encoder.","title":"LocalisedTransformerEncoder"},{"location":"reference/diffusion/layers/#instanovo.diffusion.layers.LocalisedTransformerEncoder.forward","text":"Compute representations using localised transformer. Parameters: Name Type Description Default src Tensor The source tensor. required mass Tensor | None Masses of the batch. Defaults to None. None mask Tensor | None The self-attention mask. Defaults to None. None src_key_padding_mask Tensor | None The padding mask for th source sequence. Defaults to None. None Returns: Name Type Description Tensor Tensor The encoding representation. Source code in instanovo/diffusion/layers.py 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 def forward ( self , src : Tensor , mass : Tensor | None = None , mask : Tensor | None = None , src_key_padding_mask : Tensor | None = None , ) -> Tensor : \"\"\"Compute representations using localised transformer. Args: src (Tensor): The source tensor. mass (Tensor | None, optional): Masses of the batch. Defaults to None. mask (Tensor | None, optional): The self-attention mask. Defaults to None. src_key_padding_mask (Tensor | None, optional): The padding mask for th source sequence. Defaults to None. Returns: Tensor: The encoding representation. \"\"\" src_key_padding_mask_for_layers = src_key_padding_mask output = self . layers [ 0 ]( src , mass = mass , src_mask = mask , src_key_padding_mask = src_key_padding_mask_for_layers ) for mod in self . layers [ 1 :]: output = mod ( output , mass = None , src_mask = mask , src_key_padding_mask = src_key_padding_mask_for_layers , ) return output","title":"forward()"},{"location":"reference/diffusion/model/","text":"MassSpectrumTransFusion ( cfg , max_transcript_len = 200 ) Bases: TransFusion Diffusion reconstruction model conditioned on mass spectra. Source code in instanovo/diffusion/model.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def __init__ ( self , cfg : MassSpectrumModelConfig , max_transcript_len : int = 200 ) -> None : super () . __init__ ( cfg , max_transcript_len ) layers = [] for i in range ( cfg . layers ): add_cond_cross_attn = i in list ( self . cfg . cond_cross_attn_layers ) layer = MassSpectrumTransformer ( self . cfg . dim , self . cfg . t_emb_dim , self . cfg . cond_emb_dim , self . cfg . nheads , add_cond_seq = add_cond_cross_attn , dropout = self . cfg . dropout , use_wavlm_attn = cfg . attention_type == \"wavlm\" and not add_cond_cross_attn , wavlm_num_bucket = cfg . wavlm_num_bucket , wavlm_max_dist = cfg . wavlm_max_dist , has_rel_attn_bias = ( cfg . attention_type == \"wavlm\" and i == 1 ), ) # add relative attn bias at i=1 as that is first attn where we do not use # cross attention. layers . append ( layer ) self . layers = nn . ModuleList ( layers ) self . conditioning_pos_emb = None if cfg . localised_attn : self . encoder = LocalisedSpectrumEncoder ( dim_model = cfg . dim , n_head = cfg . nheads , dim_feedforward = cfg . dim , n_layers = cfg . layers , dropout = cfg . dropout , window_size = cfg . window_size , mass_encoding = cfg . mass_encoding , ) else : self . encoder = CustomSpectrumEncoder ( dim_model = cfg . dim , n_head = cfg . nheads , dim_feedforward = cfg . dim_feedforward , n_layers = cfg . layers , dropout = cfg . dropout , mass_encoding = cfg . mass_encoding , ) # precursor embedding self . charge_encoder = torch . nn . Embedding ( cfg . max_charge , cfg . dim ) self . mass_encoder = MassEncoder ( cfg . dim ) self . cache_spectra = None self . cache_cond_emb = None self . cache_cond_padding_mask = None forward ( x , t , spectra , spectra_padding_mask , precursors , x_padding_mask = None ) Transformer with conditioning cross attention. x : (bs, seq_len) long tensor of character indices or (bs, seq_len, vocab_size) if cfg.diffusion_type == 'continuous' t : (bs, ) long tensor of timestep indices cond_emb : (bs, seq_len2, cond_emb_dim) if using wavlm encoder, else (bs, T) x_padding_mask : (bs, seq_len) if using wavlm encoder, else (bs, T) cond_padding_mask : (bs, seq_len2) Returns logits (bs, seq_len, vocab_size) Source code in instanovo/diffusion/model.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 def forward ( self , x : Tensor , t : Tensor , spectra : Tensor , spectra_padding_mask : Tensor , precursors : Tensor , x_padding_mask : Tensor | None = None , ) -> Tensor : \"\"\"Transformer with conditioning cross attention. - `x`: (bs, seq_len) long tensor of character indices or (bs, seq_len, vocab_size) if cfg.diffusion_type == 'continuous' - `t`: (bs, ) long tensor of timestep indices - `cond_emb`: (bs, seq_len2, cond_emb_dim) if using wavlm encoder, else (bs, T) - `x_padding_mask`: (bs, seq_len) if using wavlm encoder, else (bs, T) - `cond_padding_mask`: (bs, seq_len2) Returns logits (bs, seq_len, vocab_size) \"\"\" # 1. Base: character, timestep embeddings and zeroing bs = x . shape [ 0 ] x = self . char_embedding ( x ) # (bs, seq_len, dim) if self . cfg . pos_encoding == \"relative\" : x = self . pos_embedding ( x ) else : pos_emb = self . pos_embedding . weight [ None ] . expand ( bs , - 1 , - 1 ) # (seq_len, dim) --> (bs, seq_len, dim) x = x + pos_emb t_emb = timestep_embedding ( t , self . cfg . t_emb_dim , self . cfg . t_emb_max_period , dtype = spectra . dtype ) # (bs, t_dim) # 2. Classifier-free guidance: with prob cfg.drop_cond_prob, zero out and drop conditional probability if self . training : zero_cond_inds = torch . rand_like ( t , dtype = spectra . dtype ) < self . cfg . drop_cond_prob else : # never randomly zero when in eval mode zero_cond_inds = torch . zeros_like ( t , dtype = torch . bool ) if spectra_padding_mask . all (): # BUT, if all cond information is padded then we are obviously doing unconditional synthesis, # so, force zero_cond_inds to be all ones zero_cond_inds = ~ zero_cond_inds # 3. DENOVO calculate spectrum embedding here if self . training : cond_emb , cond_padding_mask = self . encoder ( spectra , spectra_padding_mask ) else : if self . cache_spectra is not None and torch . equal ( self . cache_spectra , spectra ): cond_emb , cond_padding_mask = self . cache_cond_emb , self . cache_cond_padding_mask else : cond_emb , cond_padding_mask = self . encoder ( spectra , spectra_padding_mask ) self . cache_spectra = spectra self . cache_cond_emb = cond_emb self . cache_cond_padding_mask = cond_padding_mask # set mask for these conditional entries to true everywhere (i.e. mask them out) masses = self . mass_encoder ( precursors [:, None , [ 0 ]]) charges = self . charge_encoder ( precursors [:, 1 ] . int () - 1 ) precursor_emb = masses + charges [:, None , :] cond_padding_mask [ zero_cond_inds ] = True cond_emb [ zero_cond_inds ] = 0 # 4. Iterate through layers pos_bias = None for layer in self . layers : x , pos_bias = layer ( x , t_emb , precursor_emb , cond_emb , x_padding_mask , cond_padding_mask , pos_bias = pos_bias , ) # 5. Pass through head to get logits x = self . head ( x ) # (bs, seq_len, vocab size) return x MassSpectrumTransformer Bases: Pogfuse A transformer model specialised for encoding mass spectra. forward ( x , t_emb , precursor_emb , cond_emb = None , x_padding_mask = None , cond_padding_mask = None , pos_bias = None ) Compute encodings with the model. Forward with x (bs, seq_len, dim), summing t_emb (bs, dim) before the transformer layer, and appending conditioning_emb (bs, seq_len2, dim) to the key/value pairs of the attention. Also pooled_conv_emb (bs, dim) is summed with the timestep embeddings Optionally specify key/value padding for input x with x_padding_mask (bs, seq_len), and optionally specify key/value padding mask for conditional embedding with cond_padding_mask (bs, seq_len2). By default no padding is used. Good idea to use cond padding but not x padding. pos_bias is positional bias for wavlm-style attention gated relative position bias. Returns x of same shape (bs, seq_len, dim) Source code in instanovo/diffusion/model.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def forward ( self , x : Tensor , t_emb : Tensor , precursor_emb : Tensor , cond_emb : Tensor | None = None , x_padding_mask : Tensor | None = None , cond_padding_mask : Tensor | None = None , pos_bias : Tensor | None = None , ) -> Tensor : \"\"\"Compute encodings with the model. Forward with `x` (bs, seq_len, dim), summing `t_emb` (bs, dim) before the transformer layer, and appending `conditioning_emb` (bs, seq_len2, dim) to the key/value pairs of the attention. Also `pooled_conv_emb` (bs, dim) is summed with the timestep embeddings Optionally specify key/value padding for input `x` with `x_padding_mask` (bs, seq_len), and optionally specify key/value padding mask for conditional embedding with `cond_padding_mask` (bs, seq_len2). By default no padding is used. Good idea to use cond padding but not x padding. `pos_bias` is positional bias for wavlm-style attention gated relative position bias. Returns `x` of same shape (bs, seq_len, dim) \"\"\" # ----------------------- # 1. Get and add timestep embedding t = self . t_layers ( t_emb )[:, None ] # (bs, 1, dim) p = self . cond_pooled_layers ( precursor_emb ) # (bs, 1, dim) x += t + p # (bs, seq_len, dim) # ----------------------- # 2. Get and append conditioning embeddings if self . add_cond_seq : c = self . cond_layers ( cond_emb ) # (bs, seq_len2, dim) else : c = None # ----------------------- # 3. Do transformer layer # -- Self-attention block x1 , pos_bias = self . _sa_block ( x , c , x_padding_mask = x_padding_mask , c_padding_mask = cond_padding_mask , pos_bias = pos_bias ) # -- Layer-norm with residual connection x = self . norm1 ( x + x1 ) # -- Layer-norm with feedfoward block and residual connection x = self . norm2 ( x + self . _ff_block ( x )) return x , pos_bias","title":"Model"},{"location":"reference/diffusion/model/#instanovo.diffusion.model.MassSpectrumTransFusion","text":"Bases: TransFusion Diffusion reconstruction model conditioned on mass spectra. Source code in instanovo/diffusion/model.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def __init__ ( self , cfg : MassSpectrumModelConfig , max_transcript_len : int = 200 ) -> None : super () . __init__ ( cfg , max_transcript_len ) layers = [] for i in range ( cfg . layers ): add_cond_cross_attn = i in list ( self . cfg . cond_cross_attn_layers ) layer = MassSpectrumTransformer ( self . cfg . dim , self . cfg . t_emb_dim , self . cfg . cond_emb_dim , self . cfg . nheads , add_cond_seq = add_cond_cross_attn , dropout = self . cfg . dropout , use_wavlm_attn = cfg . attention_type == \"wavlm\" and not add_cond_cross_attn , wavlm_num_bucket = cfg . wavlm_num_bucket , wavlm_max_dist = cfg . wavlm_max_dist , has_rel_attn_bias = ( cfg . attention_type == \"wavlm\" and i == 1 ), ) # add relative attn bias at i=1 as that is first attn where we do not use # cross attention. layers . append ( layer ) self . layers = nn . ModuleList ( layers ) self . conditioning_pos_emb = None if cfg . localised_attn : self . encoder = LocalisedSpectrumEncoder ( dim_model = cfg . dim , n_head = cfg . nheads , dim_feedforward = cfg . dim , n_layers = cfg . layers , dropout = cfg . dropout , window_size = cfg . window_size , mass_encoding = cfg . mass_encoding , ) else : self . encoder = CustomSpectrumEncoder ( dim_model = cfg . dim , n_head = cfg . nheads , dim_feedforward = cfg . dim_feedforward , n_layers = cfg . layers , dropout = cfg . dropout , mass_encoding = cfg . mass_encoding , ) # precursor embedding self . charge_encoder = torch . nn . Embedding ( cfg . max_charge , cfg . dim ) self . mass_encoder = MassEncoder ( cfg . dim ) self . cache_spectra = None self . cache_cond_emb = None self . cache_cond_padding_mask = None","title":"MassSpectrumTransFusion"},{"location":"reference/diffusion/model/#instanovo.diffusion.model.MassSpectrumTransFusion.forward","text":"Transformer with conditioning cross attention. x : (bs, seq_len) long tensor of character indices or (bs, seq_len, vocab_size) if cfg.diffusion_type == 'continuous' t : (bs, ) long tensor of timestep indices cond_emb : (bs, seq_len2, cond_emb_dim) if using wavlm encoder, else (bs, T) x_padding_mask : (bs, seq_len) if using wavlm encoder, else (bs, T) cond_padding_mask : (bs, seq_len2) Returns logits (bs, seq_len, vocab_size) Source code in instanovo/diffusion/model.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 def forward ( self , x : Tensor , t : Tensor , spectra : Tensor , spectra_padding_mask : Tensor , precursors : Tensor , x_padding_mask : Tensor | None = None , ) -> Tensor : \"\"\"Transformer with conditioning cross attention. - `x`: (bs, seq_len) long tensor of character indices or (bs, seq_len, vocab_size) if cfg.diffusion_type == 'continuous' - `t`: (bs, ) long tensor of timestep indices - `cond_emb`: (bs, seq_len2, cond_emb_dim) if using wavlm encoder, else (bs, T) - `x_padding_mask`: (bs, seq_len) if using wavlm encoder, else (bs, T) - `cond_padding_mask`: (bs, seq_len2) Returns logits (bs, seq_len, vocab_size) \"\"\" # 1. Base: character, timestep embeddings and zeroing bs = x . shape [ 0 ] x = self . char_embedding ( x ) # (bs, seq_len, dim) if self . cfg . pos_encoding == \"relative\" : x = self . pos_embedding ( x ) else : pos_emb = self . pos_embedding . weight [ None ] . expand ( bs , - 1 , - 1 ) # (seq_len, dim) --> (bs, seq_len, dim) x = x + pos_emb t_emb = timestep_embedding ( t , self . cfg . t_emb_dim , self . cfg . t_emb_max_period , dtype = spectra . dtype ) # (bs, t_dim) # 2. Classifier-free guidance: with prob cfg.drop_cond_prob, zero out and drop conditional probability if self . training : zero_cond_inds = torch . rand_like ( t , dtype = spectra . dtype ) < self . cfg . drop_cond_prob else : # never randomly zero when in eval mode zero_cond_inds = torch . zeros_like ( t , dtype = torch . bool ) if spectra_padding_mask . all (): # BUT, if all cond information is padded then we are obviously doing unconditional synthesis, # so, force zero_cond_inds to be all ones zero_cond_inds = ~ zero_cond_inds # 3. DENOVO calculate spectrum embedding here if self . training : cond_emb , cond_padding_mask = self . encoder ( spectra , spectra_padding_mask ) else : if self . cache_spectra is not None and torch . equal ( self . cache_spectra , spectra ): cond_emb , cond_padding_mask = self . cache_cond_emb , self . cache_cond_padding_mask else : cond_emb , cond_padding_mask = self . encoder ( spectra , spectra_padding_mask ) self . cache_spectra = spectra self . cache_cond_emb = cond_emb self . cache_cond_padding_mask = cond_padding_mask # set mask for these conditional entries to true everywhere (i.e. mask them out) masses = self . mass_encoder ( precursors [:, None , [ 0 ]]) charges = self . charge_encoder ( precursors [:, 1 ] . int () - 1 ) precursor_emb = masses + charges [:, None , :] cond_padding_mask [ zero_cond_inds ] = True cond_emb [ zero_cond_inds ] = 0 # 4. Iterate through layers pos_bias = None for layer in self . layers : x , pos_bias = layer ( x , t_emb , precursor_emb , cond_emb , x_padding_mask , cond_padding_mask , pos_bias = pos_bias , ) # 5. Pass through head to get logits x = self . head ( x ) # (bs, seq_len, vocab size) return x","title":"forward()"},{"location":"reference/diffusion/model/#instanovo.diffusion.model.MassSpectrumTransformer","text":"Bases: Pogfuse A transformer model specialised for encoding mass spectra.","title":"MassSpectrumTransformer"},{"location":"reference/diffusion/model/#instanovo.diffusion.model.MassSpectrumTransformer.forward","text":"Compute encodings with the model. Forward with x (bs, seq_len, dim), summing t_emb (bs, dim) before the transformer layer, and appending conditioning_emb (bs, seq_len2, dim) to the key/value pairs of the attention. Also pooled_conv_emb (bs, dim) is summed with the timestep embeddings Optionally specify key/value padding for input x with x_padding_mask (bs, seq_len), and optionally specify key/value padding mask for conditional embedding with cond_padding_mask (bs, seq_len2). By default no padding is used. Good idea to use cond padding but not x padding. pos_bias is positional bias for wavlm-style attention gated relative position bias. Returns x of same shape (bs, seq_len, dim) Source code in instanovo/diffusion/model.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 def forward ( self , x : Tensor , t_emb : Tensor , precursor_emb : Tensor , cond_emb : Tensor | None = None , x_padding_mask : Tensor | None = None , cond_padding_mask : Tensor | None = None , pos_bias : Tensor | None = None , ) -> Tensor : \"\"\"Compute encodings with the model. Forward with `x` (bs, seq_len, dim), summing `t_emb` (bs, dim) before the transformer layer, and appending `conditioning_emb` (bs, seq_len2, dim) to the key/value pairs of the attention. Also `pooled_conv_emb` (bs, dim) is summed with the timestep embeddings Optionally specify key/value padding for input `x` with `x_padding_mask` (bs, seq_len), and optionally specify key/value padding mask for conditional embedding with `cond_padding_mask` (bs, seq_len2). By default no padding is used. Good idea to use cond padding but not x padding. `pos_bias` is positional bias for wavlm-style attention gated relative position bias. Returns `x` of same shape (bs, seq_len, dim) \"\"\" # ----------------------- # 1. Get and add timestep embedding t = self . t_layers ( t_emb )[:, None ] # (bs, 1, dim) p = self . cond_pooled_layers ( precursor_emb ) # (bs, 1, dim) x += t + p # (bs, seq_len, dim) # ----------------------- # 2. Get and append conditioning embeddings if self . add_cond_seq : c = self . cond_layers ( cond_emb ) # (bs, seq_len2, dim) else : c = None # ----------------------- # 3. Do transformer layer # -- Self-attention block x1 , pos_bias = self . _sa_block ( x , c , x_padding_mask = x_padding_mask , c_padding_mask = cond_padding_mask , pos_bias = pos_bias ) # -- Layer-norm with residual connection x = self . norm1 ( x + x1 ) # -- Layer-norm with feedfoward block and residual connection x = self . norm2 ( x + self . _ff_block ( x )) return x , pos_bias","title":"forward()"},{"location":"reference/diffusion/multinomial_diffusion/","text":"DiffusionLoss ( model ) Bases: Module Holds logic for calculating the diffusion loss. Parameters: Name Type Description Default model MultinomialDiffusion The multinomial diffusion class. required Source code in instanovo/diffusion/multinomial_diffusion.py 276 277 278 279 def __init__ ( self , model : MultinomialDiffusion ) -> None : super () . __init__ () self . time_steps = model . time_steps self . model = model forward ( x_0 , ** kwargs ) Calculate a single Monte Carlo estimate of the multinomial diffusion loss (-ELBO). Parameters: Name Type Description Default x_0 LongTensor [ batch_size , sequence_length ] A batch of padded sequences. required Returns: Type Description FloatTensor torch.FloatTensor[1]: The loss estimate. Source code in instanovo/diffusion/multinomial_diffusion.py 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 def forward ( self , x_0 : torch . LongTensor , ** kwargs : dict ) -> torch . FloatTensor : \"\"\"Calculate a single Monte Carlo estimate of the multinomial diffusion loss (-ELBO). Args: x_0 (torch.LongTensor[batch_size, sequence_length]): A batch of padded sequences. Returns: torch.FloatTensor[1]: The loss estimate. \"\"\" # 1. Sample time step t = torch . randint ( 0 , self . time_steps - 1 , ( x_0 . shape [ 0 ],)) . to ( x_0 . device ) # 2. Compute L_t loss = self . _compute_loss ( t = t , x_0 = x_0 , ** kwargs ) . mean () # 3. Calculate prior KL term log_x_0 = torch . log ( one_hot ( x_0 , num_classes = len ( self . model . residues ))) final_log_probs = self . model . mixture_categorical ( log_x = log_x_0 , log_alpha = self . model . cumulative_schedule [ self . time_steps - 1 ] . unsqueeze ( - 1 ) . unsqueeze ( - 1 ), log_alpha_complement = self . model . cumulative_schedule_complement [ self . time_steps - 1 ] . unsqueeze ( - 1 ) . unsqueeze ( - 1 ), ) uniform_log_probs = torch . log ( torch . ones_like ( final_log_probs ) / len ( self . model . residues )) kl_loss = self . kl_divergence ( final_log_probs , uniform_log_probs ) . mean () return loss + kl_loss kl_divergence ( log_probs_first , log_probs_second ) staticmethod Calculate the Kullback-Liebler divergence between two multinomial distributions. Parameters: Name Type Description Default log_probs_first FloatTensor [..., num_classes ] The log-probabilities of the base distribution. required log_probs_second FloatTensor [..., num_classes ] The log-probabilities of the comparison distribution. required Returns: Type Description FloatTensor torch.FloatTensor[1]: The KL-divergence averaged over all but the final dimension. Source code in instanovo/diffusion/multinomial_diffusion.py 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 @staticmethod def kl_divergence ( log_probs_first : torch . FloatTensor , log_probs_second : torch . FloatTensor ) -> torch . FloatTensor : \"\"\"Calculate the Kullback-Liebler divergence between two multinomial distributions. Args: log_probs_first (torch.FloatTensor[..., num_classes]): The log-probabilities of the base distribution. log_probs_second (torch.FloatTensor[..., num_classes]): The log-probabilities of the comparison distribution. Returns: torch.FloatTensor[1]: The KL-divergence averaged over all but the final dimension. \"\"\" return ( torch . exp ( log_probs_first ) * ( log_probs_first - log_probs_second )) . sum ( - 1 ) . sum ( - 1 ) MultinomialDiffusion ( config , transition_model , diffusion_schedule , residues ) Bases: Module This class implements Multinomial Diffusion as described in Hoogeboom et al. 2021. Parameters: Name Type Description Default config DictConfig The model configuration. This should have keys: - 'name': the model name identifier. - 'time_steps': the number of time steps in the diffusion process - 'max_length': the maximum sequence for the model - 'device': the device where the Pytorch model should be loaded e.g. cpu , cuda:0 etc. - 'vocab_size': the number of residues in the vocabulary - 'transition_model': the DictConfig for the transition model This information is necessary for saving and loading the model. required transition_model Module The model that predictions the initial sequence given the sequence sampled the current time step and the sequence sampled the previous time step. This is just a sequence tagging model. required diffusion_schedule FloatTensor [ time_steps ] The sequence of diffusion probabilities. Note that diffusion_schedule[t] is \\alpha_t in the paper's terminology, not \\beta_t. required residues ResidueSet The residue vocabulary. This holds a mapping between residues and indices and residue masses. required Source code in instanovo/diffusion/multinomial_diffusion.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def __init__ ( self , config : DictConfig , transition_model : nn . Module , diffusion_schedule : torch . FloatTensor , residues : ResidueSet , ) -> None : super () . __init__ () self . config = config self . time_steps = config . time_steps self . residues = residues self . transition_model = transition_model self . register_buffer ( \"diffusion_schedule\" , torch . log ( diffusion_schedule )) self . register_buffer ( \"diffusion_schedule_complement\" , torch . log ( 1 - diffusion_schedule )) self . register_buffer ( \"cumulative_schedule\" , torch . cumsum ( self . diffusion_schedule , - 1 )) self . register_buffer ( \"cumulative_schedule_complement\" , torch . log ( 1 - torch . exp ( self . cumulative_schedule )) ) forward ( log_x_t , log_x_0 , t ) Calculate the log-posterior of the t-1 -th process values given the 0-th and t-th values. Parameters: Name Type Description Default log_x_t FloatTensor [ batch_size , sequence_length , num_classes ] The log one-hot representation of the process values at the t -th time step. required log_x_0 FloatTensor [ batch_size , sequence_length , num_classes ] The log one-hot representation of the process values at the t -th time step. required t int The time step. required Returns: Type Description FloatTensor torch.FloatTensor[batch_size, sequence_length, num_classes]: The log-posterior probabilities of the process values at the t-1 -th time step given the values at the 0-th and t -th time step i.e. q( x_{t-1} | x_{t}, x_0 ). Source code in instanovo/diffusion/multinomial_diffusion.py 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 def forward ( self , log_x_t : torch . FloatTensor , log_x_0 : torch . FloatTensor , t : torch . FloatTensor ) -> torch . FloatTensor : \"\"\"Calculate the log-posterior of the `t-1`-th process values given the 0-th and t-th values. Args: log_x_t (torch.FloatTensor[batch_size, sequence_length, num_classes]): The log one-hot representation of the process values at the `t`-th time step. log_x_0 (torch.FloatTensor[batch_size, sequence_length, num_classes]): The log one-hot representation of the process values at the `t`-th time step. t (int): The time step. Returns: torch.FloatTensor[batch_size, sequence_length, num_classes]: The log-posterior probabilities of the process values at the `t-1`-th time step given the values at the 0-th and `t`-th time step i.e. q( x_{t-1} | x_{t}, x_0 ). \"\"\" log_prior = self . mixture_categorical ( log_x = log_x_0 , log_alpha = self . cumulative_schedule [ t - 1 ] . unsqueeze ( - 1 ) . unsqueeze ( - 1 ), log_alpha_complement = self . cumulative_schedule_complement [ t - 1 ] . unsqueeze ( - 1 ) . unsqueeze ( - 1 ), ) log_likelihood = self . mixture_categorical ( log_x = log_x_t , log_alpha = self . diffusion_schedule [ t ] . unsqueeze ( - 1 ) . unsqueeze ( - 1 ), log_alpha_complement = self . diffusion_schedule_complement [ t ] . unsqueeze ( - 1 ) . unsqueeze ( - 1 ), ) t_mask = ( t == 0 ) . unsqueeze ( - 1 ) . unsqueeze ( - 1 ) . expand_as ( log_x_0 ) prior_term = torch . where ( t_mask , log_x_0 , log_prior ) logits = log_likelihood + prior_term return torch . log_softmax ( logits , - 1 ) load ( path ) classmethod Load a saved model. Parameters: Name Type Description Default path str Path to the directory where the model is saved. required Returns: Type Description MultinomialDiffusion The loaded model. Source code in instanovo/diffusion/multinomial_diffusion.py 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 @classmethod def load ( cls , path : str ) -> MultinomialDiffusion : \"\"\"Load a saved model. Args: path (str): Path to the directory where the model is saved. Returns: (MultinomialDiffusion): The loaded model. \"\"\" # Load config config = OmegaConf . load ( os . path . join ( path , \"config.yaml\" )) # Load residues residue_masses = OmegaConf . load ( os . path . join ( path , \"residues.yaml\" )) residues = ResidueSet ( residue_masses = residue_masses ) # Load schedule diffusion_schedule = torch . load ( os . path . join ( path , \"diffusion_schedule.pt\" )) # Load transition model transition_model = MassSpectrumTransFusion ( config . transition_model , config . max_length ) transition_model . load_state_dict ( torch . load ( os . path . join ( path , \"transition_model.ckpt\" ))) return cls ( config = config , transition_model = transition_model , diffusion_schedule = diffusion_schedule , residues = residues , ) mixture_categorical ( log_x , log_alpha , log_alpha_complement ) A categorical mixture between a base distribution and a uniform distribution. Parameters: Name Type Description Default log_x FloatTensor [..., num_classes ] The base distribution. required log_alpha float The log of the mixture weight. required log_alpha_complement float The log of 1 minus the mixture weight. required Returns: Type Description FloatTensor torch.FloatTensor[..., num_classes]: The log-probabilities of the mixture. Source code in instanovo/diffusion/multinomial_diffusion.py 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 def mixture_categorical ( self , log_x : torch . FloatTensor , log_alpha : float , log_alpha_complement : float ) -> torch . FloatTensor : \"\"\"A categorical mixture between a base distribution and a uniform distribution. Args: log_x (torch.FloatTensor[..., num_classes]): The base distribution. log_alpha (float): The log of the mixture weight. log_alpha_complement (float): The log of 1 minus the mixture weight. Returns: torch.FloatTensor[..., num_classes]: The log-probabilities of the mixture. \"\"\" return torch . logaddexp ( log_x + log_alpha , log_alpha_complement - math . log ( len ( self . residues )), ) prepare_fine_tuning ( residues ) Prepare a model for fine-tuning on a dataset with a new residue vocabulary. Parameters: Name Type Description Default residues ResidueSet The residue vocabulary for the new dataset. required Source code in instanovo/diffusion/multinomial_diffusion.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 def prepare_fine_tuning ( self , residues : ResidueSet ) -> None : \"\"\"Prepare a model for fine-tuning on a dataset with a new residue vocabulary. Args: residues (ResidueSet): The residue vocabulary for the new dataset. \"\"\" # 1. Update residue set self . residues = residues num_residues = len ( self . residues ) model_dim = self . config . transition_model . dim # 2. Update config self . config . transition_model . vocab_size = num_residues # 3. Update modules self . transition_model . char_embedding = nn . Embedding ( num_embeddings = num_residues , embedding_dim = model_dim ) self . transition_model . head [ 1 ] = nn . Linear ( model_dim , num_residues ) reverse_distribution ( x_t , time , ** kwargs ) Calculate the reverse transition distribution of the diffusion process. Parameters: Name Type Description Default x_t FloatTensor [ batch_size , sequence_length ] The values at the t -th time step of the reverse process. required time int The time step. required Returns: Type Description FloatTensor torch.FloatTensor[batch_size, sequence_length, num_classes]: The log-probabilities of values for the t-1 -th time step given values at the t -th time step i.e. log p( x_{t-1} | x_{t} ) . Source code in instanovo/diffusion/multinomial_diffusion.py 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 def reverse_distribution ( self , x_t : torch . FloatTensor , time : torch . FloatTensor , ** kwargs : dict ) -> torch . FloatTensor : \"\"\"Calculate the reverse transition distribution of the diffusion process. Args: x_t (torch.FloatTensor[batch_size, sequence_length]): The values at the `t`-th time step of the reverse process. time (int): The time step. Returns: torch.FloatTensor[batch_size, sequence_length, num_classes]: The log-probabilities of values for the `t-1`-th time step given values at the `t`-th time step i.e. `log p( x_{t-1} | x_{t} )`. \"\"\" log_x_0 = log_softmax ( self . transition_model ( x_t , t = time , ** kwargs ), - 1 ) return self . forward ( log_x_t = torch . log ( one_hot ( x_t , len ( self . residues ))), log_x_0 = log_x_0 , t = time ) save ( path , overwrite = False ) Save the model to a directory. Parameters: Name Type Description Default path str Path to the base directory where the model is saved. The model is saved in a subdirectory with the model's name identifier. required overwrite bool Whether to overwrite the directory if one already exists for the model. Defaults to False. False Raises: Type Description FileExistsError If overwrite is False and a directory already exists for the model identifier. Source code in instanovo/diffusion/multinomial_diffusion.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def save ( self , path : str , overwrite : bool = False ) -> None : \"\"\"Save the model to a directory. Args: path (str): Path to the base directory where the model is saved. The model is saved in a subdirectory with the model's name identifier. overwrite (bool, optional): Whether to overwrite the directory if one already exists for the model. Defaults to False. Raises: FileExistsError: If `overwrite` is `False` and a directory already exists for the model identifier. \"\"\" # Make directory model_dir = os . path . join ( path , self . config . name ) if os . path . exists ( model_dir ): if overwrite : shutil . rmtree ( model_dir ) else : raise FileExistsError os . mkdir ( path = model_dir ) # Save config OmegaConf . save ( config = self . config , f = os . path . join ( model_dir , \"config.yaml\" )) # Save residues residues = OmegaConf . create ( self . residues . residue_masses ) OmegaConf . save ( config = residues , f = os . path . join ( model_dir , \"residues.yaml\" )) # Save schedule torch . save ( torch . exp ( self . diffusion_schedule ), os . path . join ( model_dir , \"diffusion_schedule.pt\" ) ) # Save transition model self . transition_model . to ( \"cpu\" ) torch . save ( self . transition_model . state_dict (), os . path . join ( model_dir , \"transition_model.ckpt\" ) ) self . transition_model . to ( self . config . device ) cosine_beta_schedule ( timesteps , s = 0.008 ) Cosine schedule as proposed in https://arxiv.org/abs/2102.09672 . Returns alpha parameters, NOT Beta Source code in instanovo/diffusion/multinomial_diffusion.py 19 20 21 22 23 24 25 26 27 28 29 30 def cosine_beta_schedule ( timesteps : int , s : float = 0.008 ) -> torch . FloatTensor : \"\"\"Cosine schedule as proposed in https://arxiv.org/abs/2102.09672 . Returns alpha parameters, NOT Beta \"\"\" steps = timesteps + 1 x = torch . linspace ( 0 , timesteps , steps ) alphas_cumprod = torch . cos ((( x / timesteps ) + s ) / ( 1 + s ) * torch . pi * 0.5 ) ** 2 alphas_cumprod = alphas_cumprod / alphas_cumprod [ 0 ] alphas = alphas_cumprod [ 1 :] / alphas_cumprod [: - 1 ] alphas = torch . clamp ( alphas , 0.001 , 1.0 ) return torch . sqrt ( alphas )","title":"Multinomial diffusion"},{"location":"reference/diffusion/multinomial_diffusion/#instanovo.diffusion.multinomial_diffusion.DiffusionLoss","text":"Bases: Module Holds logic for calculating the diffusion loss. Parameters: Name Type Description Default model MultinomialDiffusion The multinomial diffusion class. required Source code in instanovo/diffusion/multinomial_diffusion.py 276 277 278 279 def __init__ ( self , model : MultinomialDiffusion ) -> None : super () . __init__ () self . time_steps = model . time_steps self . model = model","title":"DiffusionLoss"},{"location":"reference/diffusion/multinomial_diffusion/#instanovo.diffusion.multinomial_diffusion.DiffusionLoss.forward","text":"Calculate a single Monte Carlo estimate of the multinomial diffusion loss (-ELBO). Parameters: Name Type Description Default x_0 LongTensor [ batch_size , sequence_length ] A batch of padded sequences. required Returns: Type Description FloatTensor torch.FloatTensor[1]: The loss estimate. Source code in instanovo/diffusion/multinomial_diffusion.py 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 def forward ( self , x_0 : torch . LongTensor , ** kwargs : dict ) -> torch . FloatTensor : \"\"\"Calculate a single Monte Carlo estimate of the multinomial diffusion loss (-ELBO). Args: x_0 (torch.LongTensor[batch_size, sequence_length]): A batch of padded sequences. Returns: torch.FloatTensor[1]: The loss estimate. \"\"\" # 1. Sample time step t = torch . randint ( 0 , self . time_steps - 1 , ( x_0 . shape [ 0 ],)) . to ( x_0 . device ) # 2. Compute L_t loss = self . _compute_loss ( t = t , x_0 = x_0 , ** kwargs ) . mean () # 3. Calculate prior KL term log_x_0 = torch . log ( one_hot ( x_0 , num_classes = len ( self . model . residues ))) final_log_probs = self . model . mixture_categorical ( log_x = log_x_0 , log_alpha = self . model . cumulative_schedule [ self . time_steps - 1 ] . unsqueeze ( - 1 ) . unsqueeze ( - 1 ), log_alpha_complement = self . model . cumulative_schedule_complement [ self . time_steps - 1 ] . unsqueeze ( - 1 ) . unsqueeze ( - 1 ), ) uniform_log_probs = torch . log ( torch . ones_like ( final_log_probs ) / len ( self . model . residues )) kl_loss = self . kl_divergence ( final_log_probs , uniform_log_probs ) . mean () return loss + kl_loss","title":"forward()"},{"location":"reference/diffusion/multinomial_diffusion/#instanovo.diffusion.multinomial_diffusion.DiffusionLoss.kl_divergence","text":"Calculate the Kullback-Liebler divergence between two multinomial distributions. Parameters: Name Type Description Default log_probs_first FloatTensor [..., num_classes ] The log-probabilities of the base distribution. required log_probs_second FloatTensor [..., num_classes ] The log-probabilities of the comparison distribution. required Returns: Type Description FloatTensor torch.FloatTensor[1]: The KL-divergence averaged over all but the final dimension. Source code in instanovo/diffusion/multinomial_diffusion.py 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 @staticmethod def kl_divergence ( log_probs_first : torch . FloatTensor , log_probs_second : torch . FloatTensor ) -> torch . FloatTensor : \"\"\"Calculate the Kullback-Liebler divergence between two multinomial distributions. Args: log_probs_first (torch.FloatTensor[..., num_classes]): The log-probabilities of the base distribution. log_probs_second (torch.FloatTensor[..., num_classes]): The log-probabilities of the comparison distribution. Returns: torch.FloatTensor[1]: The KL-divergence averaged over all but the final dimension. \"\"\" return ( torch . exp ( log_probs_first ) * ( log_probs_first - log_probs_second )) . sum ( - 1 ) . sum ( - 1 )","title":"kl_divergence()"},{"location":"reference/diffusion/multinomial_diffusion/#instanovo.diffusion.multinomial_diffusion.MultinomialDiffusion","text":"Bases: Module This class implements Multinomial Diffusion as described in Hoogeboom et al. 2021. Parameters: Name Type Description Default config DictConfig The model configuration. This should have keys: - 'name': the model name identifier. - 'time_steps': the number of time steps in the diffusion process - 'max_length': the maximum sequence for the model - 'device': the device where the Pytorch model should be loaded e.g. cpu , cuda:0 etc. - 'vocab_size': the number of residues in the vocabulary - 'transition_model': the DictConfig for the transition model This information is necessary for saving and loading the model. required transition_model Module The model that predictions the initial sequence given the sequence sampled the current time step and the sequence sampled the previous time step. This is just a sequence tagging model. required diffusion_schedule FloatTensor [ time_steps ] The sequence of diffusion probabilities. Note that diffusion_schedule[t] is \\alpha_t in the paper's terminology, not \\beta_t. required residues ResidueSet The residue vocabulary. This holds a mapping between residues and indices and residue masses. required Source code in instanovo/diffusion/multinomial_diffusion.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def __init__ ( self , config : DictConfig , transition_model : nn . Module , diffusion_schedule : torch . FloatTensor , residues : ResidueSet , ) -> None : super () . __init__ () self . config = config self . time_steps = config . time_steps self . residues = residues self . transition_model = transition_model self . register_buffer ( \"diffusion_schedule\" , torch . log ( diffusion_schedule )) self . register_buffer ( \"diffusion_schedule_complement\" , torch . log ( 1 - diffusion_schedule )) self . register_buffer ( \"cumulative_schedule\" , torch . cumsum ( self . diffusion_schedule , - 1 )) self . register_buffer ( \"cumulative_schedule_complement\" , torch . log ( 1 - torch . exp ( self . cumulative_schedule )) )","title":"MultinomialDiffusion"},{"location":"reference/diffusion/multinomial_diffusion/#instanovo.diffusion.multinomial_diffusion.MultinomialDiffusion.forward","text":"Calculate the log-posterior of the t-1 -th process values given the 0-th and t-th values. Parameters: Name Type Description Default log_x_t FloatTensor [ batch_size , sequence_length , num_classes ] The log one-hot representation of the process values at the t -th time step. required log_x_0 FloatTensor [ batch_size , sequence_length , num_classes ] The log one-hot representation of the process values at the t -th time step. required t int The time step. required Returns: Type Description FloatTensor torch.FloatTensor[batch_size, sequence_length, num_classes]: The log-posterior probabilities of the process values at the t-1 -th time step given the values at the 0-th and t -th time step i.e. q( x_{t-1} | x_{t}, x_0 ). Source code in instanovo/diffusion/multinomial_diffusion.py 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 def forward ( self , log_x_t : torch . FloatTensor , log_x_0 : torch . FloatTensor , t : torch . FloatTensor ) -> torch . FloatTensor : \"\"\"Calculate the log-posterior of the `t-1`-th process values given the 0-th and t-th values. Args: log_x_t (torch.FloatTensor[batch_size, sequence_length, num_classes]): The log one-hot representation of the process values at the `t`-th time step. log_x_0 (torch.FloatTensor[batch_size, sequence_length, num_classes]): The log one-hot representation of the process values at the `t`-th time step. t (int): The time step. Returns: torch.FloatTensor[batch_size, sequence_length, num_classes]: The log-posterior probabilities of the process values at the `t-1`-th time step given the values at the 0-th and `t`-th time step i.e. q( x_{t-1} | x_{t}, x_0 ). \"\"\" log_prior = self . mixture_categorical ( log_x = log_x_0 , log_alpha = self . cumulative_schedule [ t - 1 ] . unsqueeze ( - 1 ) . unsqueeze ( - 1 ), log_alpha_complement = self . cumulative_schedule_complement [ t - 1 ] . unsqueeze ( - 1 ) . unsqueeze ( - 1 ), ) log_likelihood = self . mixture_categorical ( log_x = log_x_t , log_alpha = self . diffusion_schedule [ t ] . unsqueeze ( - 1 ) . unsqueeze ( - 1 ), log_alpha_complement = self . diffusion_schedule_complement [ t ] . unsqueeze ( - 1 ) . unsqueeze ( - 1 ), ) t_mask = ( t == 0 ) . unsqueeze ( - 1 ) . unsqueeze ( - 1 ) . expand_as ( log_x_0 ) prior_term = torch . where ( t_mask , log_x_0 , log_prior ) logits = log_likelihood + prior_term return torch . log_softmax ( logits , - 1 )","title":"forward()"},{"location":"reference/diffusion/multinomial_diffusion/#instanovo.diffusion.multinomial_diffusion.MultinomialDiffusion.load","text":"Load a saved model. Parameters: Name Type Description Default path str Path to the directory where the model is saved. required Returns: Type Description MultinomialDiffusion The loaded model. Source code in instanovo/diffusion/multinomial_diffusion.py 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 @classmethod def load ( cls , path : str ) -> MultinomialDiffusion : \"\"\"Load a saved model. Args: path (str): Path to the directory where the model is saved. Returns: (MultinomialDiffusion): The loaded model. \"\"\" # Load config config = OmegaConf . load ( os . path . join ( path , \"config.yaml\" )) # Load residues residue_masses = OmegaConf . load ( os . path . join ( path , \"residues.yaml\" )) residues = ResidueSet ( residue_masses = residue_masses ) # Load schedule diffusion_schedule = torch . load ( os . path . join ( path , \"diffusion_schedule.pt\" )) # Load transition model transition_model = MassSpectrumTransFusion ( config . transition_model , config . max_length ) transition_model . load_state_dict ( torch . load ( os . path . join ( path , \"transition_model.ckpt\" ))) return cls ( config = config , transition_model = transition_model , diffusion_schedule = diffusion_schedule , residues = residues , )","title":"load()"},{"location":"reference/diffusion/multinomial_diffusion/#instanovo.diffusion.multinomial_diffusion.MultinomialDiffusion.mixture_categorical","text":"A categorical mixture between a base distribution and a uniform distribution. Parameters: Name Type Description Default log_x FloatTensor [..., num_classes ] The base distribution. required log_alpha float The log of the mixture weight. required log_alpha_complement float The log of 1 minus the mixture weight. required Returns: Type Description FloatTensor torch.FloatTensor[..., num_classes]: The log-probabilities of the mixture. Source code in instanovo/diffusion/multinomial_diffusion.py 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 def mixture_categorical ( self , log_x : torch . FloatTensor , log_alpha : float , log_alpha_complement : float ) -> torch . FloatTensor : \"\"\"A categorical mixture between a base distribution and a uniform distribution. Args: log_x (torch.FloatTensor[..., num_classes]): The base distribution. log_alpha (float): The log of the mixture weight. log_alpha_complement (float): The log of 1 minus the mixture weight. Returns: torch.FloatTensor[..., num_classes]: The log-probabilities of the mixture. \"\"\" return torch . logaddexp ( log_x + log_alpha , log_alpha_complement - math . log ( len ( self . residues )), )","title":"mixture_categorical()"},{"location":"reference/diffusion/multinomial_diffusion/#instanovo.diffusion.multinomial_diffusion.MultinomialDiffusion.prepare_fine_tuning","text":"Prepare a model for fine-tuning on a dataset with a new residue vocabulary. Parameters: Name Type Description Default residues ResidueSet The residue vocabulary for the new dataset. required Source code in instanovo/diffusion/multinomial_diffusion.py 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 def prepare_fine_tuning ( self , residues : ResidueSet ) -> None : \"\"\"Prepare a model for fine-tuning on a dataset with a new residue vocabulary. Args: residues (ResidueSet): The residue vocabulary for the new dataset. \"\"\" # 1. Update residue set self . residues = residues num_residues = len ( self . residues ) model_dim = self . config . transition_model . dim # 2. Update config self . config . transition_model . vocab_size = num_residues # 3. Update modules self . transition_model . char_embedding = nn . Embedding ( num_embeddings = num_residues , embedding_dim = model_dim ) self . transition_model . head [ 1 ] = nn . Linear ( model_dim , num_residues )","title":"prepare_fine_tuning()"},{"location":"reference/diffusion/multinomial_diffusion/#instanovo.diffusion.multinomial_diffusion.MultinomialDiffusion.reverse_distribution","text":"Calculate the reverse transition distribution of the diffusion process. Parameters: Name Type Description Default x_t FloatTensor [ batch_size , sequence_length ] The values at the t -th time step of the reverse process. required time int The time step. required Returns: Type Description FloatTensor torch.FloatTensor[batch_size, sequence_length, num_classes]: The log-probabilities of values for the t-1 -th time step given values at the t -th time step i.e. log p( x_{t-1} | x_{t} ) . Source code in instanovo/diffusion/multinomial_diffusion.py 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 def reverse_distribution ( self , x_t : torch . FloatTensor , time : torch . FloatTensor , ** kwargs : dict ) -> torch . FloatTensor : \"\"\"Calculate the reverse transition distribution of the diffusion process. Args: x_t (torch.FloatTensor[batch_size, sequence_length]): The values at the `t`-th time step of the reverse process. time (int): The time step. Returns: torch.FloatTensor[batch_size, sequence_length, num_classes]: The log-probabilities of values for the `t-1`-th time step given values at the `t`-th time step i.e. `log p( x_{t-1} | x_{t} )`. \"\"\" log_x_0 = log_softmax ( self . transition_model ( x_t , t = time , ** kwargs ), - 1 ) return self . forward ( log_x_t = torch . log ( one_hot ( x_t , len ( self . residues ))), log_x_0 = log_x_0 , t = time )","title":"reverse_distribution()"},{"location":"reference/diffusion/multinomial_diffusion/#instanovo.diffusion.multinomial_diffusion.MultinomialDiffusion.save","text":"Save the model to a directory. Parameters: Name Type Description Default path str Path to the base directory where the model is saved. The model is saved in a subdirectory with the model's name identifier. required overwrite bool Whether to overwrite the directory if one already exists for the model. Defaults to False. False Raises: Type Description FileExistsError If overwrite is False and a directory already exists for the model identifier. Source code in instanovo/diffusion/multinomial_diffusion.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 def save ( self , path : str , overwrite : bool = False ) -> None : \"\"\"Save the model to a directory. Args: path (str): Path to the base directory where the model is saved. The model is saved in a subdirectory with the model's name identifier. overwrite (bool, optional): Whether to overwrite the directory if one already exists for the model. Defaults to False. Raises: FileExistsError: If `overwrite` is `False` and a directory already exists for the model identifier. \"\"\" # Make directory model_dir = os . path . join ( path , self . config . name ) if os . path . exists ( model_dir ): if overwrite : shutil . rmtree ( model_dir ) else : raise FileExistsError os . mkdir ( path = model_dir ) # Save config OmegaConf . save ( config = self . config , f = os . path . join ( model_dir , \"config.yaml\" )) # Save residues residues = OmegaConf . create ( self . residues . residue_masses ) OmegaConf . save ( config = residues , f = os . path . join ( model_dir , \"residues.yaml\" )) # Save schedule torch . save ( torch . exp ( self . diffusion_schedule ), os . path . join ( model_dir , \"diffusion_schedule.pt\" ) ) # Save transition model self . transition_model . to ( \"cpu\" ) torch . save ( self . transition_model . state_dict (), os . path . join ( model_dir , \"transition_model.ckpt\" ) ) self . transition_model . to ( self . config . device )","title":"save()"},{"location":"reference/diffusion/multinomial_diffusion/#instanovo.diffusion.multinomial_diffusion.cosine_beta_schedule","text":"Cosine schedule as proposed in https://arxiv.org/abs/2102.09672 . Returns alpha parameters, NOT Beta Source code in instanovo/diffusion/multinomial_diffusion.py 19 20 21 22 23 24 25 26 27 28 29 30 def cosine_beta_schedule ( timesteps : int , s : float = 0.008 ) -> torch . FloatTensor : \"\"\"Cosine schedule as proposed in https://arxiv.org/abs/2102.09672 . Returns alpha parameters, NOT Beta \"\"\" steps = timesteps + 1 x = torch . linspace ( 0 , timesteps , steps ) alphas_cumprod = torch . cos ((( x / timesteps ) + s ) / ( 1 + s ) * torch . pi * 0.5 ) ** 2 alphas_cumprod = alphas_cumprod / alphas_cumprod [ 0 ] alphas = alphas_cumprod [ 1 :] / alphas_cumprod [: - 1 ] alphas = torch . clamp ( alphas , 0.001 , 1.0 ) return torch . sqrt ( alphas )","title":"cosine_beta_schedule()"},{"location":"reference/diffusion/predict/","text":"main ( input_path , start_predictions_path , model_path , output_path , batch_size , device ) Predict peptides from spectra using the diffusion model for iterative refinement. Parameters: Name Type Description Default input_path str Path to Polars .ipc file containing spectra. This should have the columns: - \"Mass values\": the sequence of m/z values as a list or numpy array. - \"Intensities\": the instensities corresponding to the m/z values in \"Mass values\". (This should be the same length as \"Mass values\".) - \"MS/MS m/z\": the precursor mass-to-charge ratio. - \"Charge\": the precursor charge required start_predictions_path str Path to the csv holding initial predictions to be refined by the diffusion model. This should be index-aligned with the Polars .ipc file specified in input_path and the predictions should be in a column called \"Predictions\" as an already tokenized list of strings. required model_path str Path to the model checkpoint. This should be a directory (so it should first be unzipped if it is a zip file). required output_path str Path where the output should be written. This should include the filename which should be a .csv file. required batch_size int Batch size to use during decoding. This will only affect the speed and the memory used. required device str Device on which to load the model and data. Any device type supported by Pytorch (e.g. \"cpu\", \"cuda\", \"cuda:0\"). Please note this code has only been tested on CPU and CUDA GPUs. required Source code in instanovo/diffusion/predict.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 @click . command () @click . option ( \"--input-path\" , \"-i\" ) @click . option ( \"--start-predictions-path\" , \"-s\" ) @click . option ( \"--model-path\" , \"-m\" ) @click . option ( \"--output-path\" , \"-o\" ) @click . option ( \"--batch-size\" , \"-bs\" , default = 16 ) @click . option ( \"--device\" , \"-dv\" , default = \"cpu\" ) def main ( input_path : str , start_predictions_path : str , model_path : str , output_path : str , batch_size : int , device : str , ) -> None : \"\"\"Predict peptides from spectra using the diffusion model for iterative refinement. Args: input_path (str): Path to Polars `.ipc` file containing spectra. This should have the columns: - \"Mass values\": the sequence of m/z values as a list or numpy array. - \"Intensities\": the instensities corresponding to the m/z values in \"Mass values\". (This should be the same length as \"Mass values\".) - \"MS/MS m/z\": the precursor mass-to-charge ratio. - \"Charge\": the precursor charge start_predictions_path (str): Path to the csv holding initial predictions to be refined by the diffusion model. This should be index-aligned with the Polars `.ipc` file specified in `input_path` and the predictions should be in a column called \"Predictions\" as an already tokenized list of strings. model_path (str): Path to the model checkpoint. This should be a directory (so it should first be unzipped if it is a zip file). output_path (str): Path where the output should be written. This should include the filename which should be a `.csv` file. batch_size (int): Batch size to use during decoding. This will only affect the speed and the memory used. device (str): Device on which to load the model and data. Any device type supported by `Pytorch` (e.g. \"cpu\", \"cuda\", \"cuda:0\"). Please note this code has only been tested on CPU and CUDA GPUs. \"\"\" logger = logging . Logger ( name = \"diffusion/predict\" , level = logging . INFO ) # 1. Load model logger . info ( \"Loading model.\" ) model = MultinomialDiffusion . load ( path = model_path ) model = model . to ( device = device ) # 2. Initialize decoder logger . info ( \"Initializing decoder.\" ) decoder = DiffusionDecoder ( model = model ) # 3. Load data logger . info ( \"Loading data.\" ) logger . info ( \"Loading residues.\" ) residue_masses = yaml . safe_load ( open ( os . path . join ( model_path , \"residues.yaml\" ))) residues = ResidueSet ( residue_masses = residue_masses ) logger . info ( f \"Loading input data from { input_path } .\" ) input_data = polars . read_ipc ( input_path ) logger . info ( f \"Loading predictions from { start_predictions_path } .\" ) start_predictions = pandas . read_csv ( start_predictions_path ) input_dataset = AnnotatedPolarsSpectrumDataset ( data_frame = input_data , peptides = start_predictions [ \"Predictions\" ] . tolist () ) data_loader = torch . utils . data . DataLoader ( input_dataset , shuffle = False , batch_size = batch_size , collate_fn = collate_batches ( residues = residues , time_steps = model . time_steps , annotated = True , max_length = model . config . max_length , ), ) # 4. Elicit predictions logger . info ( \"Performing decoding.\" ) results = [] all_log_probs = [] with torch . no_grad (): for spectra , spectra_padding_mask , precursors , peptides , _ in tqdm . tqdm ( iter ( data_loader ), total = len ( data_loader ) ): predictions , log_probs = decoder . decode ( initial_sequence = peptides . to ( device ), spectra = spectra . to ( device ), spectra_padding_mask = spectra_padding_mask . to ( device ), precursors = precursors . to ( device ), start_step = DIFFUSION_START_STEP , ) predictions = [ prediction if \"$\" not in prediction else prediction [: prediction . index ( \"$\" )] for prediction in predictions ] predictions = [ \"\" . join ( prediction ) for prediction in predictions ] results . extend ( predictions ) all_log_probs . extend ( log_probs ) # 5. Save predictions logger . info ( \"Saving predictions.\" ) output = input_data . to_pandas () output [ \"diffusion_predictions\" ] = results output [ \"diffusion_log_probs\" ] = all_log_probs output . to_csv ( output_path )","title":"Predict"},{"location":"reference/diffusion/predict/#instanovo.diffusion.predict.main","text":"Predict peptides from spectra using the diffusion model for iterative refinement. Parameters: Name Type Description Default input_path str Path to Polars .ipc file containing spectra. This should have the columns: - \"Mass values\": the sequence of m/z values as a list or numpy array. - \"Intensities\": the instensities corresponding to the m/z values in \"Mass values\". (This should be the same length as \"Mass values\".) - \"MS/MS m/z\": the precursor mass-to-charge ratio. - \"Charge\": the precursor charge required start_predictions_path str Path to the csv holding initial predictions to be refined by the diffusion model. This should be index-aligned with the Polars .ipc file specified in input_path and the predictions should be in a column called \"Predictions\" as an already tokenized list of strings. required model_path str Path to the model checkpoint. This should be a directory (so it should first be unzipped if it is a zip file). required output_path str Path where the output should be written. This should include the filename which should be a .csv file. required batch_size int Batch size to use during decoding. This will only affect the speed and the memory used. required device str Device on which to load the model and data. Any device type supported by Pytorch (e.g. \"cpu\", \"cuda\", \"cuda:0\"). Please note this code has only been tested on CPU and CUDA GPUs. required Source code in instanovo/diffusion/predict.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 @click . command () @click . option ( \"--input-path\" , \"-i\" ) @click . option ( \"--start-predictions-path\" , \"-s\" ) @click . option ( \"--model-path\" , \"-m\" ) @click . option ( \"--output-path\" , \"-o\" ) @click . option ( \"--batch-size\" , \"-bs\" , default = 16 ) @click . option ( \"--device\" , \"-dv\" , default = \"cpu\" ) def main ( input_path : str , start_predictions_path : str , model_path : str , output_path : str , batch_size : int , device : str , ) -> None : \"\"\"Predict peptides from spectra using the diffusion model for iterative refinement. Args: input_path (str): Path to Polars `.ipc` file containing spectra. This should have the columns: - \"Mass values\": the sequence of m/z values as a list or numpy array. - \"Intensities\": the instensities corresponding to the m/z values in \"Mass values\". (This should be the same length as \"Mass values\".) - \"MS/MS m/z\": the precursor mass-to-charge ratio. - \"Charge\": the precursor charge start_predictions_path (str): Path to the csv holding initial predictions to be refined by the diffusion model. This should be index-aligned with the Polars `.ipc` file specified in `input_path` and the predictions should be in a column called \"Predictions\" as an already tokenized list of strings. model_path (str): Path to the model checkpoint. This should be a directory (so it should first be unzipped if it is a zip file). output_path (str): Path where the output should be written. This should include the filename which should be a `.csv` file. batch_size (int): Batch size to use during decoding. This will only affect the speed and the memory used. device (str): Device on which to load the model and data. Any device type supported by `Pytorch` (e.g. \"cpu\", \"cuda\", \"cuda:0\"). Please note this code has only been tested on CPU and CUDA GPUs. \"\"\" logger = logging . Logger ( name = \"diffusion/predict\" , level = logging . INFO ) # 1. Load model logger . info ( \"Loading model.\" ) model = MultinomialDiffusion . load ( path = model_path ) model = model . to ( device = device ) # 2. Initialize decoder logger . info ( \"Initializing decoder.\" ) decoder = DiffusionDecoder ( model = model ) # 3. Load data logger . info ( \"Loading data.\" ) logger . info ( \"Loading residues.\" ) residue_masses = yaml . safe_load ( open ( os . path . join ( model_path , \"residues.yaml\" ))) residues = ResidueSet ( residue_masses = residue_masses ) logger . info ( f \"Loading input data from { input_path } .\" ) input_data = polars . read_ipc ( input_path ) logger . info ( f \"Loading predictions from { start_predictions_path } .\" ) start_predictions = pandas . read_csv ( start_predictions_path ) input_dataset = AnnotatedPolarsSpectrumDataset ( data_frame = input_data , peptides = start_predictions [ \"Predictions\" ] . tolist () ) data_loader = torch . utils . data . DataLoader ( input_dataset , shuffle = False , batch_size = batch_size , collate_fn = collate_batches ( residues = residues , time_steps = model . time_steps , annotated = True , max_length = model . config . max_length , ), ) # 4. Elicit predictions logger . info ( \"Performing decoding.\" ) results = [] all_log_probs = [] with torch . no_grad (): for spectra , spectra_padding_mask , precursors , peptides , _ in tqdm . tqdm ( iter ( data_loader ), total = len ( data_loader ) ): predictions , log_probs = decoder . decode ( initial_sequence = peptides . to ( device ), spectra = spectra . to ( device ), spectra_padding_mask = spectra_padding_mask . to ( device ), precursors = precursors . to ( device ), start_step = DIFFUSION_START_STEP , ) predictions = [ prediction if \"$\" not in prediction else prediction [: prediction . index ( \"$\" )] for prediction in predictions ] predictions = [ \"\" . join ( prediction ) for prediction in predictions ] results . extend ( predictions ) all_log_probs . extend ( log_probs ) # 5. Save predictions logger . info ( \"Saving predictions.\" ) output = input_data . to_pandas () output [ \"diffusion_predictions\" ] = results output [ \"diffusion_log_probs\" ] = all_log_probs output . to_csv ( output_path )","title":"main()"},{"location":"reference/inference/","text":"","title":"Index"},{"location":"reference/inference/beam_search/","text":"BeamSearchDecoder ( model , mass_scale = MASS_SCALE ) Bases: Decoder A class for decoding from de novo sequence models using beam search. This class conforms to the Decoder interface and decodes from models that conform to the Decodable interface. Source code in instanovo/inference/beam_search.py 103 104 105 def __init__ ( self , model : Decodable , mass_scale : int = MASS_SCALE ): super () . __init__ ( model = model ) self . mass_scale = mass_scale decode ( spectra , precursors , beam_size , max_length , mass_tolerance = 5e-05 , max_isotope = 1 , return_all_beams = False ) Decode predicted residue sequence for a batch of spectra using beam search. Parameters: Name Type Description Default spectra FloatTensor The spectra to be sequenced. required precursors torch.FloatTensor[batch size, 3] The precursor mass, charge and mass-to-charge ratio. required beam_size int The maximum size of the beam. required max_length int The maximum length of a residue sequence. required mass_tolerance float The maximum relative error for which a predicted sequence is still considered to have matched the precursor mass. 5e-05 max_isotope int The maximum number of additional neutrons for isotopes whose mass a predicted sequence's mass is considered when comparing to the precursor mass. All additional nucleon numbers from 1 to max_isotope inclusive are considered. 1 return_all_beams bool Optionally return all beam-search results, not only the best beam. False Returns: Type Description list [ Any ] list[list[str]]: The predicted sequence as a list of residue tokens. This method will return an empty list for each spectrum in the batch where decoding fails i.e. no sequence that fits the precursor mass to within a tolerance is found. Source code in instanovo/inference/beam_search.py 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 def decode ( # type:ignore self , spectra : torch . FloatTensor , precursors : torch . FloatTensor , beam_size : int , max_length : int , mass_tolerance : float = 5e-5 , max_isotope : int = 1 , return_all_beams : bool = False , ) -> list [ Any ]: \"\"\"Decode predicted residue sequence for a batch of spectra using beam search. Args: spectra (torch.FloatTensor): The spectra to be sequenced. precursors (torch.FloatTensor[batch size, 3]): The precursor mass, charge and mass-to-charge ratio. beam_size (int): The maximum size of the beam. max_length (int): The maximum length of a residue sequence. mass_tolerance (float): The maximum relative error for which a predicted sequence is still considered to have matched the precursor mass. max_isotope (int): The maximum number of additional neutrons for isotopes whose mass a predicted sequence's mass is considered when comparing to the precursor mass. All additional nucleon numbers from 1 to `max_isotope` inclusive are considered. return_all_beams (bool): Optionally return all beam-search results, not only the best beam. Returns: list[list[str]]: The predicted sequence as a list of residue tokens. This method will return an empty list for each spectrum in the batch where decoding fails i.e. no sequence that fits the precursor mass to within a tolerance is found. \"\"\" with torch . no_grad (): batch_size = spectra . shape [ 0 ] complete_items : list [ list [ ScoredSequence ]] = [[] for _ in range ( batch_size )] # Precompute mass matrix and mass buffers residue_masses = self . model . get_residue_masses ( mass_scale = self . mass_scale ) . to ( spectra . device ) num_residues = residue_masses . shape [ - 1 ] mass_buffers = ( ( self . mass_scale * mass_tolerance * precursors [:, PrecursorDimension . PRECURSOR_MASS . value ] ) . round () . long () ) # Initialize beam beam : BeamState = self . init_beam ( spectra = spectra , precursor_mass_charge = precursors , residue_masses = residue_masses . unsqueeze ( 0 ) . expand ( batch_size , num_residues ), beam_size = beam_size , mass_buffers = mass_buffers , ) for _ in range ( max_length ): if beam . is_empty (): break assert beam . log_probabilities is not None if beam . log_probabilities . isinf () . all (): break # 1. Expand candidates log_probabilities , remaining_masses = self . expand_candidates ( beam_state = beam , residue_masses = residue_masses ) # 2. Filter complete items and prune incomplete ones to get the new beam complete_candidates , beam = self . filter_items ( log_probabilities = log_probabilities , beam_state = beam , beam_size = beam_size , remaining_masses = remaining_masses , mass_buffer = mass_buffers , max_isotope = max_isotope , ) for i , items in enumerate ( complete_candidates ): complete_items [ i ] . extend ( items ) for items in complete_items : items . sort ( key = lambda item : item . log_probability , reverse = True ) if not return_all_beams : sequences = [ items [ 0 ] if len ( items ) > 0 else [] for items in complete_items ] else : sequences = [ items if len ( items ) > 0 else [] for items in complete_items ] return sequences expand_candidates ( beam_state , residue_masses ) Calculate log probabilities for all candidate next tokens for all sequences in the current beam. Parameters: Name Type Description Default beam_state BeamState The current beam state required Returns: Name Type Description Result torch.FloatTensor [beam size, vocabulary size] FloatTensor The tensor of log probabilities on the candidate next tokens for FloatTensor each sequence in the beam for each spectrum in the batch. FloatTensor Result[i, j, k] is the log probability of token k in the vocabulary FloatTensor being the next token given sequence i in the beam for batch spectrum FloatTensor j is the prefix. FloatTensor exp(Result[i, j]).sum() == 1 for all i and j . Source code in instanovo/inference/beam_search.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def expand_candidates ( self , beam_state : BeamState , residue_masses : torch . LongTensor ) -> torch . FloatTensor : \"\"\"Calculate log probabilities for all candidate next tokens for all sequences in the current beam. Args: beam_state (BeamState): The current beam state Returns: Result (torch.FloatTensor [beam size, vocabulary size]): The tensor of log probabilities on the candidate next tokens for each sequence in the beam for each spectrum in the batch. Result[i, j, k] is the log probability of token `k` in the vocabulary being the next token given sequence `i` in the beam for batch spectrum `j` is the prefix. `exp(Result[i, j]).sum() == 1` for all `i` and `j`. \"\"\" assert beam_state . remaining_masses is not None remaining_masses = beam_state . remaining_masses . unsqueeze ( - 1 ) - residue_masses . unsqueeze ( 0 ) . unsqueeze ( 0 ) assert beam_state . sequences is not None sequence_length = beam_state . sequences . shape [ - 1 ] spectrum_length = beam_state . spectrum_encoding . shape [ 2 ] hidden_dim = beam_state . spectrum_encoding . shape [ 3 ] log_probabilities = self . model . score_candidates ( beam_state . sequences . reshape ( - 1 , sequence_length ), beam_state . precursor_mass_charge . reshape ( - 1 , PRECURSOR_DIM ), beam_state . spectrum_encoding . reshape ( - 1 , spectrum_length , hidden_dim ), beam_state . spectrum_mask . reshape ( - 1 , spectrum_length ), ) assert beam_state . log_probabilities is not None batch_size = beam_state . log_probabilities . shape [ 0 ] beam_size = beam_state . log_probabilities . shape [ 1 ] candidate_log_probabilities = log_probabilities . reshape ( batch_size , beam_size , - 1 ) + beam_state . log_probabilities . unsqueeze ( - 1 ) return candidate_log_probabilities , remaining_masses filter_items ( beam_state , log_probabilities , beam_size , remaining_masses , mass_buffer , max_isotope ) Separate and prune incomplete and complete sequences. Separate candidate residues into those that lead to incomplete sequences and those that lead to complete sequences. Prune the ones leading to incomplete sequences down to the top beam_size and simply return the ones leading to complete sequences. Parameters: Name Type Description Default beam_state BeamState The current beam state. required log_probabilities torch.FloatTensor[batch size, beam size, number of residues] The candidate log probabilities for each item on the beam and each potential next residue for batch spectrum in the batch. required beam_size int The maximum size of the beam. required remaining_masses torch.FloatTensor[number of residues] The masses of the residues in the vocabulary as integers in units of the mass scale. required mass_buffer torch.FloatTensor[batch size] The maximum absolute difference between the batch precursor masses and the theoretical masses of their predicted sequences. required Returns: Type Description tuple [ list [ list [ ScoredSequence ]], BeamState ] tuple[list[ScoredSequence], BeamState]: A (potentially empty) list of completed sequences and the next beam state resulting from pruning incomplete sequences. Source code in instanovo/inference/beam_search.py 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 def filter_items ( self , beam_state : BeamState , log_probabilities : torch . FloatTensor , beam_size : int , remaining_masses : torch . LongTensor , mass_buffer : torch . LongTensor , max_isotope : int , ) -> tuple [ list [ list [ ScoredSequence ]], BeamState ]: \"\"\"Separate and prune incomplete and complete sequences. Separate candidate residues into those that lead to incomplete sequences and those that lead to complete sequences. Prune the ones leading to incomplete sequences down to the top `beam_size` and simply return the ones leading to complete sequences. Args: beam_state (BeamState): The current beam state. log_probabilities (torch.FloatTensor[batch size, beam size, number of residues]): The candidate log probabilities for each item on the beam and each potential next residue for batch spectrum in the batch. beam_size (int): The maximum size of the beam. remaining_masses (torch.FloatTensor[number of residues]): The masses of the residues in the vocabulary as integers in units of the mass scale. mass_buffer (torch.FloatTensor[batch size]): The maximum absolute difference between the batch precursor masses and the theoretical masses of their predicted sequences. Returns: tuple[list[ScoredSequence], BeamState]: A (potentially empty) list of completed sequences and the next beam state resulting from pruning incomplete sequences. \"\"\" assert beam_state . remaining_masses is not None reshaped_mass_buffer = mass_buffer . unsqueeze ( - 1 ) . unsqueeze ( - 1 ) batch_size , beam_size , num_residues = log_probabilities . shape # Collect completed items completed_items : list [ list [ ScoredSequence ]] = [[] for _ in range ( batch_size )] is_eos = ( one_hot ( torch . tensor ([ self . model . get_eos_index ()]) . unsqueeze ( 0 ) . expand ( batch_size , beam_size ), num_classes = num_residues , ) . bool () . to ( log_probabilities . device ) ) item_is_complete = ( reshaped_mass_buffer >= remaining_masses ) & ( remaining_masses >= - reshaped_mass_buffer ) if max_isotope > 0 : for num_nucleons in range ( 1 , max_isotope + 1 ): isotope_is_complete = ( reshaped_mass_buffer >= remaining_masses - num_nucleons * round ( self . mass_scale * CARBON_MASS_DELTA ) ) & ( remaining_masses - num_nucleons * round ( self . mass_scale * CARBON_MASS_DELTA ) >= - reshaped_mass_buffer ) item_is_complete = item_is_complete | isotope_is_complete item_is_complete = item_is_complete & ~ is_eos & log_probabilities . isfinite () assert beam_state . sequences is not None local_variables = zip ( item_is_complete , remaining_masses , log_probabilities , beam_state . sequences ) for batch , ( is_complete , mass_errors , local_log_probabilities , sequences ) in enumerate ( local_variables ): if is_complete . any () . item (): beam_index , residues = torch . where ( is_complete ) completed_sequences = torch . column_stack (( sequences [ beam_index ], residues )) eos_log_probabilities = self . model . score_candidates ( completed_sequences , beam_state . precursor_mass_charge [ batch , beam_index ], beam_state . spectrum_encoding [ batch , beam_index ], beam_state . spectrum_mask [ batch , beam_index ], ) completed_log_probabilities = ( local_log_probabilities [ beam_index , residues ] + eos_log_probabilities [:, self . model . get_eos_index ()] ) completed_mass_errors = mass_errors [ beam_index , residues ] completed_items [ batch ] . extend ( ScoredSequence ( sequence = self . model . decode ( sequence ), mass_error = mass_error . item () / self . mass_scale , log_probability = log_probability , ) for sequence , mass_error , log_probability in zip ( completed_sequences , completed_mass_errors , completed_log_probabilities . tolist (), ) ) # Filter invalid items log_probabilities = self . prefilter_items ( log_probabilities = log_probabilities , remaining_masses = remaining_masses , beam_masses = beam_state . remaining_masses , mass_buffer = reshaped_mass_buffer , max_isotope = max_isotope , ) # Prune incomplete items to form next beam beam_log_probabilities , beam_indices = log_probabilities . reshape ( batch_size , - 1 ) . topk ( k = beam_size ) beam_sequences = self . _append_next_token ( indices = beam_indices , outer_dim = num_residues , sequences = beam_state . sequences ) remaining_masses = remaining_masses . reshape ( batch_size , - 1 ) beam_remaining_masses = [] for local_remaining_masses , local_indices in zip ( remaining_masses , beam_indices ): beam_remaining_masses . append ( local_remaining_masses [ local_indices ]) beam_remaining_masses = torch . stack ( beam_remaining_masses ) new_beam = BeamState ( sequences = beam_sequences , log_probabilities = beam_log_probabilities , remaining_masses = beam_remaining_masses , precursor_mass_charge = beam_state . precursor_mass_charge , spectrum_encoding = beam_state . spectrum_encoding , spectrum_mask = beam_state . spectrum_mask , ) return completed_items , new_beam init_beam ( spectra , precursor_mass_charge , residue_masses , mass_buffers , beam_size ) Construct the initial beam state. This means precomputing the spectrum embeddings and adding the first set of candidate tokens to the beam. Parameters: Name Type Description Default spectra torch.FloatTensor[] The spectra to be sequenced. required precursor_mass_charge torch.FloatTensor[batch size, 3] The precursor mass, charge and mass-to-charge ratio. required residue_masses torch.LongTensor[] The masses of the residues in the vocabulary as integers in units of the mass scale. required beam_size int The maximum size of the beam. required Returns: Name Type Description BeamState BeamState The initial beam state. Source code in instanovo/inference/beam_search.py 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 def init_beam ( self , spectra : torch . FloatTensor , precursor_mass_charge : torch . FloatTensor , residue_masses : torch . LongTensor , mass_buffers : torch . LongTensor , beam_size : int , ) -> BeamState : \"\"\"Construct the initial beam state. This means precomputing the spectrum embeddings and adding the first set of candidate tokens to the beam. Args: spectra (torch.FloatTensor[]): The spectra to be sequenced. precursor_mass_charge (torch.FloatTensor[batch size, 3]): The precursor mass, charge and mass-to-charge ratio. residue_masses (torch.LongTensor[]): The masses of the residues in the vocabulary as integers in units of the mass scale. beam_size (int): The maximum size of the beam. Returns: BeamState: The initial beam state. \"\"\" # 1. Compute spectrum encoding and masks ( spectrum_encoding , spectrum_mask ), log_probabilities = self . model . init ( spectra , precursor_mass_charge ) # 2. Calculate candidate residue masses and remaining mass budgets precursor_masses = ( torch . round ( self . mass_scale * precursor_mass_charge [:, PrecursorDimension . PRECURSOR_MASS . value ] ) . type ( INTEGER ) . to ( spectra . device ) ) precursor_masses = precursor_masses - round ( self . mass_scale * H2O_MASS ) log_probabilities = self . _init_prefilter ( precursor_masses = precursor_masses , log_probabilities = log_probabilities , mass_buffer = mass_buffers , ) beam_log_probabilities = log_probabilities . topk ( k = beam_size ) beam_masses = residue_masses . to ( spectra . device ) . gather ( - 1 , beam_log_probabilities . indices ) remaining_masses = precursor_masses . unsqueeze ( - 1 ) - beam_masses # 3. Copy inputs for beam candidates beam_precursor_mass_charge = precursor_mass_charge . unsqueeze ( 1 ) . expand ( - 1 , beam_size , - 1 ) beam_spectrum_encoding = spectrum_encoding . unsqueeze ( 1 ) . expand ( - 1 , beam_size , - 1 , - 1 ) beam_spectrum_mask = spectrum_mask . unsqueeze ( 1 ) . expand ( - 1 , beam_size , - 1 ) return BeamState ( sequences = beam_log_probabilities . indices . unsqueeze ( - 1 ), log_probabilities = beam_log_probabilities . values , remaining_masses = remaining_masses , precursor_mass_charge = beam_precursor_mass_charge , spectrum_encoding = beam_spectrum_encoding , spectrum_mask = beam_spectrum_mask , ) prefilter_items ( log_probabilities , remaining_masses , beam_masses , mass_buffer , max_isotope ) Filter illegal next token by setting the corresponding log probabilities to -inf . Parameters: Name Type Description Default log_probabilities torch.FloatTensor[batch size, beam size, number of residues] The candidate log probabilities for each item on the beam and each potential next residue for batch spectrum in the batch. required remaining_masses torch.LongTensor[batch size, beam size] required mass_buffer torch.LongTensor[batch size, 1, 1] description required Returns: Type Description FloatTensor torch.FloatTensor: description Source code in instanovo/inference/beam_search.py 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 def prefilter_items ( self , log_probabilities : torch . FloatTensor , remaining_masses : torch . LongTensor , beam_masses : torch . LongTensor , mass_buffer : torch . LongTensor , max_isotope : int , ) -> torch . FloatTensor : \"\"\"Filter illegal next token by setting the corresponding log probabilities to `-inf`. Args: log_probabilities (torch.FloatTensor[batch size, beam size, number of residues]): The candidate log probabilities for each item on the beam and each potential next residue for batch spectrum in the batch. remaining_masses (torch.LongTensor[batch size, beam size]): mass_buffer (torch.LongTensor[batch size, 1, 1]): _description_ Returns: torch.FloatTensor: _description_ \"\"\" # Filter out the end of sequence token since # it is added manually when a sequence's mass # means it's complete EOS_TOKEN = self . model . get_eos_index () # noqa: N806 log_probabilities [:, :, EOS_TOKEN ] = - float ( \"inf\" ) # Filter out the empty token EMPTY_TOKEN = self . model . get_empty_index () # noqa: N806 log_probabilities [:, :, EMPTY_TOKEN ] = - float ( \"inf\" ) # Filter out large masses mass_is_invalid = remaining_masses < - mass_buffer log_probabilities [ mass_is_invalid ] = - float ( \"inf\" ) return log_probabilities unravel_index ( indices , outer_dim ) staticmethod Get row and column coordinates for indices on a pair of dimensions that have been flattened. Parameters: Name Type Description Default indices LongTensor The flattened indices to unravel required outer_dim int The outermost dimension of the pair that has been flattened required Returns: Type Description tuple [ LongTensor , LongTensor ] tuple[torch.LongTensor, torch.LongTensor]: The rows and columns of the indices respectively Source code in instanovo/inference/beam_search.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 @staticmethod def unravel_index ( indices : torch . LongTensor , outer_dim : int ) -> tuple [ torch . LongTensor , torch . LongTensor ]: \"\"\"Get row and column coordinates for indices on a pair of dimensions that have been flattened. Args: indices (torch.LongTensor): The flattened indices to unravel outer_dim (int): The outermost dimension of the pair that has been flattened Returns: tuple[torch.LongTensor, torch.LongTensor]: The rows and columns of the indices respectively \"\"\" rows = indices . div ( outer_dim , rounding_mode = \"floor\" ) columns = indices . remainder ( outer_dim ) return rows , columns BeamState dataclass This class holds a specification of the beam state during beam search. Parameters: Name Type Description Default sequences torch.LongTensor[beam size, num. steps] A tensor of the partial sequences on the beam represented as indices into a residue vocabulary. required log_probabilities torch.FloatTensor[beam size] The log probabilities of each of the partial sequences on the beam. required remaining_masses torch.LongTensor[beam size] The remaining mass still to be allocated before each of the partial sequences are complete, represented as parts per MASS_SCALE. This is defined as the upper error limit on the precursor mass minus the partial sequence's theoretical mass. required precursor_mass_charge torch.FloatTensor[beam size, 3] The precursor mass, charge and mass-to-charge ratio respectively. required spectrum_encoding torch.FloatTensor[batch size, sequence length, hidden dim] The spectrum embedding. These are the outputs of running a transformer peptide encoder on the spectra. required spectrum_mask torch.BoolTensor[batch size, sequence length] The padding mask of the spectrum embedding. Indices in the original sequence map to 1 and padding tokens map to 0. required is_empty () Check whether the beam is empty. Source code in instanovo/inference/beam_search.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def is_empty ( self ) -> bool : \"\"\"Check whether the beam is empty.\"\"\" if self . sequences is None : if ( self . log_probabilities is not None ) or ( self . remaining_masses is not None ): raise ValueError ( f \"\"\"Sequences, log_probabilities and remaining masses should all be None or all not None. Sequences is None while log_probabilities is { self . log_probabilities } and remaining masses is { self . remaining_masses } . \"\"\" ) return True else : if ( self . log_probabilities is None ) or ( self . remaining_masses is None ): raise ValueError ( f \"\"\"Sequences, log_probabilities and remaining masses should all be None or all not None. Sequences is not None while log_probabilities is { self . log_probabilities } and remaining masses is { self . remaining_masses } . \"\"\" ) return False ScoredSequence dataclass This class holds a residue sequence and its log probability.","title":"Beam search"},{"location":"reference/inference/beam_search/#instanovo.inference.beam_search.BeamSearchDecoder","text":"Bases: Decoder A class for decoding from de novo sequence models using beam search. This class conforms to the Decoder interface and decodes from models that conform to the Decodable interface. Source code in instanovo/inference/beam_search.py 103 104 105 def __init__ ( self , model : Decodable , mass_scale : int = MASS_SCALE ): super () . __init__ ( model = model ) self . mass_scale = mass_scale","title":"BeamSearchDecoder"},{"location":"reference/inference/beam_search/#instanovo.inference.beam_search.BeamSearchDecoder.decode","text":"Decode predicted residue sequence for a batch of spectra using beam search. Parameters: Name Type Description Default spectra FloatTensor The spectra to be sequenced. required precursors torch.FloatTensor[batch size, 3] The precursor mass, charge and mass-to-charge ratio. required beam_size int The maximum size of the beam. required max_length int The maximum length of a residue sequence. required mass_tolerance float The maximum relative error for which a predicted sequence is still considered to have matched the precursor mass. 5e-05 max_isotope int The maximum number of additional neutrons for isotopes whose mass a predicted sequence's mass is considered when comparing to the precursor mass. All additional nucleon numbers from 1 to max_isotope inclusive are considered. 1 return_all_beams bool Optionally return all beam-search results, not only the best beam. False Returns: Type Description list [ Any ] list[list[str]]: The predicted sequence as a list of residue tokens. This method will return an empty list for each spectrum in the batch where decoding fails i.e. no sequence that fits the precursor mass to within a tolerance is found. Source code in instanovo/inference/beam_search.py 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 def decode ( # type:ignore self , spectra : torch . FloatTensor , precursors : torch . FloatTensor , beam_size : int , max_length : int , mass_tolerance : float = 5e-5 , max_isotope : int = 1 , return_all_beams : bool = False , ) -> list [ Any ]: \"\"\"Decode predicted residue sequence for a batch of spectra using beam search. Args: spectra (torch.FloatTensor): The spectra to be sequenced. precursors (torch.FloatTensor[batch size, 3]): The precursor mass, charge and mass-to-charge ratio. beam_size (int): The maximum size of the beam. max_length (int): The maximum length of a residue sequence. mass_tolerance (float): The maximum relative error for which a predicted sequence is still considered to have matched the precursor mass. max_isotope (int): The maximum number of additional neutrons for isotopes whose mass a predicted sequence's mass is considered when comparing to the precursor mass. All additional nucleon numbers from 1 to `max_isotope` inclusive are considered. return_all_beams (bool): Optionally return all beam-search results, not only the best beam. Returns: list[list[str]]: The predicted sequence as a list of residue tokens. This method will return an empty list for each spectrum in the batch where decoding fails i.e. no sequence that fits the precursor mass to within a tolerance is found. \"\"\" with torch . no_grad (): batch_size = spectra . shape [ 0 ] complete_items : list [ list [ ScoredSequence ]] = [[] for _ in range ( batch_size )] # Precompute mass matrix and mass buffers residue_masses = self . model . get_residue_masses ( mass_scale = self . mass_scale ) . to ( spectra . device ) num_residues = residue_masses . shape [ - 1 ] mass_buffers = ( ( self . mass_scale * mass_tolerance * precursors [:, PrecursorDimension . PRECURSOR_MASS . value ] ) . round () . long () ) # Initialize beam beam : BeamState = self . init_beam ( spectra = spectra , precursor_mass_charge = precursors , residue_masses = residue_masses . unsqueeze ( 0 ) . expand ( batch_size , num_residues ), beam_size = beam_size , mass_buffers = mass_buffers , ) for _ in range ( max_length ): if beam . is_empty (): break assert beam . log_probabilities is not None if beam . log_probabilities . isinf () . all (): break # 1. Expand candidates log_probabilities , remaining_masses = self . expand_candidates ( beam_state = beam , residue_masses = residue_masses ) # 2. Filter complete items and prune incomplete ones to get the new beam complete_candidates , beam = self . filter_items ( log_probabilities = log_probabilities , beam_state = beam , beam_size = beam_size , remaining_masses = remaining_masses , mass_buffer = mass_buffers , max_isotope = max_isotope , ) for i , items in enumerate ( complete_candidates ): complete_items [ i ] . extend ( items ) for items in complete_items : items . sort ( key = lambda item : item . log_probability , reverse = True ) if not return_all_beams : sequences = [ items [ 0 ] if len ( items ) > 0 else [] for items in complete_items ] else : sequences = [ items if len ( items ) > 0 else [] for items in complete_items ] return sequences","title":"decode()"},{"location":"reference/inference/beam_search/#instanovo.inference.beam_search.BeamSearchDecoder.expand_candidates","text":"Calculate log probabilities for all candidate next tokens for all sequences in the current beam. Parameters: Name Type Description Default beam_state BeamState The current beam state required Returns: Name Type Description Result torch.FloatTensor [beam size, vocabulary size] FloatTensor The tensor of log probabilities on the candidate next tokens for FloatTensor each sequence in the beam for each spectrum in the batch. FloatTensor Result[i, j, k] is the log probability of token k in the vocabulary FloatTensor being the next token given sequence i in the beam for batch spectrum FloatTensor j is the prefix. FloatTensor exp(Result[i, j]).sum() == 1 for all i and j . Source code in instanovo/inference/beam_search.py 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 def expand_candidates ( self , beam_state : BeamState , residue_masses : torch . LongTensor ) -> torch . FloatTensor : \"\"\"Calculate log probabilities for all candidate next tokens for all sequences in the current beam. Args: beam_state (BeamState): The current beam state Returns: Result (torch.FloatTensor [beam size, vocabulary size]): The tensor of log probabilities on the candidate next tokens for each sequence in the beam for each spectrum in the batch. Result[i, j, k] is the log probability of token `k` in the vocabulary being the next token given sequence `i` in the beam for batch spectrum `j` is the prefix. `exp(Result[i, j]).sum() == 1` for all `i` and `j`. \"\"\" assert beam_state . remaining_masses is not None remaining_masses = beam_state . remaining_masses . unsqueeze ( - 1 ) - residue_masses . unsqueeze ( 0 ) . unsqueeze ( 0 ) assert beam_state . sequences is not None sequence_length = beam_state . sequences . shape [ - 1 ] spectrum_length = beam_state . spectrum_encoding . shape [ 2 ] hidden_dim = beam_state . spectrum_encoding . shape [ 3 ] log_probabilities = self . model . score_candidates ( beam_state . sequences . reshape ( - 1 , sequence_length ), beam_state . precursor_mass_charge . reshape ( - 1 , PRECURSOR_DIM ), beam_state . spectrum_encoding . reshape ( - 1 , spectrum_length , hidden_dim ), beam_state . spectrum_mask . reshape ( - 1 , spectrum_length ), ) assert beam_state . log_probabilities is not None batch_size = beam_state . log_probabilities . shape [ 0 ] beam_size = beam_state . log_probabilities . shape [ 1 ] candidate_log_probabilities = log_probabilities . reshape ( batch_size , beam_size , - 1 ) + beam_state . log_probabilities . unsqueeze ( - 1 ) return candidate_log_probabilities , remaining_masses","title":"expand_candidates()"},{"location":"reference/inference/beam_search/#instanovo.inference.beam_search.BeamSearchDecoder.filter_items","text":"Separate and prune incomplete and complete sequences. Separate candidate residues into those that lead to incomplete sequences and those that lead to complete sequences. Prune the ones leading to incomplete sequences down to the top beam_size and simply return the ones leading to complete sequences. Parameters: Name Type Description Default beam_state BeamState The current beam state. required log_probabilities torch.FloatTensor[batch size, beam size, number of residues] The candidate log probabilities for each item on the beam and each potential next residue for batch spectrum in the batch. required beam_size int The maximum size of the beam. required remaining_masses torch.FloatTensor[number of residues] The masses of the residues in the vocabulary as integers in units of the mass scale. required mass_buffer torch.FloatTensor[batch size] The maximum absolute difference between the batch precursor masses and the theoretical masses of their predicted sequences. required Returns: Type Description tuple [ list [ list [ ScoredSequence ]], BeamState ] tuple[list[ScoredSequence], BeamState]: A (potentially empty) list of completed sequences and the next beam state resulting from pruning incomplete sequences. Source code in instanovo/inference/beam_search.py 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 def filter_items ( self , beam_state : BeamState , log_probabilities : torch . FloatTensor , beam_size : int , remaining_masses : torch . LongTensor , mass_buffer : torch . LongTensor , max_isotope : int , ) -> tuple [ list [ list [ ScoredSequence ]], BeamState ]: \"\"\"Separate and prune incomplete and complete sequences. Separate candidate residues into those that lead to incomplete sequences and those that lead to complete sequences. Prune the ones leading to incomplete sequences down to the top `beam_size` and simply return the ones leading to complete sequences. Args: beam_state (BeamState): The current beam state. log_probabilities (torch.FloatTensor[batch size, beam size, number of residues]): The candidate log probabilities for each item on the beam and each potential next residue for batch spectrum in the batch. beam_size (int): The maximum size of the beam. remaining_masses (torch.FloatTensor[number of residues]): The masses of the residues in the vocabulary as integers in units of the mass scale. mass_buffer (torch.FloatTensor[batch size]): The maximum absolute difference between the batch precursor masses and the theoretical masses of their predicted sequences. Returns: tuple[list[ScoredSequence], BeamState]: A (potentially empty) list of completed sequences and the next beam state resulting from pruning incomplete sequences. \"\"\" assert beam_state . remaining_masses is not None reshaped_mass_buffer = mass_buffer . unsqueeze ( - 1 ) . unsqueeze ( - 1 ) batch_size , beam_size , num_residues = log_probabilities . shape # Collect completed items completed_items : list [ list [ ScoredSequence ]] = [[] for _ in range ( batch_size )] is_eos = ( one_hot ( torch . tensor ([ self . model . get_eos_index ()]) . unsqueeze ( 0 ) . expand ( batch_size , beam_size ), num_classes = num_residues , ) . bool () . to ( log_probabilities . device ) ) item_is_complete = ( reshaped_mass_buffer >= remaining_masses ) & ( remaining_masses >= - reshaped_mass_buffer ) if max_isotope > 0 : for num_nucleons in range ( 1 , max_isotope + 1 ): isotope_is_complete = ( reshaped_mass_buffer >= remaining_masses - num_nucleons * round ( self . mass_scale * CARBON_MASS_DELTA ) ) & ( remaining_masses - num_nucleons * round ( self . mass_scale * CARBON_MASS_DELTA ) >= - reshaped_mass_buffer ) item_is_complete = item_is_complete | isotope_is_complete item_is_complete = item_is_complete & ~ is_eos & log_probabilities . isfinite () assert beam_state . sequences is not None local_variables = zip ( item_is_complete , remaining_masses , log_probabilities , beam_state . sequences ) for batch , ( is_complete , mass_errors , local_log_probabilities , sequences ) in enumerate ( local_variables ): if is_complete . any () . item (): beam_index , residues = torch . where ( is_complete ) completed_sequences = torch . column_stack (( sequences [ beam_index ], residues )) eos_log_probabilities = self . model . score_candidates ( completed_sequences , beam_state . precursor_mass_charge [ batch , beam_index ], beam_state . spectrum_encoding [ batch , beam_index ], beam_state . spectrum_mask [ batch , beam_index ], ) completed_log_probabilities = ( local_log_probabilities [ beam_index , residues ] + eos_log_probabilities [:, self . model . get_eos_index ()] ) completed_mass_errors = mass_errors [ beam_index , residues ] completed_items [ batch ] . extend ( ScoredSequence ( sequence = self . model . decode ( sequence ), mass_error = mass_error . item () / self . mass_scale , log_probability = log_probability , ) for sequence , mass_error , log_probability in zip ( completed_sequences , completed_mass_errors , completed_log_probabilities . tolist (), ) ) # Filter invalid items log_probabilities = self . prefilter_items ( log_probabilities = log_probabilities , remaining_masses = remaining_masses , beam_masses = beam_state . remaining_masses , mass_buffer = reshaped_mass_buffer , max_isotope = max_isotope , ) # Prune incomplete items to form next beam beam_log_probabilities , beam_indices = log_probabilities . reshape ( batch_size , - 1 ) . topk ( k = beam_size ) beam_sequences = self . _append_next_token ( indices = beam_indices , outer_dim = num_residues , sequences = beam_state . sequences ) remaining_masses = remaining_masses . reshape ( batch_size , - 1 ) beam_remaining_masses = [] for local_remaining_masses , local_indices in zip ( remaining_masses , beam_indices ): beam_remaining_masses . append ( local_remaining_masses [ local_indices ]) beam_remaining_masses = torch . stack ( beam_remaining_masses ) new_beam = BeamState ( sequences = beam_sequences , log_probabilities = beam_log_probabilities , remaining_masses = beam_remaining_masses , precursor_mass_charge = beam_state . precursor_mass_charge , spectrum_encoding = beam_state . spectrum_encoding , spectrum_mask = beam_state . spectrum_mask , ) return completed_items , new_beam","title":"filter_items()"},{"location":"reference/inference/beam_search/#instanovo.inference.beam_search.BeamSearchDecoder.init_beam","text":"Construct the initial beam state. This means precomputing the spectrum embeddings and adding the first set of candidate tokens to the beam. Parameters: Name Type Description Default spectra torch.FloatTensor[] The spectra to be sequenced. required precursor_mass_charge torch.FloatTensor[batch size, 3] The precursor mass, charge and mass-to-charge ratio. required residue_masses torch.LongTensor[] The masses of the residues in the vocabulary as integers in units of the mass scale. required beam_size int The maximum size of the beam. required Returns: Name Type Description BeamState BeamState The initial beam state. Source code in instanovo/inference/beam_search.py 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 def init_beam ( self , spectra : torch . FloatTensor , precursor_mass_charge : torch . FloatTensor , residue_masses : torch . LongTensor , mass_buffers : torch . LongTensor , beam_size : int , ) -> BeamState : \"\"\"Construct the initial beam state. This means precomputing the spectrum embeddings and adding the first set of candidate tokens to the beam. Args: spectra (torch.FloatTensor[]): The spectra to be sequenced. precursor_mass_charge (torch.FloatTensor[batch size, 3]): The precursor mass, charge and mass-to-charge ratio. residue_masses (torch.LongTensor[]): The masses of the residues in the vocabulary as integers in units of the mass scale. beam_size (int): The maximum size of the beam. Returns: BeamState: The initial beam state. \"\"\" # 1. Compute spectrum encoding and masks ( spectrum_encoding , spectrum_mask ), log_probabilities = self . model . init ( spectra , precursor_mass_charge ) # 2. Calculate candidate residue masses and remaining mass budgets precursor_masses = ( torch . round ( self . mass_scale * precursor_mass_charge [:, PrecursorDimension . PRECURSOR_MASS . value ] ) . type ( INTEGER ) . to ( spectra . device ) ) precursor_masses = precursor_masses - round ( self . mass_scale * H2O_MASS ) log_probabilities = self . _init_prefilter ( precursor_masses = precursor_masses , log_probabilities = log_probabilities , mass_buffer = mass_buffers , ) beam_log_probabilities = log_probabilities . topk ( k = beam_size ) beam_masses = residue_masses . to ( spectra . device ) . gather ( - 1 , beam_log_probabilities . indices ) remaining_masses = precursor_masses . unsqueeze ( - 1 ) - beam_masses # 3. Copy inputs for beam candidates beam_precursor_mass_charge = precursor_mass_charge . unsqueeze ( 1 ) . expand ( - 1 , beam_size , - 1 ) beam_spectrum_encoding = spectrum_encoding . unsqueeze ( 1 ) . expand ( - 1 , beam_size , - 1 , - 1 ) beam_spectrum_mask = spectrum_mask . unsqueeze ( 1 ) . expand ( - 1 , beam_size , - 1 ) return BeamState ( sequences = beam_log_probabilities . indices . unsqueeze ( - 1 ), log_probabilities = beam_log_probabilities . values , remaining_masses = remaining_masses , precursor_mass_charge = beam_precursor_mass_charge , spectrum_encoding = beam_spectrum_encoding , spectrum_mask = beam_spectrum_mask , )","title":"init_beam()"},{"location":"reference/inference/beam_search/#instanovo.inference.beam_search.BeamSearchDecoder.prefilter_items","text":"Filter illegal next token by setting the corresponding log probabilities to -inf . Parameters: Name Type Description Default log_probabilities torch.FloatTensor[batch size, beam size, number of residues] The candidate log probabilities for each item on the beam and each potential next residue for batch spectrum in the batch. required remaining_masses torch.LongTensor[batch size, beam size] required mass_buffer torch.LongTensor[batch size, 1, 1] description required Returns: Type Description FloatTensor torch.FloatTensor: description Source code in instanovo/inference/beam_search.py 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 def prefilter_items ( self , log_probabilities : torch . FloatTensor , remaining_masses : torch . LongTensor , beam_masses : torch . LongTensor , mass_buffer : torch . LongTensor , max_isotope : int , ) -> torch . FloatTensor : \"\"\"Filter illegal next token by setting the corresponding log probabilities to `-inf`. Args: log_probabilities (torch.FloatTensor[batch size, beam size, number of residues]): The candidate log probabilities for each item on the beam and each potential next residue for batch spectrum in the batch. remaining_masses (torch.LongTensor[batch size, beam size]): mass_buffer (torch.LongTensor[batch size, 1, 1]): _description_ Returns: torch.FloatTensor: _description_ \"\"\" # Filter out the end of sequence token since # it is added manually when a sequence's mass # means it's complete EOS_TOKEN = self . model . get_eos_index () # noqa: N806 log_probabilities [:, :, EOS_TOKEN ] = - float ( \"inf\" ) # Filter out the empty token EMPTY_TOKEN = self . model . get_empty_index () # noqa: N806 log_probabilities [:, :, EMPTY_TOKEN ] = - float ( \"inf\" ) # Filter out large masses mass_is_invalid = remaining_masses < - mass_buffer log_probabilities [ mass_is_invalid ] = - float ( \"inf\" ) return log_probabilities","title":"prefilter_items()"},{"location":"reference/inference/beam_search/#instanovo.inference.beam_search.BeamSearchDecoder.unravel_index","text":"Get row and column coordinates for indices on a pair of dimensions that have been flattened. Parameters: Name Type Description Default indices LongTensor The flattened indices to unravel required outer_dim int The outermost dimension of the pair that has been flattened required Returns: Type Description tuple [ LongTensor , LongTensor ] tuple[torch.LongTensor, torch.LongTensor]: The rows and columns of the indices respectively Source code in instanovo/inference/beam_search.py 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 @staticmethod def unravel_index ( indices : torch . LongTensor , outer_dim : int ) -> tuple [ torch . LongTensor , torch . LongTensor ]: \"\"\"Get row and column coordinates for indices on a pair of dimensions that have been flattened. Args: indices (torch.LongTensor): The flattened indices to unravel outer_dim (int): The outermost dimension of the pair that has been flattened Returns: tuple[torch.LongTensor, torch.LongTensor]: The rows and columns of the indices respectively \"\"\" rows = indices . div ( outer_dim , rounding_mode = \"floor\" ) columns = indices . remainder ( outer_dim ) return rows , columns","title":"unravel_index()"},{"location":"reference/inference/beam_search/#instanovo.inference.beam_search.BeamState","text":"This class holds a specification of the beam state during beam search. Parameters: Name Type Description Default sequences torch.LongTensor[beam size, num. steps] A tensor of the partial sequences on the beam represented as indices into a residue vocabulary. required log_probabilities torch.FloatTensor[beam size] The log probabilities of each of the partial sequences on the beam. required remaining_masses torch.LongTensor[beam size] The remaining mass still to be allocated before each of the partial sequences are complete, represented as parts per MASS_SCALE. This is defined as the upper error limit on the precursor mass minus the partial sequence's theoretical mass. required precursor_mass_charge torch.FloatTensor[beam size, 3] The precursor mass, charge and mass-to-charge ratio respectively. required spectrum_encoding torch.FloatTensor[batch size, sequence length, hidden dim] The spectrum embedding. These are the outputs of running a transformer peptide encoder on the spectra. required spectrum_mask torch.BoolTensor[batch size, sequence length] The padding mask of the spectrum embedding. Indices in the original sequence map to 1 and padding tokens map to 0. required","title":"BeamState"},{"location":"reference/inference/beam_search/#instanovo.inference.beam_search.BeamState.is_empty","text":"Check whether the beam is empty. Source code in instanovo/inference/beam_search.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def is_empty ( self ) -> bool : \"\"\"Check whether the beam is empty.\"\"\" if self . sequences is None : if ( self . log_probabilities is not None ) or ( self . remaining_masses is not None ): raise ValueError ( f \"\"\"Sequences, log_probabilities and remaining masses should all be None or all not None. Sequences is None while log_probabilities is { self . log_probabilities } and remaining masses is { self . remaining_masses } . \"\"\" ) return True else : if ( self . log_probabilities is None ) or ( self . remaining_masses is None ): raise ValueError ( f \"\"\"Sequences, log_probabilities and remaining masses should all be None or all not None. Sequences is not None while log_probabilities is { self . log_probabilities } and remaining masses is { self . remaining_masses } . \"\"\" ) return False","title":"is_empty()"},{"location":"reference/inference/beam_search/#instanovo.inference.beam_search.ScoredSequence","text":"This class holds a residue sequence and its log probability.","title":"ScoredSequence"},{"location":"reference/inference/diffusion/","text":"DiffusionDecoder ( model ) Class for decoding from a diffusion model by forward sampling. Source code in instanovo/inference/diffusion.py 15 16 17 18 19 def __init__ ( self , model : MultinomialDiffusion ) -> None : self . model = model self . time_steps = model . time_steps self . residues = model . residues self . loss_function = DiffusionLoss ( model = self . model ) decode ( spectra , spectra_padding_mask , precursors , initial_sequence = None , start_step = DIFFUSION_START_STEP , eval_steps = DIFFUSION_EVAL_STEPS ) Decoding predictions from a diffusion model by forward sampling. Parameters: Name Type Description Default spectra FloatTensor [ batch_size , sequence_length , 2] A batch of spectra to be decoded. required spectra_padding_mask BoolTensor [ batch_size , sequence_length ] Padding mask for a batch of variable length spectra. required precursors FloatTensor [ batch_size , 3] Precursor mass, charge and m/z for a batch of spectra. required initial_sequence None | LongTensor [ batch_size , output_sequence_length ] An initial sequence for the model to refine. If no initial sequence is provided (the value is None), will sample a random sequence from a uniform unigram model. Defaults to None. None start_step int The step at which to insert the initial sequence and start refinement. If initial_sequence is not provided, this will be set to time_steps - 1 . DIFFUSION_START_STEP Returns: Type Description tuple [ list [ list [ str ]], list [ float ]] tuple[list[list[str]], list[float]]: The decoded peptides and their log-probabilities for a batch of spectra. Source code in instanovo/inference/diffusion.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def decode ( self , spectra : torch . FloatTensor , spectra_padding_mask : torch . BoolTensor , precursors : torch . FloatTensor , initial_sequence : None | torch . LongTensor = None , start_step : int = DIFFUSION_START_STEP , eval_steps : tuple [ int , ... ] = DIFFUSION_EVAL_STEPS , ) -> tuple [ list [ list [ str ]], list [ float ]]: \"\"\"Decoding predictions from a diffusion model by forward sampling. Args: spectra (torch.FloatTensor[batch_size, sequence_length, 2]): A batch of spectra to be decoded. spectra_padding_mask (torch.BoolTensor[batch_size, sequence_length]): Padding mask for a batch of variable length spectra. precursors (torch.FloatTensor[batch_size, 3]): Precursor mass, charge and m/z for a batch of spectra. initial_sequence (None | torch.LongTensor[batch_size, output_sequence_length], optional): An initial sequence for the model to refine. If no initial sequence is provided (the value is None), will sample a random sequence from a uniform unigram model. Defaults to None. start_step (int): The step at which to insert the initial sequence and start refinement. If `initial_sequence` is not provided, this will be set to `time_steps - 1`. Returns: tuple[list[list[str]], list[float]]: The decoded peptides and their log-probabilities for a batch of spectra. \"\"\" device = spectra . device sequence_length = self . model . config . max_length batch_size , num_classes = spectra . size ( 0 ), len ( self . model . residues ) if initial_sequence is None : # Sample uniformly initial_distribution = Categorical ( torch . ones ( batch_size , sequence_length , num_classes ) / num_classes ) sample = initial_distribution . sample () . to ( device ) start_step = self . time_steps - 1 else : sample = initial_sequence peptide_mask = torch . zeros ( batch_size , sequence_length ) . bool () . to ( device ) log_probs = torch . zeros (( batch_size , sequence_length )) . to ( device ) # Sample through reverse process for t in range ( start_step , - 1 , - 1 ): times = ( t * torch . ones (( batch_size ,))) . long () . to ( spectra . device ) distribution = Categorical ( logits = self . model . reverse_distribution ( x_t = sample , time = times , spectra = spectra , spectra_padding_mask = spectra_padding_mask , precursors = precursors , x_padding_mask = peptide_mask , ) ) sample = distribution . sample () # Calculate log-probabilities as average loss across `eval_steps` losses = [] for t in eval_steps : times = ( t * torch . ones (( batch_size ,))) . long () . to ( spectra . device ) losses . append ( self . loss_function . _compute_loss ( x_0 = sample , t = times , spectra = spectra , spectra_padding_mask = spectra_padding_mask , precursors = precursors , x_padding_mask = peptide_mask , ) ) log_probs = ( - torch . stack ( losses ) . mean ( axis = 0 ) . cpu ()) . tolist () sequences = self . _extract_predictions ( sample ) return sequences , log_probs","title":"Diffusion"},{"location":"reference/inference/diffusion/#instanovo.inference.diffusion.DiffusionDecoder","text":"Class for decoding from a diffusion model by forward sampling. Source code in instanovo/inference/diffusion.py 15 16 17 18 19 def __init__ ( self , model : MultinomialDiffusion ) -> None : self . model = model self . time_steps = model . time_steps self . residues = model . residues self . loss_function = DiffusionLoss ( model = self . model )","title":"DiffusionDecoder"},{"location":"reference/inference/diffusion/#instanovo.inference.diffusion.DiffusionDecoder.decode","text":"Decoding predictions from a diffusion model by forward sampling. Parameters: Name Type Description Default spectra FloatTensor [ batch_size , sequence_length , 2] A batch of spectra to be decoded. required spectra_padding_mask BoolTensor [ batch_size , sequence_length ] Padding mask for a batch of variable length spectra. required precursors FloatTensor [ batch_size , 3] Precursor mass, charge and m/z for a batch of spectra. required initial_sequence None | LongTensor [ batch_size , output_sequence_length ] An initial sequence for the model to refine. If no initial sequence is provided (the value is None), will sample a random sequence from a uniform unigram model. Defaults to None. None start_step int The step at which to insert the initial sequence and start refinement. If initial_sequence is not provided, this will be set to time_steps - 1 . DIFFUSION_START_STEP Returns: Type Description tuple [ list [ list [ str ]], list [ float ]] tuple[list[list[str]], list[float]]: The decoded peptides and their log-probabilities for a batch of spectra. Source code in instanovo/inference/diffusion.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def decode ( self , spectra : torch . FloatTensor , spectra_padding_mask : torch . BoolTensor , precursors : torch . FloatTensor , initial_sequence : None | torch . LongTensor = None , start_step : int = DIFFUSION_START_STEP , eval_steps : tuple [ int , ... ] = DIFFUSION_EVAL_STEPS , ) -> tuple [ list [ list [ str ]], list [ float ]]: \"\"\"Decoding predictions from a diffusion model by forward sampling. Args: spectra (torch.FloatTensor[batch_size, sequence_length, 2]): A batch of spectra to be decoded. spectra_padding_mask (torch.BoolTensor[batch_size, sequence_length]): Padding mask for a batch of variable length spectra. precursors (torch.FloatTensor[batch_size, 3]): Precursor mass, charge and m/z for a batch of spectra. initial_sequence (None | torch.LongTensor[batch_size, output_sequence_length], optional): An initial sequence for the model to refine. If no initial sequence is provided (the value is None), will sample a random sequence from a uniform unigram model. Defaults to None. start_step (int): The step at which to insert the initial sequence and start refinement. If `initial_sequence` is not provided, this will be set to `time_steps - 1`. Returns: tuple[list[list[str]], list[float]]: The decoded peptides and their log-probabilities for a batch of spectra. \"\"\" device = spectra . device sequence_length = self . model . config . max_length batch_size , num_classes = spectra . size ( 0 ), len ( self . model . residues ) if initial_sequence is None : # Sample uniformly initial_distribution = Categorical ( torch . ones ( batch_size , sequence_length , num_classes ) / num_classes ) sample = initial_distribution . sample () . to ( device ) start_step = self . time_steps - 1 else : sample = initial_sequence peptide_mask = torch . zeros ( batch_size , sequence_length ) . bool () . to ( device ) log_probs = torch . zeros (( batch_size , sequence_length )) . to ( device ) # Sample through reverse process for t in range ( start_step , - 1 , - 1 ): times = ( t * torch . ones (( batch_size ,))) . long () . to ( spectra . device ) distribution = Categorical ( logits = self . model . reverse_distribution ( x_t = sample , time = times , spectra = spectra , spectra_padding_mask = spectra_padding_mask , precursors = precursors , x_padding_mask = peptide_mask , ) ) sample = distribution . sample () # Calculate log-probabilities as average loss across `eval_steps` losses = [] for t in eval_steps : times = ( t * torch . ones (( batch_size ,))) . long () . to ( spectra . device ) losses . append ( self . loss_function . _compute_loss ( x_0 = sample , t = times , spectra = spectra , spectra_padding_mask = spectra_padding_mask , precursors = precursors , x_padding_mask = peptide_mask , ) ) log_probs = ( - torch . stack ( losses ) . mean ( axis = 0 ) . cpu ()) . tolist () sequences = self . _extract_predictions ( sample ) return sequences , log_probs","title":"decode()"},{"location":"reference/inference/interfaces/","text":"Decodable An interface for models that can be decoded by algorithms that conform to the search interface. decode ( sequence ) abstractmethod Map sequences of indices to residues using the model's residue vocabulary. Parameters: Name Type Description Default sequence LongTensor The sequence of residue indices to be mapped to the corresponding residue strings. required Source code in instanovo/inference/interfaces.py 57 58 59 60 61 62 63 64 65 66 @abstractmethod def decode ( self , sequence : torch . LongTensor ) -> list [ str ]: \"\"\"Map sequences of indices to residues using the model's residue vocabulary. Args: sequence (torch.LongTensor): The sequence of residue indices to be mapped to the corresponding residue strings. \"\"\" pass get_empty_index () abstractmethod Get the empty token's index in the model's residue vocabulary. Source code in instanovo/inference/interfaces.py 73 74 75 76 @abstractmethod def get_empty_index ( self ) -> int : \"\"\"Get the empty token's index in the model's residue vocabulary.\"\"\" pass get_eos_index () abstractmethod Get the end of sequence token's index in the model's residue vocabulary. Source code in instanovo/inference/interfaces.py 68 69 70 71 @abstractmethod def get_eos_index ( self ) -> int : \"\"\"Get the end of sequence token's index in the model's residue vocabulary.\"\"\" pass get_residue_masses ( mass_scale ) abstractmethod Get residue masses for the model's residue vocabulary. Parameters: Name Type Description Default mass_scale int The scale in Daltons at which masses are calculated and rounded off. For example, a scale of 10000 would represent masses at a scale of 1e4 Da. required Source code in instanovo/inference/interfaces.py 44 45 46 47 48 49 50 51 52 53 54 55 @abstractmethod def get_residue_masses ( self , mass_scale : int ) -> torch . LongTensor : \"\"\"Get residue masses for the model's residue vocabulary. Args: mass_scale (int): The scale in Daltons at which masses are calculated and rounded off. For example, a scale of 10000 would represent masses at a scale of 1e4 Da. \"\"\" pass init ( spectra , precursors , * args , ** kwargs ) abstractmethod Initialize the search state. Parameters: Name Type Description Default spectra FloatTensor The spectra to be sequenced. required precursors torch.FloatTensor[batch size, 3] The precursor mass, charge and mass-to-charge ratio. required Source code in instanovo/inference/interfaces.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 @abstractmethod def init ( # type:ignore self , spectra : torch . FloatTensor , precursors : torch . FloatTensor , * args , ** kwargs ) -> Any : \"\"\"Initialize the search state. Args: spectra (torch.FloatTensor): The spectra to be sequenced. precursors (torch.FloatTensor[batch size, 3]): The precursor mass, charge and mass-to-charge ratio. \"\"\" pass score_candidates ( sequences , precursor_mass_charge , * args , ** kwargs ) abstractmethod Generate and score the next set of candidates. Parameters: Name Type Description Default sequences LongTensor Partial residue sequences in generated the course of decoding. required precursor_mass_charge torch.FloatTensor[batch size, 3] The precursor mass, charge and mass-to-charge ratio. required Source code in instanovo/inference/interfaces.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 @abstractmethod def score_candidates ( # type:ignore self , sequences : torch . LongTensor , precursor_mass_charge : torch . FloatTensor , * args , ** kwargs ) -> torch . FloatTensor : \"\"\"Generate and score the next set of candidates. Args: sequences (torch.LongTensor): Partial residue sequences in generated the course of decoding. precursor_mass_charge (torch.FloatTensor[batch size, 3]): The precursor mass, charge and mass-to-charge ratio. \"\"\" pass Decoder ( model ) A class that implements some search algorithm for decoding from a model that conforms to the Decodable interface. Parameters: Name Type Description Default model Decodable The model to predict residue sequences from using the implemented search algorithm. required Source code in instanovo/inference/interfaces.py 89 90 def __init__ ( self , model : Decodable ): self . model = model decode ( spectra , precursors , * args , ** kwargs ) abstractmethod Generate the predicted residue sequence using the decoder's search algorithm. Parameters: Name Type Description Default spectra FloatTensor The spectra to be sequenced. required precursors FloatTensor The precursor mass, charge and mass-to-charge ratio. required Source code in instanovo/inference/interfaces.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 @abstractmethod def decode ( # type:ignore self , spectra : torch . FloatTensor , precursors : torch . FloatTensor , * args , ** kwargs ) -> list [ list [ str ]]: \"\"\"Generate the predicted residue sequence using the decoder's search algorithm. Args: spectra (torch.FloatTensor): The spectra to be sequenced. precursors (torch.FloatTensor): The precursor mass, charge and mass-to-charge ratio. \"\"\" pass","title":"Interfaces"},{"location":"reference/inference/interfaces/#instanovo.inference.interfaces.Decodable","text":"An interface for models that can be decoded by algorithms that conform to the search interface.","title":"Decodable"},{"location":"reference/inference/interfaces/#instanovo.inference.interfaces.Decodable.decode","text":"Map sequences of indices to residues using the model's residue vocabulary. Parameters: Name Type Description Default sequence LongTensor The sequence of residue indices to be mapped to the corresponding residue strings. required Source code in instanovo/inference/interfaces.py 57 58 59 60 61 62 63 64 65 66 @abstractmethod def decode ( self , sequence : torch . LongTensor ) -> list [ str ]: \"\"\"Map sequences of indices to residues using the model's residue vocabulary. Args: sequence (torch.LongTensor): The sequence of residue indices to be mapped to the corresponding residue strings. \"\"\" pass","title":"decode()"},{"location":"reference/inference/interfaces/#instanovo.inference.interfaces.Decodable.get_empty_index","text":"Get the empty token's index in the model's residue vocabulary. Source code in instanovo/inference/interfaces.py 73 74 75 76 @abstractmethod def get_empty_index ( self ) -> int : \"\"\"Get the empty token's index in the model's residue vocabulary.\"\"\" pass","title":"get_empty_index()"},{"location":"reference/inference/interfaces/#instanovo.inference.interfaces.Decodable.get_eos_index","text":"Get the end of sequence token's index in the model's residue vocabulary. Source code in instanovo/inference/interfaces.py 68 69 70 71 @abstractmethod def get_eos_index ( self ) -> int : \"\"\"Get the end of sequence token's index in the model's residue vocabulary.\"\"\" pass","title":"get_eos_index()"},{"location":"reference/inference/interfaces/#instanovo.inference.interfaces.Decodable.get_residue_masses","text":"Get residue masses for the model's residue vocabulary. Parameters: Name Type Description Default mass_scale int The scale in Daltons at which masses are calculated and rounded off. For example, a scale of 10000 would represent masses at a scale of 1e4 Da. required Source code in instanovo/inference/interfaces.py 44 45 46 47 48 49 50 51 52 53 54 55 @abstractmethod def get_residue_masses ( self , mass_scale : int ) -> torch . LongTensor : \"\"\"Get residue masses for the model's residue vocabulary. Args: mass_scale (int): The scale in Daltons at which masses are calculated and rounded off. For example, a scale of 10000 would represent masses at a scale of 1e4 Da. \"\"\" pass","title":"get_residue_masses()"},{"location":"reference/inference/interfaces/#instanovo.inference.interfaces.Decodable.init","text":"Initialize the search state. Parameters: Name Type Description Default spectra FloatTensor The spectra to be sequenced. required precursors torch.FloatTensor[batch size, 3] The precursor mass, charge and mass-to-charge ratio. required Source code in instanovo/inference/interfaces.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 @abstractmethod def init ( # type:ignore self , spectra : torch . FloatTensor , precursors : torch . FloatTensor , * args , ** kwargs ) -> Any : \"\"\"Initialize the search state. Args: spectra (torch.FloatTensor): The spectra to be sequenced. precursors (torch.FloatTensor[batch size, 3]): The precursor mass, charge and mass-to-charge ratio. \"\"\" pass","title":"init()"},{"location":"reference/inference/interfaces/#instanovo.inference.interfaces.Decodable.score_candidates","text":"Generate and score the next set of candidates. Parameters: Name Type Description Default sequences LongTensor Partial residue sequences in generated the course of decoding. required precursor_mass_charge torch.FloatTensor[batch size, 3] The precursor mass, charge and mass-to-charge ratio. required Source code in instanovo/inference/interfaces.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 @abstractmethod def score_candidates ( # type:ignore self , sequences : torch . LongTensor , precursor_mass_charge : torch . FloatTensor , * args , ** kwargs ) -> torch . FloatTensor : \"\"\"Generate and score the next set of candidates. Args: sequences (torch.LongTensor): Partial residue sequences in generated the course of decoding. precursor_mass_charge (torch.FloatTensor[batch size, 3]): The precursor mass, charge and mass-to-charge ratio. \"\"\" pass","title":"score_candidates()"},{"location":"reference/inference/interfaces/#instanovo.inference.interfaces.Decoder","text":"A class that implements some search algorithm for decoding from a model that conforms to the Decodable interface. Parameters: Name Type Description Default model Decodable The model to predict residue sequences from using the implemented search algorithm. required Source code in instanovo/inference/interfaces.py 89 90 def __init__ ( self , model : Decodable ): self . model = model","title":"Decoder"},{"location":"reference/inference/interfaces/#instanovo.inference.interfaces.Decoder.decode","text":"Generate the predicted residue sequence using the decoder's search algorithm. Parameters: Name Type Description Default spectra FloatTensor The spectra to be sequenced. required precursors FloatTensor The precursor mass, charge and mass-to-charge ratio. required Source code in instanovo/inference/interfaces.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 @abstractmethod def decode ( # type:ignore self , spectra : torch . FloatTensor , precursors : torch . FloatTensor , * args , ** kwargs ) -> list [ list [ str ]]: \"\"\"Generate the predicted residue sequence using the decoder's search algorithm. Args: spectra (torch.FloatTensor): The spectra to be sequenced. precursors (torch.FloatTensor): The precursor mass, charge and mass-to-charge ratio. \"\"\" pass","title":"decode()"},{"location":"reference/inference/knapsack/","text":"Knapsack dataclass A class that precomputes and stores a knapsack chart. Parameters: Name Type Description Default max_mass float The maximum mass up to which the chart is calculated. required mass_scale int The scale in Daltons at which masses are calculated and rounded off. For example, a scale of 10000 would represent masses at a scale of 1e4 Da. required residues list [ str ] The list of residues that are considered in knapsack decoding. The order of this list is the inverse of residue_indices . required residue_indices dict [ str , int ] A mapping from residues as strings to indices in the knapsack chart. This is the inverse of residues . required masses numpy.ndarray[number of masses] The set of realisable masses in ascending order. required chart torch.BoolTensor[number of masses, number of residues] The chart of realisable masses and residues that can lead to these masses. chart[mass, residue] is True if and only if a sequence of mass can be generated starting with the residue with index residue . required construct_knapsack ( residue_masses , residue_indices , max_mass , mass_scale ) classmethod Construct a knapsack chart using depth-first search. Previous construction algorithms have used dynamic programming, but its space and time complexity scale linearly with mass resolution since every possible mass is iterated over rather than only the feasible masses. Graph search algorithms only iterate over feasible masses which become a smaller and smaller share of possible masses as the mass resolution increases. This leads to dramatic performance improvements. This implementation uses depth-first search since its agenda is a stack which can be implemented using python lists whose operations have amortized constant time complexity. Parameters: Name Type Description Default residue_masses dict [ str , float ] A mapping from considered residues to their masses. required max_mass float The maximum mass up to which the chart is calculated. required mass_scale int The scale in Daltons at which masses are calculated and rounded off. For example, a scale of 10000 would represent masses at a scale of 1e4 Da. required Source code in instanovo/inference/knapsack.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 @classmethod def construct_knapsack ( cls , residue_masses : dict [ str , float ], residue_indices : dict [ str , int ], max_mass : float , mass_scale : int , ) -> Knapsack : \"\"\"Construct a knapsack chart using depth-first search. Previous construction algorithms have used dynamic programming, but its space and time complexity scale linearly with mass resolution since every `possible` mass is iterated over rather than only the `feasible` masses. Graph search algorithms only iterate over `feasible` masses which become a smaller and smaller share of possible masses as the mass resolution increases. This leads to dramatic performance improvements. This implementation uses depth-first search since its agenda is a stack which can be implemented using python lists whose operations have amortized constant time complexity. Args: residue_masses (dict[str, float]): A mapping from considered residues to their masses. max_mass (float): The maximum mass up to which the chart is calculated. mass_scale (int): The scale in Daltons at which masses are calculated and rounded off. For example, a scale of 10000 would represent masses at a scale of 1e4 Da. \"\"\" # Convert the maximum mass to units of the mass scale scaled_max_mass = round ( max_mass * mass_scale ) logger . info ( \"Scaling masses.\" ) # Load residue information into appropriate data structures residues , scaled_residue_masses = [ \"\" ], {} for residue , mass in residue_masses . items (): residues . append ( residue ) if abs ( mass ) > 0 : scaled_residue_masses [ residue ] = round ( mass * mass_scale ) # Initialize the search agenda mass_dim = round ( max_mass * mass_scale ) + 1 residue_dim = max ( residue_indices . values ()) + 1 chart = np . full (( mass_dim , residue_dim ), False ) logger . info ( \"Initializing chart.\" ) agenda , visited = [], set () for residue , mass in scaled_residue_masses . items (): agenda . append ( mass ) chart [ mass , residue_indices [ residue ]] = True # Perform depth-first search logger . info ( \"Performing search.\" ) while agenda : current_mass = agenda . pop () if current_mass in visited : continue for residue , mass in scaled_residue_masses . items (): next_mass = current_mass + mass if next_mass <= scaled_max_mass : agenda . append ( next_mass ) chart [ next_mass , residue_indices [ residue ]] = True visited . add ( current_mass ) masses = np . array ( sorted ( visited )) return cls ( max_mass = max_mass , mass_scale = mass_scale , residues = residues , residue_indices = residue_indices , masses = masses , chart = chart , ) from_file ( path ) classmethod Load a knapsack saved to a directory. Parameters: Name Type Description Default path str The path to the directory. required Returns: Name Type Description _type_ Knapsack description Source code in instanovo/inference/knapsack.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 @classmethod def from_file ( cls , path : str ) -> Knapsack : \"\"\"Load a knapsack saved to a directory. Args: path (str): The path to the directory. Returns: _type_: _description_ \"\"\" max_mass , mass_scale , residues , residue_indices = pickle . load ( open ( os . path . join ( path , \"parameters.pkl\" ), \"rb\" ) ) masses = np . load ( os . path . join ( path , \"masses.npy\" )) chart = np . load ( os . path . join ( path , \"chart.npy\" )) return cls ( max_mass = max_mass , mass_scale = mass_scale , residues = residues , residue_indices = residue_indices , masses = masses , chart = chart , ) get_feasible_masses ( target_mass , tolerance ) Find a set of feasible masses for a given target mass and tolerance using binary search. Parameters: Name Type Description Default target_mass float The masses to be decoded in Daltons. required tolerance float The mass tolerance in Daltons. required Returns: Type Description list [ int ] list[int]: A list of feasible masses. Source code in instanovo/inference/knapsack.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 def get_feasible_masses ( self , target_mass : float , tolerance : float ) -> list [ int ]: \"\"\"Find a set of feasible masses for a given target mass and tolerance using binary search. Args: target_mass (float): The masses to be decoded in Daltons. tolerance (float): The mass tolerance in Daltons. Returns: list[int]: A list of feasible masses. \"\"\" scaled_min_mass = round ( self . mass_scale * ( target_mass - tolerance )) scaled_max_mass = round ( self . mass_scale * ( target_mass + tolerance )) left_endpoint = bisect . bisect_right ( self . masses , scaled_min_mass ) right_endpoint = bisect . bisect_left ( self . masses , scaled_max_mass ) feasible_masses : list [ int ] = self . masses [ left_endpoint : right_endpoint ] . tolist () return feasible_masses save ( path ) Save the knapsack file to a directory. Parameters: Name Type Description Default path str The path to the directory. required Raises: Type Description FileExistsError If the directory path already exists, this message raise an exception. Source code in instanovo/inference/knapsack.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def save ( self , path : str ) -> None : \"\"\"Save the knapsack file to a directory. Args: path (str): The path to the directory. Raises: FileExistsError: If the directory `path` already exists, this message raise an exception. \"\"\" if os . path . exists ( path ): raise FileExistsError os . mkdir ( path = path ) parameters = ( self . max_mass , self . mass_scale , self . residues , self . residue_indices ) pickle . dump ( parameters , open ( os . path . join ( path , \"parameters.pkl\" ), \"wb\" )) np . save ( os . path . join ( path , \"masses.npy\" ), self . masses ) np . save ( os . path . join ( path , \"chart.npy\" ), self . chart )","title":"Knapsack"},{"location":"reference/inference/knapsack/#instanovo.inference.knapsack.Knapsack","text":"A class that precomputes and stores a knapsack chart. Parameters: Name Type Description Default max_mass float The maximum mass up to which the chart is calculated. required mass_scale int The scale in Daltons at which masses are calculated and rounded off. For example, a scale of 10000 would represent masses at a scale of 1e4 Da. required residues list [ str ] The list of residues that are considered in knapsack decoding. The order of this list is the inverse of residue_indices . required residue_indices dict [ str , int ] A mapping from residues as strings to indices in the knapsack chart. This is the inverse of residues . required masses numpy.ndarray[number of masses] The set of realisable masses in ascending order. required chart torch.BoolTensor[number of masses, number of residues] The chart of realisable masses and residues that can lead to these masses. chart[mass, residue] is True if and only if a sequence of mass can be generated starting with the residue with index residue . required","title":"Knapsack"},{"location":"reference/inference/knapsack/#instanovo.inference.knapsack.Knapsack.construct_knapsack","text":"Construct a knapsack chart using depth-first search. Previous construction algorithms have used dynamic programming, but its space and time complexity scale linearly with mass resolution since every possible mass is iterated over rather than only the feasible masses. Graph search algorithms only iterate over feasible masses which become a smaller and smaller share of possible masses as the mass resolution increases. This leads to dramatic performance improvements. This implementation uses depth-first search since its agenda is a stack which can be implemented using python lists whose operations have amortized constant time complexity. Parameters: Name Type Description Default residue_masses dict [ str , float ] A mapping from considered residues to their masses. required max_mass float The maximum mass up to which the chart is calculated. required mass_scale int The scale in Daltons at which masses are calculated and rounded off. For example, a scale of 10000 would represent masses at a scale of 1e4 Da. required Source code in instanovo/inference/knapsack.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 @classmethod def construct_knapsack ( cls , residue_masses : dict [ str , float ], residue_indices : dict [ str , int ], max_mass : float , mass_scale : int , ) -> Knapsack : \"\"\"Construct a knapsack chart using depth-first search. Previous construction algorithms have used dynamic programming, but its space and time complexity scale linearly with mass resolution since every `possible` mass is iterated over rather than only the `feasible` masses. Graph search algorithms only iterate over `feasible` masses which become a smaller and smaller share of possible masses as the mass resolution increases. This leads to dramatic performance improvements. This implementation uses depth-first search since its agenda is a stack which can be implemented using python lists whose operations have amortized constant time complexity. Args: residue_masses (dict[str, float]): A mapping from considered residues to their masses. max_mass (float): The maximum mass up to which the chart is calculated. mass_scale (int): The scale in Daltons at which masses are calculated and rounded off. For example, a scale of 10000 would represent masses at a scale of 1e4 Da. \"\"\" # Convert the maximum mass to units of the mass scale scaled_max_mass = round ( max_mass * mass_scale ) logger . info ( \"Scaling masses.\" ) # Load residue information into appropriate data structures residues , scaled_residue_masses = [ \"\" ], {} for residue , mass in residue_masses . items (): residues . append ( residue ) if abs ( mass ) > 0 : scaled_residue_masses [ residue ] = round ( mass * mass_scale ) # Initialize the search agenda mass_dim = round ( max_mass * mass_scale ) + 1 residue_dim = max ( residue_indices . values ()) + 1 chart = np . full (( mass_dim , residue_dim ), False ) logger . info ( \"Initializing chart.\" ) agenda , visited = [], set () for residue , mass in scaled_residue_masses . items (): agenda . append ( mass ) chart [ mass , residue_indices [ residue ]] = True # Perform depth-first search logger . info ( \"Performing search.\" ) while agenda : current_mass = agenda . pop () if current_mass in visited : continue for residue , mass in scaled_residue_masses . items (): next_mass = current_mass + mass if next_mass <= scaled_max_mass : agenda . append ( next_mass ) chart [ next_mass , residue_indices [ residue ]] = True visited . add ( current_mass ) masses = np . array ( sorted ( visited )) return cls ( max_mass = max_mass , mass_scale = mass_scale , residues = residues , residue_indices = residue_indices , masses = masses , chart = chart , )","title":"construct_knapsack()"},{"location":"reference/inference/knapsack/#instanovo.inference.knapsack.Knapsack.from_file","text":"Load a knapsack saved to a directory. Parameters: Name Type Description Default path str The path to the directory. required Returns: Name Type Description _type_ Knapsack description Source code in instanovo/inference/knapsack.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 @classmethod def from_file ( cls , path : str ) -> Knapsack : \"\"\"Load a knapsack saved to a directory. Args: path (str): The path to the directory. Returns: _type_: _description_ \"\"\" max_mass , mass_scale , residues , residue_indices = pickle . load ( open ( os . path . join ( path , \"parameters.pkl\" ), \"rb\" ) ) masses = np . load ( os . path . join ( path , \"masses.npy\" )) chart = np . load ( os . path . join ( path , \"chart.npy\" )) return cls ( max_mass = max_mass , mass_scale = mass_scale , residues = residues , residue_indices = residue_indices , masses = masses , chart = chart , )","title":"from_file()"},{"location":"reference/inference/knapsack/#instanovo.inference.knapsack.Knapsack.get_feasible_masses","text":"Find a set of feasible masses for a given target mass and tolerance using binary search. Parameters: Name Type Description Default target_mass float The masses to be decoded in Daltons. required tolerance float The mass tolerance in Daltons. required Returns: Type Description list [ int ] list[int]: A list of feasible masses. Source code in instanovo/inference/knapsack.py 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 def get_feasible_masses ( self , target_mass : float , tolerance : float ) -> list [ int ]: \"\"\"Find a set of feasible masses for a given target mass and tolerance using binary search. Args: target_mass (float): The masses to be decoded in Daltons. tolerance (float): The mass tolerance in Daltons. Returns: list[int]: A list of feasible masses. \"\"\" scaled_min_mass = round ( self . mass_scale * ( target_mass - tolerance )) scaled_max_mass = round ( self . mass_scale * ( target_mass + tolerance )) left_endpoint = bisect . bisect_right ( self . masses , scaled_min_mass ) right_endpoint = bisect . bisect_left ( self . masses , scaled_max_mass ) feasible_masses : list [ int ] = self . masses [ left_endpoint : right_endpoint ] . tolist () return feasible_masses","title":"get_feasible_masses()"},{"location":"reference/inference/knapsack/#instanovo.inference.knapsack.Knapsack.save","text":"Save the knapsack file to a directory. Parameters: Name Type Description Default path str The path to the directory. required Raises: Type Description FileExistsError If the directory path already exists, this message raise an exception. Source code in instanovo/inference/knapsack.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def save ( self , path : str ) -> None : \"\"\"Save the knapsack file to a directory. Args: path (str): The path to the directory. Raises: FileExistsError: If the directory `path` already exists, this message raise an exception. \"\"\" if os . path . exists ( path ): raise FileExistsError os . mkdir ( path = path ) parameters = ( self . max_mass , self . mass_scale , self . residues , self . residue_indices ) pickle . dump ( parameters , open ( os . path . join ( path , \"parameters.pkl\" ), \"wb\" )) np . save ( os . path . join ( path , \"masses.npy\" ), self . masses ) np . save ( os . path . join ( path , \"chart.npy\" ), self . chart )","title":"save()"},{"location":"reference/inference/knapsack_beam_search/","text":"KnapsackBeamSearchDecoder ( model , knapsack ) Bases: BeamSearchDecoder A class for decoding from de novo sequencing models using beam search with knapsack filtering. Source code in instanovo/inference/knapsack_beam_search.py 15 16 17 18 19 20 21 22 def __init__ ( self , model : Decodable , knapsack : Knapsack , ): super () . __init__ ( model = model , mass_scale = knapsack . mass_scale ) self . knapsack = knapsack self . chart = torch . tensor ( self . knapsack . chart ) from_file ( model , path ) classmethod Initialize a decoder by loading a saved knapsack. Parameters: Name Type Description Default model Decodable The model to be decoded from. required path str The path to the directory where the knapsack was saved to. required Returns: Name Type Description KnapsackBeamSearchDecoder KnapsackBeamSearchDecoder The decoder. Source code in instanovo/inference/knapsack_beam_search.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 @classmethod def from_file ( cls , model : Decodable , path : str ) -> KnapsackBeamSearchDecoder : \"\"\"Initialize a decoder by loading a saved knapsack. Args: model (Decodable): The model to be decoded from. path (str): The path to the directory where the knapsack was saved to. Returns: KnapsackBeamSearchDecoder: The decoder. \"\"\" knapsack = Knapsack . from_file ( path = path ) return cls ( model = model , knapsack = knapsack ) prefilter_items ( log_probabilities , remaining_masses , beam_masses , mass_buffer , max_isotope ) Filter illegal next token by setting the corresponding log probabilities to -inf . Parameters: Name Type Description Default log_probabilities torch.FloatTensor[batch size, beam size, number of residues] The candidate log probabilities for each item on the beam and each potential next residue for batch spectrum in the batch. required remaining_masses torch.LongTensor[batch size, beam size] required mass_buffer torch.LongTensor[batch size, 1, 1] description required Returns: Type Description FloatTensor torch.FloatTensor: description Source code in instanovo/inference/knapsack_beam_search.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def prefilter_items ( self , log_probabilities : torch . FloatTensor , remaining_masses : torch . LongTensor , beam_masses : torch . LongTensor , mass_buffer : torch . LongTensor , max_isotope : int , ) -> torch . FloatTensor : \"\"\"Filter illegal next token by setting the corresponding log probabilities to `-inf`. Args: log_probabilities (torch.FloatTensor[batch size, beam size, number of residues]): The candidate log probabilities for each item on the beam and each potential next residue for batch spectrum in the batch. remaining_masses (torch.LongTensor[batch size, beam size]): mass_buffer (torch.LongTensor[batch size, 1, 1]): _description_ Returns: torch.FloatTensor: _description_ \"\"\" log_probabilities = super () . prefilter_items ( log_probabilities = log_probabilities , remaining_masses = remaining_masses , beam_masses = beam_masses , mass_buffer = mass_buffer , max_isotope = max_isotope , ) mass_lower_bound = torch . clamp ( beam_masses - mass_buffer . squeeze ( - 1 ), min = 0 ) mass_upper_bound = beam_masses + mass_buffer . squeeze ( - 1 ) batch_size , beam_size , num_residues = log_probabilities . shape scaled_nucleon_mass = round ( self . mass_scale * CARBON_MASS_DELTA ) for batch in range ( batch_size ): for beam in range ( beam_size ): beam_lower_bound = mass_lower_bound [ batch , beam ] . item () beam_upper_bound = mass_upper_bound [ batch , beam ] . item () for residue in range ( num_residues ): if log_probabilities [ batch , beam , residue ] . isfinite () . item (): valid_residue = self . chart [ beam_lower_bound : ( beam_upper_bound + 1 ), residue ] . any () if max_isotope > 0 : for num_nucleons in range ( 1 , max_isotope + 1 ): local_valid_residue = self . chart [ beam_lower_bound - num_nucleons * scaled_nucleon_mass : ( beam_upper_bound - num_nucleons * scaled_nucleon_mass + 1 ), residue , ] . any () valid_residue = valid_residue or local_valid_residue if not valid_residue : log_probabilities [ batch , beam , residue ] = - float ( \"inf\" ) return log_probabilities","title":"Knapsack beam search"},{"location":"reference/inference/knapsack_beam_search/#instanovo.inference.knapsack_beam_search.KnapsackBeamSearchDecoder","text":"Bases: BeamSearchDecoder A class for decoding from de novo sequencing models using beam search with knapsack filtering. Source code in instanovo/inference/knapsack_beam_search.py 15 16 17 18 19 20 21 22 def __init__ ( self , model : Decodable , knapsack : Knapsack , ): super () . __init__ ( model = model , mass_scale = knapsack . mass_scale ) self . knapsack = knapsack self . chart = torch . tensor ( self . knapsack . chart )","title":"KnapsackBeamSearchDecoder"},{"location":"reference/inference/knapsack_beam_search/#instanovo.inference.knapsack_beam_search.KnapsackBeamSearchDecoder.from_file","text":"Initialize a decoder by loading a saved knapsack. Parameters: Name Type Description Default model Decodable The model to be decoded from. required path str The path to the directory where the knapsack was saved to. required Returns: Name Type Description KnapsackBeamSearchDecoder KnapsackBeamSearchDecoder The decoder. Source code in instanovo/inference/knapsack_beam_search.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 @classmethod def from_file ( cls , model : Decodable , path : str ) -> KnapsackBeamSearchDecoder : \"\"\"Initialize a decoder by loading a saved knapsack. Args: model (Decodable): The model to be decoded from. path (str): The path to the directory where the knapsack was saved to. Returns: KnapsackBeamSearchDecoder: The decoder. \"\"\" knapsack = Knapsack . from_file ( path = path ) return cls ( model = model , knapsack = knapsack )","title":"from_file()"},{"location":"reference/inference/knapsack_beam_search/#instanovo.inference.knapsack_beam_search.KnapsackBeamSearchDecoder.prefilter_items","text":"Filter illegal next token by setting the corresponding log probabilities to -inf . Parameters: Name Type Description Default log_probabilities torch.FloatTensor[batch size, beam size, number of residues] The candidate log probabilities for each item on the beam and each potential next residue for batch spectrum in the batch. required remaining_masses torch.LongTensor[batch size, beam size] required mass_buffer torch.LongTensor[batch size, 1, 1] description required Returns: Type Description FloatTensor torch.FloatTensor: description Source code in instanovo/inference/knapsack_beam_search.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 def prefilter_items ( self , log_probabilities : torch . FloatTensor , remaining_masses : torch . LongTensor , beam_masses : torch . LongTensor , mass_buffer : torch . LongTensor , max_isotope : int , ) -> torch . FloatTensor : \"\"\"Filter illegal next token by setting the corresponding log probabilities to `-inf`. Args: log_probabilities (torch.FloatTensor[batch size, beam size, number of residues]): The candidate log probabilities for each item on the beam and each potential next residue for batch spectrum in the batch. remaining_masses (torch.LongTensor[batch size, beam size]): mass_buffer (torch.LongTensor[batch size, 1, 1]): _description_ Returns: torch.FloatTensor: _description_ \"\"\" log_probabilities = super () . prefilter_items ( log_probabilities = log_probabilities , remaining_masses = remaining_masses , beam_masses = beam_masses , mass_buffer = mass_buffer , max_isotope = max_isotope , ) mass_lower_bound = torch . clamp ( beam_masses - mass_buffer . squeeze ( - 1 ), min = 0 ) mass_upper_bound = beam_masses + mass_buffer . squeeze ( - 1 ) batch_size , beam_size , num_residues = log_probabilities . shape scaled_nucleon_mass = round ( self . mass_scale * CARBON_MASS_DELTA ) for batch in range ( batch_size ): for beam in range ( beam_size ): beam_lower_bound = mass_lower_bound [ batch , beam ] . item () beam_upper_bound = mass_upper_bound [ batch , beam ] . item () for residue in range ( num_residues ): if log_probabilities [ batch , beam , residue ] . isfinite () . item (): valid_residue = self . chart [ beam_lower_bound : ( beam_upper_bound + 1 ), residue ] . any () if max_isotope > 0 : for num_nucleons in range ( 1 , max_isotope + 1 ): local_valid_residue = self . chart [ beam_lower_bound - num_nucleons * scaled_nucleon_mass : ( beam_upper_bound - num_nucleons * scaled_nucleon_mass + 1 ), residue , ] . any () valid_residue = valid_residue or local_valid_residue if not valid_residue : log_probabilities [ batch , beam , residue ] = - float ( \"inf\" ) return log_probabilities","title":"prefilter_items()"},{"location":"reference/transformer/","text":"","title":"Index"},{"location":"reference/transformer/dataset/","text":"SpectrumDataset ( df , s2i , n_peaks = 200 , min_mz = 50.0 , max_mz = 2500.0 , min_intensity = 0.01 , remove_precursor_tol = 2.0 , reverse_peptide = True , eos_symbol = '</s>' , annotated = True , return_str = False ) Bases: Dataset Spectrum dataset class supporting .ipc and .csv . Source code in instanovo/transformer/dataset.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def __init__ ( self , df : pd . DataFrame | pl . DataFrame , s2i : dict [ str , int ], n_peaks : int = 200 , min_mz : float = 50.0 , max_mz : float = 2500.0 , min_intensity : float = 0.01 , remove_precursor_tol : float = 2.0 , reverse_peptide : bool = True , eos_symbol : str = \"</s>\" , annotated : bool = True , return_str : bool = False , ) -> None : super () . __init__ () self . df = df self . s2i = s2i self . n_peaks = n_peaks self . min_mz = min_mz self . max_mz = max_mz self . remove_precursor_tol = remove_precursor_tol self . min_intensity = min_intensity self . reverse_peptide = reverse_peptide self . annotated = annotated self . return_str = return_str if eos_symbol in self . s2i : self . EOS_ID = self . s2i [ eos_symbol ] else : self . EOS_ID = - 1 if isinstance ( df , pd . DataFrame ): self . data_type = \"pd\" elif isinstance ( df , pl . DataFrame ): self . data_type = \"pl\" elif isinstance ( df , datasets . Dataset ): self . data_type = \"hf\" else : raise Exception ( f \"Unsupported data type { type ( df ) } \" ) collate_batch ( batch ) Collate batch of samples. Source code in instanovo/transformer/dataset.py 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 def collate_batch ( batch : list [ tuple [ Tensor , float , int , Tensor ]] ) -> tuple [ Tensor , Tensor , Tensor , Tensor , Tensor ]: \"\"\"Collate batch of samples.\"\"\" spectra , precursor_mzs , precursor_charges , peptides = zip ( * batch ) # Pad spectra ll = torch . tensor ([ x . shape [ 0 ] for x in spectra ], dtype = torch . long ) spectra = nn . utils . rnn . pad_sequence ( spectra , batch_first = True ) spectra_mask = torch . arange ( spectra . shape [ 1 ], dtype = torch . long )[ None , :] >= ll [:, None ] # Pad peptide if isinstance ( peptides [ 0 ], str ): peptides_mask = None else : ll = torch . tensor ([ x . shape [ 0 ] for x in peptides ], dtype = torch . long ) peptides = nn . utils . rnn . pad_sequence ( peptides , batch_first = True ) peptides_mask = torch . arange ( peptides . shape [ 1 ], dtype = torch . long )[ None , :] >= ll [:, None ] precursor_mzs = torch . tensor ( precursor_mzs ) precursor_charges = torch . tensor ( precursor_charges ) precursor_masses = ( precursor_mzs - PROTON_MASS_AMU ) * precursor_charges precursors = torch . vstack ([ precursor_masses , precursor_charges , precursor_mzs ]) . T . float () return spectra , precursors , spectra_mask , peptides , peptides_mask","title":"Dataset"},{"location":"reference/transformer/dataset/#instanovo.transformer.dataset.SpectrumDataset","text":"Bases: Dataset Spectrum dataset class supporting .ipc and .csv . Source code in instanovo/transformer/dataset.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def __init__ ( self , df : pd . DataFrame | pl . DataFrame , s2i : dict [ str , int ], n_peaks : int = 200 , min_mz : float = 50.0 , max_mz : float = 2500.0 , min_intensity : float = 0.01 , remove_precursor_tol : float = 2.0 , reverse_peptide : bool = True , eos_symbol : str = \"</s>\" , annotated : bool = True , return_str : bool = False , ) -> None : super () . __init__ () self . df = df self . s2i = s2i self . n_peaks = n_peaks self . min_mz = min_mz self . max_mz = max_mz self . remove_precursor_tol = remove_precursor_tol self . min_intensity = min_intensity self . reverse_peptide = reverse_peptide self . annotated = annotated self . return_str = return_str if eos_symbol in self . s2i : self . EOS_ID = self . s2i [ eos_symbol ] else : self . EOS_ID = - 1 if isinstance ( df , pd . DataFrame ): self . data_type = \"pd\" elif isinstance ( df , pl . DataFrame ): self . data_type = \"pl\" elif isinstance ( df , datasets . Dataset ): self . data_type = \"hf\" else : raise Exception ( f \"Unsupported data type { type ( df ) } \" )","title":"SpectrumDataset"},{"location":"reference/transformer/dataset/#instanovo.transformer.dataset.collate_batch","text":"Collate batch of samples. Source code in instanovo/transformer/dataset.py 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 def collate_batch ( batch : list [ tuple [ Tensor , float , int , Tensor ]] ) -> tuple [ Tensor , Tensor , Tensor , Tensor , Tensor ]: \"\"\"Collate batch of samples.\"\"\" spectra , precursor_mzs , precursor_charges , peptides = zip ( * batch ) # Pad spectra ll = torch . tensor ([ x . shape [ 0 ] for x in spectra ], dtype = torch . long ) spectra = nn . utils . rnn . pad_sequence ( spectra , batch_first = True ) spectra_mask = torch . arange ( spectra . shape [ 1 ], dtype = torch . long )[ None , :] >= ll [:, None ] # Pad peptide if isinstance ( peptides [ 0 ], str ): peptides_mask = None else : ll = torch . tensor ([ x . shape [ 0 ] for x in peptides ], dtype = torch . long ) peptides = nn . utils . rnn . pad_sequence ( peptides , batch_first = True ) peptides_mask = torch . arange ( peptides . shape [ 1 ], dtype = torch . long )[ None , :] >= ll [:, None ] precursor_mzs = torch . tensor ( precursor_mzs ) precursor_charges = torch . tensor ( precursor_charges ) precursor_masses = ( precursor_mzs - PROTON_MASS_AMU ) * precursor_charges precursors = torch . vstack ([ precursor_masses , precursor_charges , precursor_mzs ]) . T . float () return spectra , precursors , spectra_mask , peptides , peptides_mask","title":"collate_batch()"},{"location":"reference/transformer/decoding/","text":"BaseDecoder ( model , i2s , max_length = 30 , eos_id = 2 , bos_id = 1 , pad_id = 0 ) Base model decoder. Source code in instanovo/transformer/decoding.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def __init__ ( self , model : nn . Module , i2s : dict [ int , str ], max_length : int = 30 , eos_id : int = 2 , bos_id : int = 1 , pad_id : int = 0 , ) -> None : self . model = model self . i2s = i2s self . max_length = max_length self . eos_id = eos_id self . bos_id = bos_id self . pad_id = pad_id __call__ ( * args , ** kwargs ) Decoder call. Source code in instanovo/transformer/decoding.py 53 54 55 def __call__ ( self , * args : Any , ** kwargs : Any ) -> Any : \"\"\"Decoder call.\"\"\" return self . decode ( * args , ** kwargs ) batch_idx_to_aa ( idx ) Decode a batch of indices to aa lists. Source code in instanovo/transformer/decoding.py 49 50 51 def batch_idx_to_aa ( self , idx : Tensor ) -> list [ list [ str ]]: \"\"\"Decode a batch of indices to aa lists.\"\"\" return [ self . idx_to_aa ( i ) for i in idx ] decode ( * args , ** kwds ) abstractmethod Abstract decoding method. Source code in instanovo/transformer/decoding.py 32 33 34 35 @abstractmethod def decode ( self , * args : Any , ** kwds : Any ) -> Any : \"\"\"Abstract decoding method.\"\"\" pass idx_to_aa ( idx ) Decode a single sample of indices to aa list. Source code in instanovo/transformer/decoding.py 37 38 39 40 41 42 43 44 45 46 47 def idx_to_aa ( self , idx : Tensor ) -> list [ str ]: \"\"\"Decode a single sample of indices to aa list.\"\"\" idx = idx . cpu () . numpy () t = [] for i in idx : if i == self . eos_id : break if i == self . bos_id or i == self . pad_id : continue t . append ( i ) return [ self . i2s [ x . item ()] for x in t ] GreedyDecoder ( model , i2s , max_length = 30 , eos_id = 2 , bos_id = 1 , pad_id = 0 ) Bases: BaseDecoder Greedy model decoder. Source code in instanovo/transformer/decoding.py 61 62 63 64 65 66 67 68 69 70 def __init__ ( self , model : InstaNovo , i2s : dict [ int , str ], max_length : int = 30 , eos_id : int = 2 , bos_id : int = 1 , pad_id : int = 0 , ) -> None : super () . __init__ ( model , i2s , max_length , eos_id , bos_id , pad_id ) decode ( spectra , precursors , spectra_mask ) Greedy model decode. Source code in instanovo/transformer/decoding.py 90 91 92 93 94 def decode ( self , spectra : Tensor , precursors : Tensor , spectra_mask : Tensor ) -> tuple [ Tensor , Tensor ]: \"\"\"Greedy model decode.\"\"\" return self . greedy ( spectra , precursors , spectra_mask ) greedy ( spectra , precursors , spectra_mask ) Greedy decoding strategy. Source code in instanovo/transformer/decoding.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def greedy ( self , spectra : Tensor , precursors : Tensor , spectra_mask : Tensor ) -> tuple [ Tensor , Tensor ]: \"\"\"Greedy decoding strategy.\"\"\" device = spectra . device bs = spectra . shape [ 0 ] y = torch . ones (( bs , 1 )) . to ( device ) . long () y_mask = torch . zeros (( bs , 1 ), dtype = bool , device = device ) # BS, N eos_reached = torch . zeros ( bs , dtype = bool , device = device ) # BS for _ in range ( self . max_length ): with torch . no_grad (): logits = self . model ( spectra , precursors , y , spectra_mask , y_mask , add_bos = False ) preds = logits [:, - 1 ] . argmax ( dim =- 1 ) y = torch . cat ([ y , preds [:, None ]], dim =- 1 ) eos_reached = eos_reached | ( y [:, - 1 ] == self . eos_id ) y_mask = torch . cat ( [ y_mask , eos_reached [:, None ], ], axis = 1 , ) . bool () if eos_reached . all (): break # TODO check if logits is corrects shape return y , logits score ( spectra , precursors , peptide , spectra_mask , y_mask ) Score peptide sequences. Source code in instanovo/transformer/decoding.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def score ( self , spectra : Tensor , precursors : Tensor , peptide : Tensor , spectra_mask : Tensor , y_mask : Tensor , ) -> tuple [ Tensor , Tensor ]: \"\"\"Score peptide sequences.\"\"\" with torch . no_grad (): logits = self . model ( spectra , precursors , peptide , spectra_mask , y_mask , add_bos = True ) logits = logits [:, : - 1 ] . softmax ( dim =- 1 ) # check this aa_scores = torch . gather ( logits , peptide , dim = 2 ) return aa_scores , aa_scores . mean ( dim = 1 )","title":"Decoding"},{"location":"reference/transformer/decoding/#instanovo.transformer.decoding.BaseDecoder","text":"Base model decoder. Source code in instanovo/transformer/decoding.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def __init__ ( self , model : nn . Module , i2s : dict [ int , str ], max_length : int = 30 , eos_id : int = 2 , bos_id : int = 1 , pad_id : int = 0 , ) -> None : self . model = model self . i2s = i2s self . max_length = max_length self . eos_id = eos_id self . bos_id = bos_id self . pad_id = pad_id","title":"BaseDecoder"},{"location":"reference/transformer/decoding/#instanovo.transformer.decoding.BaseDecoder.__call__","text":"Decoder call. Source code in instanovo/transformer/decoding.py 53 54 55 def __call__ ( self , * args : Any , ** kwargs : Any ) -> Any : \"\"\"Decoder call.\"\"\" return self . decode ( * args , ** kwargs )","title":"__call__()"},{"location":"reference/transformer/decoding/#instanovo.transformer.decoding.BaseDecoder.batch_idx_to_aa","text":"Decode a batch of indices to aa lists. Source code in instanovo/transformer/decoding.py 49 50 51 def batch_idx_to_aa ( self , idx : Tensor ) -> list [ list [ str ]]: \"\"\"Decode a batch of indices to aa lists.\"\"\" return [ self . idx_to_aa ( i ) for i in idx ]","title":"batch_idx_to_aa()"},{"location":"reference/transformer/decoding/#instanovo.transformer.decoding.BaseDecoder.decode","text":"Abstract decoding method. Source code in instanovo/transformer/decoding.py 32 33 34 35 @abstractmethod def decode ( self , * args : Any , ** kwds : Any ) -> Any : \"\"\"Abstract decoding method.\"\"\" pass","title":"decode()"},{"location":"reference/transformer/decoding/#instanovo.transformer.decoding.BaseDecoder.idx_to_aa","text":"Decode a single sample of indices to aa list. Source code in instanovo/transformer/decoding.py 37 38 39 40 41 42 43 44 45 46 47 def idx_to_aa ( self , idx : Tensor ) -> list [ str ]: \"\"\"Decode a single sample of indices to aa list.\"\"\" idx = idx . cpu () . numpy () t = [] for i in idx : if i == self . eos_id : break if i == self . bos_id or i == self . pad_id : continue t . append ( i ) return [ self . i2s [ x . item ()] for x in t ]","title":"idx_to_aa()"},{"location":"reference/transformer/decoding/#instanovo.transformer.decoding.GreedyDecoder","text":"Bases: BaseDecoder Greedy model decoder. Source code in instanovo/transformer/decoding.py 61 62 63 64 65 66 67 68 69 70 def __init__ ( self , model : InstaNovo , i2s : dict [ int , str ], max_length : int = 30 , eos_id : int = 2 , bos_id : int = 1 , pad_id : int = 0 , ) -> None : super () . __init__ ( model , i2s , max_length , eos_id , bos_id , pad_id )","title":"GreedyDecoder"},{"location":"reference/transformer/decoding/#instanovo.transformer.decoding.GreedyDecoder.decode","text":"Greedy model decode. Source code in instanovo/transformer/decoding.py 90 91 92 93 94 def decode ( self , spectra : Tensor , precursors : Tensor , spectra_mask : Tensor ) -> tuple [ Tensor , Tensor ]: \"\"\"Greedy model decode.\"\"\" return self . greedy ( spectra , precursors , spectra_mask )","title":"decode()"},{"location":"reference/transformer/decoding/#instanovo.transformer.decoding.GreedyDecoder.greedy","text":"Greedy decoding strategy. Source code in instanovo/transformer/decoding.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def greedy ( self , spectra : Tensor , precursors : Tensor , spectra_mask : Tensor ) -> tuple [ Tensor , Tensor ]: \"\"\"Greedy decoding strategy.\"\"\" device = spectra . device bs = spectra . shape [ 0 ] y = torch . ones (( bs , 1 )) . to ( device ) . long () y_mask = torch . zeros (( bs , 1 ), dtype = bool , device = device ) # BS, N eos_reached = torch . zeros ( bs , dtype = bool , device = device ) # BS for _ in range ( self . max_length ): with torch . no_grad (): logits = self . model ( spectra , precursors , y , spectra_mask , y_mask , add_bos = False ) preds = logits [:, - 1 ] . argmax ( dim =- 1 ) y = torch . cat ([ y , preds [:, None ]], dim =- 1 ) eos_reached = eos_reached | ( y [:, - 1 ] == self . eos_id ) y_mask = torch . cat ( [ y_mask , eos_reached [:, None ], ], axis = 1 , ) . bool () if eos_reached . all (): break # TODO check if logits is corrects shape return y , logits","title":"greedy()"},{"location":"reference/transformer/decoding/#instanovo.transformer.decoding.GreedyDecoder.score","text":"Score peptide sequences. Source code in instanovo/transformer/decoding.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def score ( self , spectra : Tensor , precursors : Tensor , peptide : Tensor , spectra_mask : Tensor , y_mask : Tensor , ) -> tuple [ Tensor , Tensor ]: \"\"\"Score peptide sequences.\"\"\" with torch . no_grad (): logits = self . model ( spectra , precursors , peptide , spectra_mask , y_mask , add_bos = True ) logits = logits [:, : - 1 ] . softmax ( dim =- 1 ) # check this aa_scores = torch . gather ( logits , peptide , dim = 2 ) return aa_scores , aa_scores . mean ( dim = 1 )","title":"score()"},{"location":"reference/transformer/layers/","text":"MultiScalePeakEmbedding ( h_size , dropout = 0 ) Bases: Module Multi-scale sinusoidal embedding based on Voronov et. al. Source code in instanovo/transformer/layers.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def __init__ ( self , h_size : int , dropout : float = 0 ) -> None : super () . __init__ () self . h_size = h_size self . mlp = nn . Sequential ( nn . Linear ( h_size , h_size ), nn . ReLU (), nn . Dropout ( dropout ), nn . Linear ( h_size , h_size ), nn . Dropout ( dropout ), ) self . head = nn . Sequential ( nn . Linear ( h_size + 1 , h_size ), nn . ReLU (), nn . Dropout ( dropout ), nn . Linear ( h_size , h_size ), nn . Dropout ( dropout ), ) freqs = 2 * np . pi / torch . logspace ( - 2 , - 3 , int ( h_size / 2 ), dtype = torch . float64 ) self . register_buffer ( \"freqs\" , freqs ) encode_mass ( x ) Encode mz. Source code in instanovo/transformer/layers.py 68 69 70 71 72 def encode_mass ( self , x : Tensor ) -> Tensor : \"\"\"Encode mz.\"\"\" x = self . freqs [ None , None , :] * x x = torch . cat ([ torch . sin ( x ), torch . cos ( x )], axis = 2 ) return x . float () forward ( mz_values , intensities ) Encode peaks. Source code in instanovo/transformer/layers.py 61 62 63 64 65 66 def forward ( self , mz_values : Tensor , intensities : Tensor ) -> Tensor : \"\"\"Encode peaks.\"\"\" x = self . encode_mass ( mz_values ) x = self . mlp ( x ) x = torch . cat ([ x , intensities ], axis = 2 ) return self . head ( x ) PositionalEncoding ( d_model , dropout = 0.1 , max_len = 5000 ) Bases: Module Standard sinusoidal positional encoding. Source code in instanovo/transformer/layers.py 14 15 16 17 18 19 20 21 22 23 def __init__ ( self , d_model : int , dropout : float = 0.1 , max_len : int = 5000 ): super () . __init__ () self . dropout = nn . Dropout ( p = dropout ) position = torch . arange ( max_len ) . unsqueeze ( 1 ) div_term = torch . exp ( torch . arange ( 0 , d_model , 2 ) * ( - math . log ( 10000.0 ) / d_model )) pe = torch . zeros ( 1 , max_len , d_model ) pe [ 0 , :, 0 :: 2 ] = torch . sin ( position * div_term ) pe [ 0 , :, 1 :: 2 ] = torch . cos ( position * div_term ) self . register_buffer ( \"pe\" , pe ) forward ( x ) Positional encoding forward pass. Parameters: Name Type Description Default x Tensor Tensor, shape [seq_len, batch_size, embedding_dim] required Source code in instanovo/transformer/layers.py 25 26 27 28 29 30 31 32 def forward ( self , x : Tensor ) -> Tensor : \"\"\"Positional encoding forward pass. Arguments: x: Tensor, shape ``[seq_len, batch_size, embedding_dim]`` \"\"\" x = x + self . pe [:, : x . size ( 1 )] return self . dropout ( x )","title":"Layers"},{"location":"reference/transformer/layers/#instanovo.transformer.layers.MultiScalePeakEmbedding","text":"Bases: Module Multi-scale sinusoidal embedding based on Voronov et. al. Source code in instanovo/transformer/layers.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def __init__ ( self , h_size : int , dropout : float = 0 ) -> None : super () . __init__ () self . h_size = h_size self . mlp = nn . Sequential ( nn . Linear ( h_size , h_size ), nn . ReLU (), nn . Dropout ( dropout ), nn . Linear ( h_size , h_size ), nn . Dropout ( dropout ), ) self . head = nn . Sequential ( nn . Linear ( h_size + 1 , h_size ), nn . ReLU (), nn . Dropout ( dropout ), nn . Linear ( h_size , h_size ), nn . Dropout ( dropout ), ) freqs = 2 * np . pi / torch . logspace ( - 2 , - 3 , int ( h_size / 2 ), dtype = torch . float64 ) self . register_buffer ( \"freqs\" , freqs )","title":"MultiScalePeakEmbedding"},{"location":"reference/transformer/layers/#instanovo.transformer.layers.MultiScalePeakEmbedding.encode_mass","text":"Encode mz. Source code in instanovo/transformer/layers.py 68 69 70 71 72 def encode_mass ( self , x : Tensor ) -> Tensor : \"\"\"Encode mz.\"\"\" x = self . freqs [ None , None , :] * x x = torch . cat ([ torch . sin ( x ), torch . cos ( x )], axis = 2 ) return x . float ()","title":"encode_mass()"},{"location":"reference/transformer/layers/#instanovo.transformer.layers.MultiScalePeakEmbedding.forward","text":"Encode peaks. Source code in instanovo/transformer/layers.py 61 62 63 64 65 66 def forward ( self , mz_values : Tensor , intensities : Tensor ) -> Tensor : \"\"\"Encode peaks.\"\"\" x = self . encode_mass ( mz_values ) x = self . mlp ( x ) x = torch . cat ([ x , intensities ], axis = 2 ) return self . head ( x )","title":"forward()"},{"location":"reference/transformer/layers/#instanovo.transformer.layers.PositionalEncoding","text":"Bases: Module Standard sinusoidal positional encoding. Source code in instanovo/transformer/layers.py 14 15 16 17 18 19 20 21 22 23 def __init__ ( self , d_model : int , dropout : float = 0.1 , max_len : int = 5000 ): super () . __init__ () self . dropout = nn . Dropout ( p = dropout ) position = torch . arange ( max_len ) . unsqueeze ( 1 ) div_term = torch . exp ( torch . arange ( 0 , d_model , 2 ) * ( - math . log ( 10000.0 ) / d_model )) pe = torch . zeros ( 1 , max_len , d_model ) pe [ 0 , :, 0 :: 2 ] = torch . sin ( position * div_term ) pe [ 0 , :, 1 :: 2 ] = torch . cos ( position * div_term ) self . register_buffer ( \"pe\" , pe )","title":"PositionalEncoding"},{"location":"reference/transformer/layers/#instanovo.transformer.layers.PositionalEncoding.forward","text":"Positional encoding forward pass. Parameters: Name Type Description Default x Tensor Tensor, shape [seq_len, batch_size, embedding_dim] required Source code in instanovo/transformer/layers.py 25 26 27 28 29 30 31 32 def forward ( self , x : Tensor ) -> Tensor : \"\"\"Positional encoding forward pass. Arguments: x: Tensor, shape ``[seq_len, batch_size, embedding_dim]`` \"\"\" x = x + self . pe [:, : x . size ( 1 )] return self . dropout ( x )","title":"forward()"},{"location":"reference/transformer/model/","text":"InstaNovo ( i2s , residues , dim_model = 768 , n_head = 16 , dim_feedforward = 2048 , n_layers = 9 , dropout = 0.1 , max_length = 30 , max_charge = 5 , bos_id = 1 , eos_id = 2 , use_depthcharge = True , enc_type = 'depthcharge' , dec_type = 'depthcharge' , dec_precursor_sos = False ) Bases: Module The Instanovo model. Source code in instanovo/transformer/model.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def __init__ ( self , i2s : dict [ int , str ], residues : dict [ str , float ], dim_model : int = 768 , n_head : int = 16 , dim_feedforward : int = 2048 , n_layers : int = 9 , dropout : float = 0.1 , max_length : int = 30 , max_charge : int = 5 , bos_id : int = 1 , eos_id : int = 2 , use_depthcharge : bool = True , enc_type : str = \"depthcharge\" , dec_type : str = \"depthcharge\" , dec_precursor_sos : bool = False , ) -> None : super () . __init__ () self . i2s = i2s self . n_vocab = len ( self . i2s ) self . residues = residues self . bos_id = bos_id # beginning of sentence ID, prepend to y self . eos_id = eos_id # stop token self . pad_id = 0 self . use_depthcharge = use_depthcharge self . enc_type = enc_type self . dec_type = dec_type self . dec_precursor_sos = dec_precursor_sos self . peptide_mass_calculator = depthcharge . masses . PeptideMass ( self . residues ) self . latent_spectrum = nn . Parameter ( torch . randn ( 1 , 1 , dim_model )) # Encoder if self . enc_type == \"depthcharge\" : self . encoder = SpectrumEncoder ( dim_model = dim_model , n_head = n_head , dim_feedforward = dim_feedforward , n_layers = n_layers , dropout = dropout , dim_intensity = None , ) if not self . dec_precursor_sos : self . mass_encoder = MassEncoder ( dim_model ) self . charge_encoder = nn . Embedding ( max_charge , dim_model ) else : if not self . use_depthcharge : self . peak_encoder = MultiScalePeakEmbedding ( dim_model , dropout = dropout ) self . mass_encoder = self . peak_encoder . encode_mass else : self . mass_encoder = MassEncoder ( dim_model ) self . peak_encoder = PeakEncoder ( dim_model , dim_intensity = None ) encoder_layer = nn . TransformerEncoderLayer ( d_model = dim_model , nhead = n_head , dim_feedforward = dim_feedforward , batch_first = True , dropout = dropout , ) self . encoder = nn . TransformerEncoder ( encoder_layer , num_layers = n_layers , enable_nested_tensor = False , ) # Decoder if dec_type == \"depthcharge\" : self . decoder = PeptideDecoder ( dim_model = dim_model , n_head = n_head , dim_feedforward = dim_feedforward , n_layers = n_layers , dropout = dropout , residues = residues , max_charge = max_charge , ) if not dec_precursor_sos : del self . decoder . charge_encoder self . decoder . charge_encoder = lambda x : torch . zeros ( x . shape [ 0 ], dim_model , device = x . device ) self . sos_embedding = nn . Parameter ( torch . randn ( 1 , 1 , dim_model )) del self . decoder . mass_encoder self . decoder . mass_encoder = lambda x : self . sos_embedding . expand ( x . shape [ 0 ], - 1 , - 1 ) else : self . aa_embed = nn . Embedding ( self . n_vocab , dim_model , padding_idx = 0 ) if not self . use_depthcharge : self . aa_pos_embed = PositionalEncoding ( dim_model , dropout , max_len = 200 ) assert max_length <= 200 # update value if necessary else : self . aa_pos_embed = PositionalEncoder ( dim_model ) decoder_layer = nn . TransformerDecoderLayer ( d_model = dim_model , nhead = n_head , dim_feedforward = dim_feedforward , batch_first = True , dropout = dropout , # norm_first=True, ) self . decoder = nn . TransformerDecoder ( decoder_layer , num_layers = n_layers , ) self . head = nn . Linear ( dim_model , self . n_vocab ) self . charge_encoder = nn . Embedding ( max_charge , dim_model ) if self . dec_type == \"depthcharge\" : self . eos_id = self . decoder . _aa2idx [ \"$\" ] batch_idx_to_aa ( idx ) Decode a batch of indices to aa lists. Source code in instanovo/transformer/model.py 255 256 257 def batch_idx_to_aa ( self , idx : Tensor ) -> list [ list [ str ]]: \"\"\"Decode a batch of indices to aa lists.\"\"\" return [ self . idx_to_aa ( i ) for i in idx ] decode ( sequence ) Decode a single sequence of AA IDs. Source code in instanovo/transformer/model.py 239 240 241 def decode ( self , sequence : torch . LongTensor ) -> list [ str ]: \"\"\"Decode a single sequence of AA IDs.\"\"\" return self . decoder . detokenize ( sequence ) # type: ignore forward ( x , p , y , x_mask = None , y_mask = None , add_bos = True ) Model forward pass. Parameters: Name Type Description Default x Tensor Spectra, float Tensor (batch, n_peaks, 2) required p Tensor Precursors, float Tensor (batch, 3) required y Tensor Peptide, long Tensor (batch, seq_len, vocab) required x_mask Tensor Spectra padding mask, True for padded indices, bool Tensor (batch, n_peaks) None y_mask Tensor Peptide padding mask, bool Tensor (batch, seq_len) None add_bos bool Force add a prefix to y, bool True Returns: Name Type Description logits Tensor float Tensor (batch, n, vocab_size), Tensor (batch, n+1, vocab_size) if add_bos==True. Source code in instanovo/transformer/model.py 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 def forward ( self , x : Tensor , p : Tensor , y : Tensor , x_mask : Tensor = None , y_mask : Tensor = None , add_bos : bool = True , ) -> Tensor : \"\"\"Model forward pass. Args: x: Spectra, float Tensor (batch, n_peaks, 2) p: Precursors, float Tensor (batch, 3) y: Peptide, long Tensor (batch, seq_len, vocab) x_mask: Spectra padding mask, True for padded indices, bool Tensor (batch, n_peaks) y_mask: Peptide padding mask, bool Tensor (batch, seq_len) add_bos: Force add a <s> prefix to y, bool Returns: logits: float Tensor (batch, n, vocab_size), (batch, n+1, vocab_size) if add_bos==True. \"\"\" x , x_mask = self . _encoder ( x , p , x_mask ) return self . _decoder ( x , p , y , x_mask , y_mask , add_bos ) get_empty_index () Get the PAD token ID. Source code in instanovo/transformer/model.py 235 236 237 def get_empty_index ( self ) -> int : \"\"\"Get the PAD token ID.\"\"\" return 0 get_eos_index () Get the EOS token ID. Source code in instanovo/transformer/model.py 231 232 233 def get_eos_index ( self ) -> int : \"\"\"Get the EOS token ID.\"\"\" return self . eos_id get_residue_masses ( mass_scale ) Get the scaled masses of all residues. Source code in instanovo/transformer/model.py 221 222 223 224 225 226 227 228 229 def get_residue_masses ( self , mass_scale : int ) -> torch . LongTensor : \"\"\"Get the scaled masses of all residues.\"\"\" residue_masses = torch . zeros ( max ( self . decoder . _idx2aa . keys ()) + 1 ) . type ( torch . int64 ) for index , residue in self . decoder . _idx2aa . items (): if residue in self . peptide_mass_calculator . masses : residue_masses [ index ] = round ( mass_scale * self . peptide_mass_calculator . masses [ residue ] ) return residue_masses idx_to_aa ( idx ) Decode a single sample of indices to aa list. Source code in instanovo/transformer/model.py 243 244 245 246 247 248 249 250 251 252 253 def idx_to_aa ( self , idx : Tensor ) -> list [ str ]: \"\"\"Decode a single sample of indices to aa list.\"\"\" idx = idx . cpu () . numpy () t = [] for i in idx : if i == self . eos_id : break if i == self . bos_id or i == self . pad_id : continue t . append ( i ) return [ self . i2s [ x . item ()] for x in t ] init ( x , p , x_mask = None ) Initialise model encoder. Source code in instanovo/transformer/model.py 200 201 202 203 204 205 206 207 def init ( self , x : Tensor , p : Tensor , x_mask : Tensor = None ) -> tuple [ tuple [ Tensor , Tensor ], Tensor ]: \"\"\"Initialise model encoder.\"\"\" x , x_mask = self . _encoder ( x , p , x_mask ) # y = torch.ones((x.shape[0], 1), dtype=torch.long, device=x.device) * self.bos_id logits , _ = self . _decoder ( x , p , None , x_mask , None , add_bos = False ) return ( x , x_mask ), torch . log_softmax ( logits [:, - 1 , :], - 1 ) load ( path ) classmethod Load model from checkpoint. Source code in instanovo/transformer/model.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 @classmethod def load ( cls , path : str ) -> nn . Module : \"\"\"Load model from checkpoint.\"\"\" ckpt = torch . load ( path , map_location = \"cpu\" ) config = ckpt [ \"config\" ] # check if PTL checkpoint if all ([ x . startswith ( \"model\" ) for x in ckpt [ \"state_dict\" ] . keys ()]): ckpt [ \"state_dict\" ] = { k . replace ( \"model.\" , \"\" ): v for k , v in ckpt [ \"state_dict\" ] . items ()} i2s = { i : v for i , v in enumerate ( config [ \"vocab\" ])} model = cls ( i2s = i2s , residues = config [ \"residues\" ], dim_model = config [ \"dim_model\" ], n_head = config [ \"n_head\" ], dim_feedforward = config [ \"dim_feedforward\" ], n_layers = config [ \"n_layers\" ], dropout = config [ \"dropout\" ], max_length = config [ \"max_length\" ], max_charge = config [ \"max_charge\" ], use_depthcharge = config [ \"use_depthcharge\" ], enc_type = config [ \"enc_type\" ], dec_type = config [ \"dec_type\" ], dec_precursor_sos = config [ \"dec_precursor_sos\" ], ) model . load_state_dict ( ckpt [ \"state_dict\" ]) return model , config score_candidates ( y , p , x , x_mask ) Score a set of candidate sequences. Source code in instanovo/transformer/model.py 209 210 211 212 213 214 215 216 217 218 219 def score_candidates ( self , y : torch . LongTensor , p : torch . FloatTensor , x : torch . FloatTensor , x_mask : torch . BoolTensor , ) -> torch . FloatTensor : \"\"\"Score a set of candidate sequences.\"\"\" logits , _ = self . _decoder ( x , p , y , x_mask , None , add_bos = True ) return torch . log_softmax ( logits [:, - 1 , :], - 1 )","title":"Model"},{"location":"reference/transformer/model/#instanovo.transformer.model.InstaNovo","text":"Bases: Module The Instanovo model. Source code in instanovo/transformer/model.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def __init__ ( self , i2s : dict [ int , str ], residues : dict [ str , float ], dim_model : int = 768 , n_head : int = 16 , dim_feedforward : int = 2048 , n_layers : int = 9 , dropout : float = 0.1 , max_length : int = 30 , max_charge : int = 5 , bos_id : int = 1 , eos_id : int = 2 , use_depthcharge : bool = True , enc_type : str = \"depthcharge\" , dec_type : str = \"depthcharge\" , dec_precursor_sos : bool = False , ) -> None : super () . __init__ () self . i2s = i2s self . n_vocab = len ( self . i2s ) self . residues = residues self . bos_id = bos_id # beginning of sentence ID, prepend to y self . eos_id = eos_id # stop token self . pad_id = 0 self . use_depthcharge = use_depthcharge self . enc_type = enc_type self . dec_type = dec_type self . dec_precursor_sos = dec_precursor_sos self . peptide_mass_calculator = depthcharge . masses . PeptideMass ( self . residues ) self . latent_spectrum = nn . Parameter ( torch . randn ( 1 , 1 , dim_model )) # Encoder if self . enc_type == \"depthcharge\" : self . encoder = SpectrumEncoder ( dim_model = dim_model , n_head = n_head , dim_feedforward = dim_feedforward , n_layers = n_layers , dropout = dropout , dim_intensity = None , ) if not self . dec_precursor_sos : self . mass_encoder = MassEncoder ( dim_model ) self . charge_encoder = nn . Embedding ( max_charge , dim_model ) else : if not self . use_depthcharge : self . peak_encoder = MultiScalePeakEmbedding ( dim_model , dropout = dropout ) self . mass_encoder = self . peak_encoder . encode_mass else : self . mass_encoder = MassEncoder ( dim_model ) self . peak_encoder = PeakEncoder ( dim_model , dim_intensity = None ) encoder_layer = nn . TransformerEncoderLayer ( d_model = dim_model , nhead = n_head , dim_feedforward = dim_feedforward , batch_first = True , dropout = dropout , ) self . encoder = nn . TransformerEncoder ( encoder_layer , num_layers = n_layers , enable_nested_tensor = False , ) # Decoder if dec_type == \"depthcharge\" : self . decoder = PeptideDecoder ( dim_model = dim_model , n_head = n_head , dim_feedforward = dim_feedforward , n_layers = n_layers , dropout = dropout , residues = residues , max_charge = max_charge , ) if not dec_precursor_sos : del self . decoder . charge_encoder self . decoder . charge_encoder = lambda x : torch . zeros ( x . shape [ 0 ], dim_model , device = x . device ) self . sos_embedding = nn . Parameter ( torch . randn ( 1 , 1 , dim_model )) del self . decoder . mass_encoder self . decoder . mass_encoder = lambda x : self . sos_embedding . expand ( x . shape [ 0 ], - 1 , - 1 ) else : self . aa_embed = nn . Embedding ( self . n_vocab , dim_model , padding_idx = 0 ) if not self . use_depthcharge : self . aa_pos_embed = PositionalEncoding ( dim_model , dropout , max_len = 200 ) assert max_length <= 200 # update value if necessary else : self . aa_pos_embed = PositionalEncoder ( dim_model ) decoder_layer = nn . TransformerDecoderLayer ( d_model = dim_model , nhead = n_head , dim_feedforward = dim_feedforward , batch_first = True , dropout = dropout , # norm_first=True, ) self . decoder = nn . TransformerDecoder ( decoder_layer , num_layers = n_layers , ) self . head = nn . Linear ( dim_model , self . n_vocab ) self . charge_encoder = nn . Embedding ( max_charge , dim_model ) if self . dec_type == \"depthcharge\" : self . eos_id = self . decoder . _aa2idx [ \"$\" ]","title":"InstaNovo"},{"location":"reference/transformer/model/#instanovo.transformer.model.InstaNovo.batch_idx_to_aa","text":"Decode a batch of indices to aa lists. Source code in instanovo/transformer/model.py 255 256 257 def batch_idx_to_aa ( self , idx : Tensor ) -> list [ list [ str ]]: \"\"\"Decode a batch of indices to aa lists.\"\"\" return [ self . idx_to_aa ( i ) for i in idx ]","title":"batch_idx_to_aa()"},{"location":"reference/transformer/model/#instanovo.transformer.model.InstaNovo.decode","text":"Decode a single sequence of AA IDs. Source code in instanovo/transformer/model.py 239 240 241 def decode ( self , sequence : torch . LongTensor ) -> list [ str ]: \"\"\"Decode a single sequence of AA IDs.\"\"\" return self . decoder . detokenize ( sequence ) # type: ignore","title":"decode()"},{"location":"reference/transformer/model/#instanovo.transformer.model.InstaNovo.forward","text":"Model forward pass. Parameters: Name Type Description Default x Tensor Spectra, float Tensor (batch, n_peaks, 2) required p Tensor Precursors, float Tensor (batch, 3) required y Tensor Peptide, long Tensor (batch, seq_len, vocab) required x_mask Tensor Spectra padding mask, True for padded indices, bool Tensor (batch, n_peaks) None y_mask Tensor Peptide padding mask, bool Tensor (batch, seq_len) None add_bos bool Force add a prefix to y, bool True Returns: Name Type Description logits Tensor float Tensor (batch, n, vocab_size), Tensor (batch, n+1, vocab_size) if add_bos==True. Source code in instanovo/transformer/model.py 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 def forward ( self , x : Tensor , p : Tensor , y : Tensor , x_mask : Tensor = None , y_mask : Tensor = None , add_bos : bool = True , ) -> Tensor : \"\"\"Model forward pass. Args: x: Spectra, float Tensor (batch, n_peaks, 2) p: Precursors, float Tensor (batch, 3) y: Peptide, long Tensor (batch, seq_len, vocab) x_mask: Spectra padding mask, True for padded indices, bool Tensor (batch, n_peaks) y_mask: Peptide padding mask, bool Tensor (batch, seq_len) add_bos: Force add a <s> prefix to y, bool Returns: logits: float Tensor (batch, n, vocab_size), (batch, n+1, vocab_size) if add_bos==True. \"\"\" x , x_mask = self . _encoder ( x , p , x_mask ) return self . _decoder ( x , p , y , x_mask , y_mask , add_bos )","title":"forward()"},{"location":"reference/transformer/model/#instanovo.transformer.model.InstaNovo.get_empty_index","text":"Get the PAD token ID. Source code in instanovo/transformer/model.py 235 236 237 def get_empty_index ( self ) -> int : \"\"\"Get the PAD token ID.\"\"\" return 0","title":"get_empty_index()"},{"location":"reference/transformer/model/#instanovo.transformer.model.InstaNovo.get_eos_index","text":"Get the EOS token ID. Source code in instanovo/transformer/model.py 231 232 233 def get_eos_index ( self ) -> int : \"\"\"Get the EOS token ID.\"\"\" return self . eos_id","title":"get_eos_index()"},{"location":"reference/transformer/model/#instanovo.transformer.model.InstaNovo.get_residue_masses","text":"Get the scaled masses of all residues. Source code in instanovo/transformer/model.py 221 222 223 224 225 226 227 228 229 def get_residue_masses ( self , mass_scale : int ) -> torch . LongTensor : \"\"\"Get the scaled masses of all residues.\"\"\" residue_masses = torch . zeros ( max ( self . decoder . _idx2aa . keys ()) + 1 ) . type ( torch . int64 ) for index , residue in self . decoder . _idx2aa . items (): if residue in self . peptide_mass_calculator . masses : residue_masses [ index ] = round ( mass_scale * self . peptide_mass_calculator . masses [ residue ] ) return residue_masses","title":"get_residue_masses()"},{"location":"reference/transformer/model/#instanovo.transformer.model.InstaNovo.idx_to_aa","text":"Decode a single sample of indices to aa list. Source code in instanovo/transformer/model.py 243 244 245 246 247 248 249 250 251 252 253 def idx_to_aa ( self , idx : Tensor ) -> list [ str ]: \"\"\"Decode a single sample of indices to aa list.\"\"\" idx = idx . cpu () . numpy () t = [] for i in idx : if i == self . eos_id : break if i == self . bos_id or i == self . pad_id : continue t . append ( i ) return [ self . i2s [ x . item ()] for x in t ]","title":"idx_to_aa()"},{"location":"reference/transformer/model/#instanovo.transformer.model.InstaNovo.init","text":"Initialise model encoder. Source code in instanovo/transformer/model.py 200 201 202 203 204 205 206 207 def init ( self , x : Tensor , p : Tensor , x_mask : Tensor = None ) -> tuple [ tuple [ Tensor , Tensor ], Tensor ]: \"\"\"Initialise model encoder.\"\"\" x , x_mask = self . _encoder ( x , p , x_mask ) # y = torch.ones((x.shape[0], 1), dtype=torch.long, device=x.device) * self.bos_id logits , _ = self . _decoder ( x , p , None , x_mask , None , add_bos = False ) return ( x , x_mask ), torch . log_softmax ( logits [:, - 1 , :], - 1 )","title":"init()"},{"location":"reference/transformer/model/#instanovo.transformer.model.InstaNovo.load","text":"Load model from checkpoint. Source code in instanovo/transformer/model.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 @classmethod def load ( cls , path : str ) -> nn . Module : \"\"\"Load model from checkpoint.\"\"\" ckpt = torch . load ( path , map_location = \"cpu\" ) config = ckpt [ \"config\" ] # check if PTL checkpoint if all ([ x . startswith ( \"model\" ) for x in ckpt [ \"state_dict\" ] . keys ()]): ckpt [ \"state_dict\" ] = { k . replace ( \"model.\" , \"\" ): v for k , v in ckpt [ \"state_dict\" ] . items ()} i2s = { i : v for i , v in enumerate ( config [ \"vocab\" ])} model = cls ( i2s = i2s , residues = config [ \"residues\" ], dim_model = config [ \"dim_model\" ], n_head = config [ \"n_head\" ], dim_feedforward = config [ \"dim_feedforward\" ], n_layers = config [ \"n_layers\" ], dropout = config [ \"dropout\" ], max_length = config [ \"max_length\" ], max_charge = config [ \"max_charge\" ], use_depthcharge = config [ \"use_depthcharge\" ], enc_type = config [ \"enc_type\" ], dec_type = config [ \"dec_type\" ], dec_precursor_sos = config [ \"dec_precursor_sos\" ], ) model . load_state_dict ( ckpt [ \"state_dict\" ]) return model , config","title":"load()"},{"location":"reference/transformer/model/#instanovo.transformer.model.InstaNovo.score_candidates","text":"Score a set of candidate sequences. Source code in instanovo/transformer/model.py 209 210 211 212 213 214 215 216 217 218 219 def score_candidates ( self , y : torch . LongTensor , p : torch . FloatTensor , x : torch . FloatTensor , x_mask : torch . BoolTensor , ) -> torch . FloatTensor : \"\"\"Score a set of candidate sequences.\"\"\" logits , _ = self . _decoder ( x , p , y , x_mask , None , add_bos = True ) return torch . log_softmax ( logits [:, - 1 , :], - 1 )","title":"score_candidates()"},{"location":"reference/transformer/predict/","text":"get_preds ( data_path , model , config , denovo = False , output_path = None , knapsack_path = None , device = 'cuda' ) Get predictions from a trained model. Source code in instanovo/transformer/predict.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def get_preds ( data_path : str , model : InstaNovo , config : dict [ str , Any ], denovo : bool = False , output_path : str | None = None , knapsack_path : str | None = None , device : str = \"cuda\" , ) -> None : \"\"\"Get predictions from a trained model.\"\"\" if denovo and output_path is None : raise ValueError ( \"Must specify an output path in denovo mode. Specify an output csv file with --output_path\" ) if Path ( data_path ) . suffix . lower () != \".ipc\" : raise ValueError ( f \"Unknown filetype of { data_path } . Only Polars .ipc is currently supported.\" ) logging . info ( f \"Loading data from { data_path } \" ) df = pl . read_ipc ( data_path ) df = df . sample ( fraction = config [ \"subset\" ], seed = 0 ) logging . info ( f \"Data loaded, evaluating { config [ 'subset' ] * 100 : .1f } %, { df . shape [ 0 ] } samples in total.\" ) if not denovo and ( df [ \"modified_sequence\" ] == \"\" ) . all (): raise ValueError ( \"The modified_sequence column is empty, are you trying to run de novo prediction? Add the --denovo flag\" ) vocab = list ( config [ \"residues\" ] . keys ()) config [ \"vocab\" ] = vocab s2i = { v : i for i , v in enumerate ( vocab )} i2s = { i : v for i , v in enumerate ( vocab )} ds = SpectrumDataset ( df , s2i , config [ \"n_peaks\" ], return_str = True , annotated = not denovo ) dl = DataLoader ( ds , batch_size = config [ \"predict_batch_size\" ], num_workers = config [ \"n_workers\" ], shuffle = False , collate_fn = collate_batch , ) model = model . to ( device ) model = model . eval () # setup decoder if knapsack_path is None or not os . path . exists ( knapsack_path ): logging . info ( \"Knapsack path missing or not specified, generating...\" ) knapsack = _setup_knapsack ( model ) decoder = KnapsackBeamSearchDecoder ( model , knapsack ) if knapsack_path is not None : logging . info ( f \"Saving knapsack to { knapsack_path } \" ) knapsack . save ( knapsack_path ) else : logging . info ( \"Knapsack path found. Loading...\" ) decoder = KnapsackBeamSearchDecoder . from_file ( model = model , path = knapsack_path ) index_cols = [ \"id\" , \"global_index\" , \"spectrum_index\" , \"file_index\" , \"sample\" , \"file\" , \"index\" , \"fileno\" , ] cols = [ x for x in df . columns if x in index_cols ] pred_df = df . to_pandas ()[ cols ] . copy () preds = [] targs = [] probs = [] start = time . time () for _ , batch in tqdm ( enumerate ( dl ), total = len ( dl )): spectra , precursors , spectra_mask , peptides , _ = batch spectra = spectra . to ( device ) precursors = precursors . to ( device ) spectra_mask = spectra_mask . to ( device ) with torch . no_grad (): p = decoder . decode ( spectra = spectra , precursors = precursors , beam_size = config [ \"n_beams\" ], max_length = config [ \"max_length\" ], ) preds += [ \"\" . join ( x . sequence ) if not isinstance ( x , list ) else \"\" for x in p ] probs += [ x . log_probability if not isinstance ( x , list ) else - 1 for x in p ] targs += list ( peptides ) delta = time . time () - start logging . info ( f \"Time taken for { data_path } is { delta : .1f } seconds\" ) logging . info ( f \"Average time per batch (bs= { config [ 'predict_batch_size' ] } ): { delta / len ( dl ) : .1f } seconds\" ) if not denovo : pred_df [ \"targets\" ] = targs pred_df [ \"preds\" ] = preds pred_df [ \"log_probs\" ] = probs if output_path is not None : pred_df . to_csv ( output_path , index = False ) logging . info ( f \"Predictions saved to { output_path } \" ) # calculate metrics if not denovo : metrics = Metrics ( config [ \"residues\" ], config [ \"isotope_error_range\" ]) aa_prec , aa_recall , pep_recall , pep_prec = metrics . compute_precision_recall ( pred_df [ \"targets\" ], pred_df [ \"preds\" ] ) aa_er = metrics . compute_aa_er ( pred_df [ \"targets\" ], pred_df [ \"preds\" ]) auc = metrics . calc_auc ( pred_df [ \"targets\" ], pred_df [ \"preds\" ], np . exp ( pred_df [ \"log_probs\" ])) logging . info ( f \"Performance on { data_path } :\" ) logging . info ( f \"aa_er { aa_er } \" ) logging . info ( f \"aa_prec { aa_prec } \" ) logging . info ( f \"aa_recall { aa_recall } \" ) logging . info ( f \"pep_prec { pep_prec } \" ) logging . info ( f \"pep_recall { pep_recall } \" ) logging . info ( f \"auc { auc } \" ) main () Predict with the model. Source code in instanovo/transformer/predict.py 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 def main () -> None : \"\"\"Predict with the model.\"\"\" logging . info ( \"Initializing inference.\" ) parser = argparse . ArgumentParser () parser . add_argument ( \"data_path\" ) parser . add_argument ( \"model_path\" ) parser . add_argument ( \"--denovo\" , action = \"store_true\" ) parser . add_argument ( \"--output_path\" , default = None ) parser . add_argument ( \"--subset\" , default = 1.0 ) parser . add_argument ( \"--knapsack_path\" , default = None ) parser . add_argument ( \"--n_workers\" , default = 8 ) args = parser . parse_args () model , config = InstaNovo . load ( args . model_path ) config [ \"n_workers\" ] = int ( args . n_workers ) config [ \"subset\" ] = float ( args . subset ) data_path = args . data_path denovo = args . denovo output_path = args . output_path knapsack_path = args . knapsack_path get_preds ( data_path , model , config , denovo , output_path , knapsack_path )","title":"Predict"},{"location":"reference/transformer/predict/#instanovo.transformer.predict.get_preds","text":"Get predictions from a trained model. Source code in instanovo/transformer/predict.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 def get_preds ( data_path : str , model : InstaNovo , config : dict [ str , Any ], denovo : bool = False , output_path : str | None = None , knapsack_path : str | None = None , device : str = \"cuda\" , ) -> None : \"\"\"Get predictions from a trained model.\"\"\" if denovo and output_path is None : raise ValueError ( \"Must specify an output path in denovo mode. Specify an output csv file with --output_path\" ) if Path ( data_path ) . suffix . lower () != \".ipc\" : raise ValueError ( f \"Unknown filetype of { data_path } . Only Polars .ipc is currently supported.\" ) logging . info ( f \"Loading data from { data_path } \" ) df = pl . read_ipc ( data_path ) df = df . sample ( fraction = config [ \"subset\" ], seed = 0 ) logging . info ( f \"Data loaded, evaluating { config [ 'subset' ] * 100 : .1f } %, { df . shape [ 0 ] } samples in total.\" ) if not denovo and ( df [ \"modified_sequence\" ] == \"\" ) . all (): raise ValueError ( \"The modified_sequence column is empty, are you trying to run de novo prediction? Add the --denovo flag\" ) vocab = list ( config [ \"residues\" ] . keys ()) config [ \"vocab\" ] = vocab s2i = { v : i for i , v in enumerate ( vocab )} i2s = { i : v for i , v in enumerate ( vocab )} ds = SpectrumDataset ( df , s2i , config [ \"n_peaks\" ], return_str = True , annotated = not denovo ) dl = DataLoader ( ds , batch_size = config [ \"predict_batch_size\" ], num_workers = config [ \"n_workers\" ], shuffle = False , collate_fn = collate_batch , ) model = model . to ( device ) model = model . eval () # setup decoder if knapsack_path is None or not os . path . exists ( knapsack_path ): logging . info ( \"Knapsack path missing or not specified, generating...\" ) knapsack = _setup_knapsack ( model ) decoder = KnapsackBeamSearchDecoder ( model , knapsack ) if knapsack_path is not None : logging . info ( f \"Saving knapsack to { knapsack_path } \" ) knapsack . save ( knapsack_path ) else : logging . info ( \"Knapsack path found. Loading...\" ) decoder = KnapsackBeamSearchDecoder . from_file ( model = model , path = knapsack_path ) index_cols = [ \"id\" , \"global_index\" , \"spectrum_index\" , \"file_index\" , \"sample\" , \"file\" , \"index\" , \"fileno\" , ] cols = [ x for x in df . columns if x in index_cols ] pred_df = df . to_pandas ()[ cols ] . copy () preds = [] targs = [] probs = [] start = time . time () for _ , batch in tqdm ( enumerate ( dl ), total = len ( dl )): spectra , precursors , spectra_mask , peptides , _ = batch spectra = spectra . to ( device ) precursors = precursors . to ( device ) spectra_mask = spectra_mask . to ( device ) with torch . no_grad (): p = decoder . decode ( spectra = spectra , precursors = precursors , beam_size = config [ \"n_beams\" ], max_length = config [ \"max_length\" ], ) preds += [ \"\" . join ( x . sequence ) if not isinstance ( x , list ) else \"\" for x in p ] probs += [ x . log_probability if not isinstance ( x , list ) else - 1 for x in p ] targs += list ( peptides ) delta = time . time () - start logging . info ( f \"Time taken for { data_path } is { delta : .1f } seconds\" ) logging . info ( f \"Average time per batch (bs= { config [ 'predict_batch_size' ] } ): { delta / len ( dl ) : .1f } seconds\" ) if not denovo : pred_df [ \"targets\" ] = targs pred_df [ \"preds\" ] = preds pred_df [ \"log_probs\" ] = probs if output_path is not None : pred_df . to_csv ( output_path , index = False ) logging . info ( f \"Predictions saved to { output_path } \" ) # calculate metrics if not denovo : metrics = Metrics ( config [ \"residues\" ], config [ \"isotope_error_range\" ]) aa_prec , aa_recall , pep_recall , pep_prec = metrics . compute_precision_recall ( pred_df [ \"targets\" ], pred_df [ \"preds\" ] ) aa_er = metrics . compute_aa_er ( pred_df [ \"targets\" ], pred_df [ \"preds\" ]) auc = metrics . calc_auc ( pred_df [ \"targets\" ], pred_df [ \"preds\" ], np . exp ( pred_df [ \"log_probs\" ])) logging . info ( f \"Performance on { data_path } :\" ) logging . info ( f \"aa_er { aa_er } \" ) logging . info ( f \"aa_prec { aa_prec } \" ) logging . info ( f \"aa_recall { aa_recall } \" ) logging . info ( f \"pep_prec { pep_prec } \" ) logging . info ( f \"pep_recall { pep_recall } \" ) logging . info ( f \"auc { auc } \" )","title":"get_preds()"},{"location":"reference/transformer/predict/#instanovo.transformer.predict.main","text":"Predict with the model. Source code in instanovo/transformer/predict.py 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 def main () -> None : \"\"\"Predict with the model.\"\"\" logging . info ( \"Initializing inference.\" ) parser = argparse . ArgumentParser () parser . add_argument ( \"data_path\" ) parser . add_argument ( \"model_path\" ) parser . add_argument ( \"--denovo\" , action = \"store_true\" ) parser . add_argument ( \"--output_path\" , default = None ) parser . add_argument ( \"--subset\" , default = 1.0 ) parser . add_argument ( \"--knapsack_path\" , default = None ) parser . add_argument ( \"--n_workers\" , default = 8 ) args = parser . parse_args () model , config = InstaNovo . load ( args . model_path ) config [ \"n_workers\" ] = int ( args . n_workers ) config [ \"subset\" ] = float ( args . subset ) data_path = args . data_path denovo = args . denovo output_path = args . output_path knapsack_path = args . knapsack_path get_preds ( data_path , model , config , denovo , output_path , knapsack_path )","title":"main()"},{"location":"reference/transformer/train/","text":"PTModule ( config , model , decoder , metrics , sw , optim , scheduler ) Bases: LightningModule PTL wrapper for model. Source code in instanovo/transformer/train.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def __init__ ( self , config : dict [ str , Any ], model : InstaNovo , decoder : BeamSearchDecoder , metrics : Metrics , sw : SummaryWriter , optim : torch . optim . Optimizer , scheduler : torch . optim . lr_scheduler . _LRScheduler , # device: str = 'cpu', ) -> None : super () . __init__ () self . config = config self . model = model self . decoder = decoder self . metrics = metrics self . sw = sw self . optim = optim self . scheduler = scheduler self . loss_fn = nn . CrossEntropyLoss ( ignore_index = 0 ) self . running_loss = None self . _reset_valid_metrics () self . steps = 0 # Update rates based on bs=32 self . step_scale = 32 / config [ \"train_batch_size\" ] configure_optimizers () Initialize the optimizer. This is used by pytorch-lightning when preparing the model for training. Returns Tuple[torch.optim.Optimizer, Dict[str, Any]] The initialized Adam optimizer and its learning rate scheduler. Source code in instanovo/transformer/train.py 198 199 200 201 202 203 204 205 206 207 208 209 210 def configure_optimizers ( self , ) -> tuple [ torch . optim . Optimizer , dict [ str , Any ]]: \"\"\"Initialize the optimizer. This is used by pytorch-lightning when preparing the model for training. Returns ------- Tuple[torch.optim.Optimizer, Dict[str, Any]] The initialized Adam optimizer and its learning rate scheduler. \"\"\" return [ self . optim ], { \"scheduler\" : self . scheduler , \"interval\" : \"step\" } forward ( spectra , precursors , peptides , spectra_mask , peptides_mask ) Model forward pass. Source code in instanovo/transformer/train.py 63 64 65 66 67 68 69 70 71 72 def forward ( self , spectra : Tensor , precursors : Tensor , peptides : list [ str ] | Tensor , spectra_mask : Tensor , peptides_mask : Tensor , ) -> tuple [ Tensor , Tensor ]: \"\"\"Model forward pass.\"\"\" return self . model ( spectra , precursors , peptides , spectra_mask , peptides_mask ) # type: ignore on_load_checkpoint ( checkpoint ) Attempt to load config with checkpoint. Source code in instanovo/transformer/train.py 194 195 196 def on_load_checkpoint ( self , checkpoint : dict [ str , Any ]) -> None : \"\"\"Attempt to load config with checkpoint.\"\"\" self . config = checkpoint [ \"config\" ] on_save_checkpoint ( checkpoint ) Save config with checkpoint. Source code in instanovo/transformer/train.py 190 191 192 def on_save_checkpoint ( self , checkpoint : dict [ str , Any ]) -> None : \"\"\"Save config with checkpoint.\"\"\" checkpoint [ \"config\" ] = self . config on_train_epoch_end () Log the training loss at the end of each epoch. Source code in instanovo/transformer/train.py 166 167 168 169 170 171 def on_train_epoch_end ( self ) -> None : \"\"\"Log the training loss at the end of each epoch.\"\"\" epoch = self . trainer . current_epoch self . sw . add_scalar ( f \"eval/train_loss\" , self . running_loss , epoch ) self . running_loss = None on_validation_epoch_end () Log the validation metrics at the end of each epoch. Source code in instanovo/transformer/train.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 def on_validation_epoch_end ( self ) -> None : \"\"\"Log the validation metrics at the end of each epoch.\"\"\" epoch = self . trainer . current_epoch for k , v in self . valid_metrics . items (): self . sw . add_scalar ( f \"eval/ { k } \" , np . mean ( v ), epoch ) valid_loss = np . mean ( self . valid_metrics [ \"valid_loss\" ]) logging . info ( f \"[Epoch { epoch : 02d } ] train_loss= { self . running_loss : .5f } , valid_loss= { valid_loss : .5f } \" ) logging . info ( f \"[Epoch { epoch : 02d } ] Metrics:\" ) for metric in [ \"aa_er\" , \"aa_prec\" , \"aa_recall\" , \"pep_recall\" ]: val = np . mean ( self . valid_metrics [ metric ]) logging . info ( f \"[Epoch { epoch : 02d } ] - { metric : 11s }{ val : .3f } \" ) self . _reset_valid_metrics () training_step ( batch ) A single training step. Parameters: Name Type Description Default batch tuple[torch.FloatTensor, torch.FloatTensor, torch.LongTensor]) A batch of MS/MS spectra, precursor information, and peptide sequences as torch Tensors. required Returns: Type Description Tensor torch.FloatTensor: training loss Source code in instanovo/transformer/train.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def training_step ( # need to update this self , batch : tuple [ Tensor , Tensor , Tensor , list [ str ] | Tensor , Tensor ], ) -> torch . Tensor : \"\"\"A single training step. Args: batch (tuple[torch.FloatTensor, torch.FloatTensor, torch.LongTensor]) : A batch of MS/MS spectra, precursor information, and peptide sequences as torch Tensors. Returns: torch.FloatTensor: training loss \"\"\" spectra , precursors , spectra_mask , peptides , peptides_mask = batch spectra = spectra . to ( self . device ) precursors = precursors . to ( self . device ) spectra_mask = spectra_mask . to ( self . device ) # peptides = peptides.to(self.device) # peptides_mask = peptides_mask.to(self.device) preds , truth = self . forward ( spectra , precursors , peptides , spectra_mask , peptides_mask ) # cut off EOS's prediction, ignore_index should take care of masking # preds = preds[:, :-1].reshape(-1, preds.shape[-1]) preds = preds [:, : - 1 , :] . reshape ( - 1 , self . model . decoder . vocab_size + 1 ) loss = self . loss_fn ( preds , truth . flatten ()) if self . running_loss is None : self . running_loss = loss . item () else : self . running_loss = 0.99 * self . running_loss + ( 1 - 0.99 ) * loss . item () if (( self . steps + 1 ) % int ( 2000 * self . step_scale )) == 0 : lr = self . trainer . lr_scheduler_configs [ 0 ] . scheduler . get_last_lr ()[ 0 ] logging . info ( f \"[Step { self . steps - 1 : 06d } ]: train_loss_raw= { loss . item () : .4f } , running_loss= { self . running_loss : .4f } , LR= { lr } \" ) if ( self . steps + 1 ) % int ( 500 * self . step_scale ) == 0 : lr = self . trainer . lr_scheduler_configs [ 0 ] . scheduler . get_last_lr ()[ 0 ] self . sw . add_scalar ( \"train/loss_raw\" , loss . item (), self . steps - 1 ) self . sw . add_scalar ( \"train/loss_smooth\" , self . running_loss , self . steps - 1 ) self . sw . add_scalar ( \"optim/lr\" , lr , self . steps - 1 ) self . sw . add_scalar ( \"optim/epoch\" , self . trainer . current_epoch , self . steps - 1 ) self . steps += 1 return loss validation_step ( batch , * args ) Single validation step. Source code in instanovo/transformer/train.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def validation_step ( self , batch : tuple [ Tensor , Tensor , Tensor , list [ str ] | Tensor , Tensor ], * args : Any ) -> torch . Tensor : \"\"\"Single validation step.\"\"\" spectra , precursors , spectra_mask , peptides , peptides_mask = batch spectra = spectra . to ( self . device ) precursors = precursors . to ( self . device ) spectra_mask = spectra_mask . to ( self . device ) # peptides = peptides.to(self.device) # peptides_mask = peptides_mask.to(self.device) # Loss with torch . no_grad (): preds , truth = self . forward ( spectra , precursors , peptides , spectra_mask , peptides_mask ) preds = preds [:, : - 1 , :] . reshape ( - 1 , self . model . decoder . vocab_size + 1 ) loss = self . loss_fn ( preds , truth . flatten ()) # Greedy decoding with torch . no_grad (): # y, _ = decoder(spectra, precursors, spectra_mask) p = self . decoder . decode ( spectra = spectra , precursors = precursors , beam_size = self . config [ \"n_beams\" ], max_length = self . config [ \"max_length\" ], ) # targets = self.model.batch_idx_to_aa(peptides) y = [ \"\" . join ( x . sequence ) if not isinstance ( x , list ) else \"\" for x in p ] targets = peptides aa_prec , aa_recall , pep_recall , _ = self . metrics . compute_precision_recall ( targets , y ) aa_er = self . metrics . compute_aa_er ( targets , y ) self . valid_metrics [ \"valid_loss\" ] . append ( loss . item ()) self . valid_metrics [ \"aa_er\" ] . append ( aa_er ) self . valid_metrics [ \"aa_prec\" ] . append ( aa_prec ) self . valid_metrics [ \"aa_recall\" ] . append ( aa_recall ) self . valid_metrics [ \"pep_recall\" ] . append ( pep_recall ) return loss . item () WarmupScheduler ( optimizer , warmup ) Bases: _LRScheduler Linear warmup scheduler. Source code in instanovo/transformer/train.py 409 410 411 def __init__ ( self , optimizer : torch . optim . Optimizer , warmup : int ) -> None : self . warmup = warmup super () . __init__ ( optimizer ) get_lr () Get the learning rate at the current step. Source code in instanovo/transformer/train.py 413 414 415 416 def get_lr ( self ) -> list [ float ]: \"\"\"Get the learning rate at the current step.\"\"\" lr_factor = self . get_lr_factor ( epoch = self . last_epoch ) return [ base_lr * lr_factor for base_lr in self . base_lrs ] get_lr_factor ( epoch ) Get the LR factor at the current step. Source code in instanovo/transformer/train.py 418 419 420 421 422 423 def get_lr_factor ( self , epoch : int ) -> float : \"\"\"Get the LR factor at the current step.\"\"\" lr_factor = 1.0 if epoch <= self . warmup : lr_factor *= epoch / self . warmup return lr_factor main () Train the model. Source code in instanovo/transformer/train.py 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def main () -> None : \"\"\"Train the model.\"\"\" logging . info ( \"Initializing training.\" ) parser = argparse . ArgumentParser () parser . add_argument ( \"train_path\" ) parser . add_argument ( \"valid_path\" ) parser . add_argument ( \"--config\" , default = \"base.yaml\" ) parser . add_argument ( \"--n_gpu\" , default = 1 ) parser . add_argument ( \"--n_workers\" , default = 8 ) args = parser . parse_args () config_path = os . path . join ( os . path . dirname ( os . path . realpath ( __file__ )), f \"../../configs/instanovo/ { args . config } \" ) with open ( config_path ) as f_in : config = yaml . safe_load ( f_in ) # config[\"residues\"] = {str(aa): float(mass) for aa, mass in config[\"residues\"].items()} config [ \"n_gpu\" ] = int ( args . n_gpu ) config [ \"n_workers\" ] = int ( args . n_workers ) if config [ \"n_gpu\" ] > 1 : raise Exception ( \"n_gpu > 1 currently not supported.\" ) if not config [ \"train_from_scratch\" ]: model_path = config [ \"resume_checkpoint\" ] else : model_path = None train ( args . train_path , args . valid_path , config , model_path ) train ( train_path , valid_path , config , model_path = None ) Training function. Source code in instanovo/transformer/train.py 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 def train ( train_path : str , valid_path : str , config : dict , model_path : str | None = None , ) -> None : \"\"\"Training function.\"\"\" config [ \"tb_summarywriter\" ] = config [ \"tb_summarywriter\" ] + datetime . datetime . now () . strftime ( \"_%y_%m_ %d _%H_%M\" ) sw = SummaryWriter ( config [ \"tb_summarywriter\" ]) # Transformer vocabulary, should we include an UNK token? if config [ \"dec_type\" ] != \"depthcharge\" : vocab = [ \"PAD\" , \"<s>\" , \"</s>\" ] + list ( config [ \"residues\" ] . keys ()) else : vocab = list ( config [ \"residues\" ] . keys ()) config [ \"vocab\" ] = vocab s2i = { v : i for i , v in enumerate ( vocab )} i2s = { i : v for i , v in enumerate ( vocab )} logging . info ( f \"Vocab: { i2s } \" ) logging . info ( \"Loading data\" ) if train_path . endswith ( \".ipc\" ): train_df = pl . read_ipc ( train_path ) train_df = train_df . sample ( fraction = config [ \"train_subset\" ], seed = 0 ) valid_df = pl . read_ipc ( valid_path ) valid_df = valid_df . sample ( fraction = config [ \"valid_subset\" ], seed = 0 ) elif train_path . endswith ( \".csv\" ): train_df = pd . read_csv ( train_path ) train_df = train_df . sample ( frac = config [ \"train_subset\" ], random_state = 0 ) valid_df = pd . read_csv ( valid_path ) valid_df = valid_df . sample ( frac = config [ \"valid_subset\" ], random_state = 0 ) train_ds = SpectrumDataset ( train_df , s2i , config [ \"n_peaks\" ], return_str = True ) valid_ds = SpectrumDataset ( valid_df , s2i , config [ \"n_peaks\" ], return_str = True ) logging . info ( f \"Data loaded: { len ( train_ds ) : , } training samples; { len ( valid_ds ) : , } validation samples\" ) train_dl = DataLoader ( train_ds , batch_size = config [ \"train_batch_size\" ], num_workers = config [ \"n_workers\" ], shuffle = True , collate_fn = collate_batch , ) valid_dl = DataLoader ( valid_ds , batch_size = config [ \"predict_batch_size\" ], num_workers = config [ \"n_workers\" ], shuffle = False , collate_fn = collate_batch , ) # Update rates based on bs=32 step_scale = 32 / config [ \"train_batch_size\" ] logging . info ( f \"Updates per epoch: { len ( train_dl ) : , } , step_scale= { step_scale } \" ) batch = next ( iter ( train_dl )) spectra , precursors , spectra_mask , peptides , peptides_mask = batch logging . info ( \"Sample batch:\" ) logging . info ( f \" - spectra.shape= { spectra . shape } \" ) logging . info ( f \" - precursors.shape= { precursors . shape } \" ) logging . info ( f \" - spectra_mask.shape= { spectra_mask . shape } \" ) logging . info ( f \" - len(peptides)= { len ( peptides ) } \" ) logging . info ( f \" - peptides_mask= { peptides_mask } \" ) # init model model = InstaNovo ( i2s = i2s , residues = config [ \"residues\" ], dim_model = config [ \"dim_model\" ], n_head = config [ \"n_head\" ], dim_feedforward = config [ \"dim_feedforward\" ], n_layers = config [ \"n_layers\" ], dropout = config [ \"dropout\" ], max_length = config [ \"max_length\" ], max_charge = config [ \"max_charge\" ], use_depthcharge = config [ \"use_depthcharge\" ], enc_type = config [ \"enc_type\" ], dec_type = config [ \"dec_type\" ], dec_precursor_sos = config [ \"dec_precursor_sos\" ], ) if model_path is not None : logging . info ( f \"Loading model checkpoint from ' { model_path } '\" ) model_state = torch . load ( model_path , map_location = \"cpu\" ) # check if PTL checkpoint if \"state_dict\" in model_state : model_state = { k . replace ( \"model.\" , \"\" ): v for k , v in model_state [ \"state_dict\" ] . items ()} k_missing = np . sum ( [ x not in list ( model_state . keys ()) for x in list ( model . state_dict () . keys ())] ) if k_missing > 0 : logging . warning ( f \"Model checkpoint is missing { k_missing } keys!\" ) k_missing = np . sum ( [ x not in list ( model . state_dict () . keys ()) for x in list ( model_state . keys ())] ) if k_missing > 0 : logging . warning ( f \"Model state is missing { k_missing } keys!\" ) model . load_state_dict ( model_state , strict = False ) logging . info ( f \"Model loaded with { np . sum ([ p . numel () for p in model . parameters ()]) : ,d } parameters\" ) logging . info ( \"Test forward pass:\" ) with torch . no_grad (): y , _ = model ( spectra , precursors , peptides , spectra_mask , peptides_mask ) logging . info ( f \" - y.shape= { y . shape } \" ) # Train on GPU device = \"cuda:0\" if torch . cuda . is_available () else \"cpu\" model = model . to ( device ) # decoder = GreedyDecoder(model, i2s, max_length=config[\"max_length\"]) decoder = BeamSearchDecoder ( model = model ) metrics = Metrics ( config [ \"residues\" ], config [ \"isotope_error_range\" ]) # init optim # assert s2i[\"PAD\"] == 0 # require PAD token to be index 0, all padding should use zeros # loss_fn = nn.CrossEntropyLoss(ignore_index=0) optim = torch . optim . Adam ( model . parameters (), lr = float ( config [ \"learning_rate\" ]), weight_decay = float ( config [ \"weight_decay\" ]), ) scheduler = WarmupScheduler ( optim , config [ \"warmup_iters\" ]) strategy = _get_strategy () ptmodel = PTModule ( config , model , decoder , metrics , sw , optim , scheduler ) if config [ \"save_model\" ]: callbacks = [ ptl . callbacks . ModelCheckpoint ( dirpath = config [ \"model_save_folder_path\" ], save_top_k =- 1 , save_weights_only = config [ \"save_weights_only\" ], every_n_train_steps = config [ \"ckpt_interval\" ], ) ] else : callbacks = None logging . info ( \"Initializing PL trainer.\" ) trainer = ptl . Trainer ( accelerator = \"auto\" , auto_select_gpus = True , callbacks = callbacks , devices = \"auto\" , logger = config [ \"logger\" ], max_epochs = config [ \"epochs\" ], num_sanity_val_steps = config [ \"num_sanity_val_steps\" ], accumulate_grad_batches = config [ \"grad_accumulation\" ], gradient_clip_val = config [ \"gradient_clip_val\" ], strategy = strategy , ) # Train the model. trainer . fit ( ptmodel , train_dl , valid_dl ) logging . info ( \"Training complete.\" )","title":"Train"},{"location":"reference/transformer/train/#instanovo.transformer.train.PTModule","text":"Bases: LightningModule PTL wrapper for model. Source code in instanovo/transformer/train.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 def __init__ ( self , config : dict [ str , Any ], model : InstaNovo , decoder : BeamSearchDecoder , metrics : Metrics , sw : SummaryWriter , optim : torch . optim . Optimizer , scheduler : torch . optim . lr_scheduler . _LRScheduler , # device: str = 'cpu', ) -> None : super () . __init__ () self . config = config self . model = model self . decoder = decoder self . metrics = metrics self . sw = sw self . optim = optim self . scheduler = scheduler self . loss_fn = nn . CrossEntropyLoss ( ignore_index = 0 ) self . running_loss = None self . _reset_valid_metrics () self . steps = 0 # Update rates based on bs=32 self . step_scale = 32 / config [ \"train_batch_size\" ]","title":"PTModule"},{"location":"reference/transformer/train/#instanovo.transformer.train.PTModule.configure_optimizers","text":"Initialize the optimizer. This is used by pytorch-lightning when preparing the model for training.","title":"configure_optimizers()"},{"location":"reference/transformer/train/#instanovo.transformer.train.PTModule.configure_optimizers--returns","text":"Tuple[torch.optim.Optimizer, Dict[str, Any]] The initialized Adam optimizer and its learning rate scheduler. Source code in instanovo/transformer/train.py 198 199 200 201 202 203 204 205 206 207 208 209 210 def configure_optimizers ( self , ) -> tuple [ torch . optim . Optimizer , dict [ str , Any ]]: \"\"\"Initialize the optimizer. This is used by pytorch-lightning when preparing the model for training. Returns ------- Tuple[torch.optim.Optimizer, Dict[str, Any]] The initialized Adam optimizer and its learning rate scheduler. \"\"\" return [ self . optim ], { \"scheduler\" : self . scheduler , \"interval\" : \"step\" }","title":"Returns"},{"location":"reference/transformer/train/#instanovo.transformer.train.PTModule.forward","text":"Model forward pass. Source code in instanovo/transformer/train.py 63 64 65 66 67 68 69 70 71 72 def forward ( self , spectra : Tensor , precursors : Tensor , peptides : list [ str ] | Tensor , spectra_mask : Tensor , peptides_mask : Tensor , ) -> tuple [ Tensor , Tensor ]: \"\"\"Model forward pass.\"\"\" return self . model ( spectra , precursors , peptides , spectra_mask , peptides_mask ) # type: ignore","title":"forward()"},{"location":"reference/transformer/train/#instanovo.transformer.train.PTModule.on_load_checkpoint","text":"Attempt to load config with checkpoint. Source code in instanovo/transformer/train.py 194 195 196 def on_load_checkpoint ( self , checkpoint : dict [ str , Any ]) -> None : \"\"\"Attempt to load config with checkpoint.\"\"\" self . config = checkpoint [ \"config\" ]","title":"on_load_checkpoint()"},{"location":"reference/transformer/train/#instanovo.transformer.train.PTModule.on_save_checkpoint","text":"Save config with checkpoint. Source code in instanovo/transformer/train.py 190 191 192 def on_save_checkpoint ( self , checkpoint : dict [ str , Any ]) -> None : \"\"\"Save config with checkpoint.\"\"\" checkpoint [ \"config\" ] = self . config","title":"on_save_checkpoint()"},{"location":"reference/transformer/train/#instanovo.transformer.train.PTModule.on_train_epoch_end","text":"Log the training loss at the end of each epoch. Source code in instanovo/transformer/train.py 166 167 168 169 170 171 def on_train_epoch_end ( self ) -> None : \"\"\"Log the training loss at the end of each epoch.\"\"\" epoch = self . trainer . current_epoch self . sw . add_scalar ( f \"eval/train_loss\" , self . running_loss , epoch ) self . running_loss = None","title":"on_train_epoch_end()"},{"location":"reference/transformer/train/#instanovo.transformer.train.PTModule.on_validation_epoch_end","text":"Log the validation metrics at the end of each epoch. Source code in instanovo/transformer/train.py 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 def on_validation_epoch_end ( self ) -> None : \"\"\"Log the validation metrics at the end of each epoch.\"\"\" epoch = self . trainer . current_epoch for k , v in self . valid_metrics . items (): self . sw . add_scalar ( f \"eval/ { k } \" , np . mean ( v ), epoch ) valid_loss = np . mean ( self . valid_metrics [ \"valid_loss\" ]) logging . info ( f \"[Epoch { epoch : 02d } ] train_loss= { self . running_loss : .5f } , valid_loss= { valid_loss : .5f } \" ) logging . info ( f \"[Epoch { epoch : 02d } ] Metrics:\" ) for metric in [ \"aa_er\" , \"aa_prec\" , \"aa_recall\" , \"pep_recall\" ]: val = np . mean ( self . valid_metrics [ metric ]) logging . info ( f \"[Epoch { epoch : 02d } ] - { metric : 11s }{ val : .3f } \" ) self . _reset_valid_metrics ()","title":"on_validation_epoch_end()"},{"location":"reference/transformer/train/#instanovo.transformer.train.PTModule.training_step","text":"A single training step. Parameters: Name Type Description Default batch tuple[torch.FloatTensor, torch.FloatTensor, torch.LongTensor]) A batch of MS/MS spectra, precursor information, and peptide sequences as torch Tensors. required Returns: Type Description Tensor torch.FloatTensor: training loss Source code in instanovo/transformer/train.py 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def training_step ( # need to update this self , batch : tuple [ Tensor , Tensor , Tensor , list [ str ] | Tensor , Tensor ], ) -> torch . Tensor : \"\"\"A single training step. Args: batch (tuple[torch.FloatTensor, torch.FloatTensor, torch.LongTensor]) : A batch of MS/MS spectra, precursor information, and peptide sequences as torch Tensors. Returns: torch.FloatTensor: training loss \"\"\" spectra , precursors , spectra_mask , peptides , peptides_mask = batch spectra = spectra . to ( self . device ) precursors = precursors . to ( self . device ) spectra_mask = spectra_mask . to ( self . device ) # peptides = peptides.to(self.device) # peptides_mask = peptides_mask.to(self.device) preds , truth = self . forward ( spectra , precursors , peptides , spectra_mask , peptides_mask ) # cut off EOS's prediction, ignore_index should take care of masking # preds = preds[:, :-1].reshape(-1, preds.shape[-1]) preds = preds [:, : - 1 , :] . reshape ( - 1 , self . model . decoder . vocab_size + 1 ) loss = self . loss_fn ( preds , truth . flatten ()) if self . running_loss is None : self . running_loss = loss . item () else : self . running_loss = 0.99 * self . running_loss + ( 1 - 0.99 ) * loss . item () if (( self . steps + 1 ) % int ( 2000 * self . step_scale )) == 0 : lr = self . trainer . lr_scheduler_configs [ 0 ] . scheduler . get_last_lr ()[ 0 ] logging . info ( f \"[Step { self . steps - 1 : 06d } ]: train_loss_raw= { loss . item () : .4f } , running_loss= { self . running_loss : .4f } , LR= { lr } \" ) if ( self . steps + 1 ) % int ( 500 * self . step_scale ) == 0 : lr = self . trainer . lr_scheduler_configs [ 0 ] . scheduler . get_last_lr ()[ 0 ] self . sw . add_scalar ( \"train/loss_raw\" , loss . item (), self . steps - 1 ) self . sw . add_scalar ( \"train/loss_smooth\" , self . running_loss , self . steps - 1 ) self . sw . add_scalar ( \"optim/lr\" , lr , self . steps - 1 ) self . sw . add_scalar ( \"optim/epoch\" , self . trainer . current_epoch , self . steps - 1 ) self . steps += 1 return loss","title":"training_step()"},{"location":"reference/transformer/train/#instanovo.transformer.train.PTModule.validation_step","text":"Single validation step. Source code in instanovo/transformer/train.py 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def validation_step ( self , batch : tuple [ Tensor , Tensor , Tensor , list [ str ] | Tensor , Tensor ], * args : Any ) -> torch . Tensor : \"\"\"Single validation step.\"\"\" spectra , precursors , spectra_mask , peptides , peptides_mask = batch spectra = spectra . to ( self . device ) precursors = precursors . to ( self . device ) spectra_mask = spectra_mask . to ( self . device ) # peptides = peptides.to(self.device) # peptides_mask = peptides_mask.to(self.device) # Loss with torch . no_grad (): preds , truth = self . forward ( spectra , precursors , peptides , spectra_mask , peptides_mask ) preds = preds [:, : - 1 , :] . reshape ( - 1 , self . model . decoder . vocab_size + 1 ) loss = self . loss_fn ( preds , truth . flatten ()) # Greedy decoding with torch . no_grad (): # y, _ = decoder(spectra, precursors, spectra_mask) p = self . decoder . decode ( spectra = spectra , precursors = precursors , beam_size = self . config [ \"n_beams\" ], max_length = self . config [ \"max_length\" ], ) # targets = self.model.batch_idx_to_aa(peptides) y = [ \"\" . join ( x . sequence ) if not isinstance ( x , list ) else \"\" for x in p ] targets = peptides aa_prec , aa_recall , pep_recall , _ = self . metrics . compute_precision_recall ( targets , y ) aa_er = self . metrics . compute_aa_er ( targets , y ) self . valid_metrics [ \"valid_loss\" ] . append ( loss . item ()) self . valid_metrics [ \"aa_er\" ] . append ( aa_er ) self . valid_metrics [ \"aa_prec\" ] . append ( aa_prec ) self . valid_metrics [ \"aa_recall\" ] . append ( aa_recall ) self . valid_metrics [ \"pep_recall\" ] . append ( pep_recall ) return loss . item ()","title":"validation_step()"},{"location":"reference/transformer/train/#instanovo.transformer.train.WarmupScheduler","text":"Bases: _LRScheduler Linear warmup scheduler. Source code in instanovo/transformer/train.py 409 410 411 def __init__ ( self , optimizer : torch . optim . Optimizer , warmup : int ) -> None : self . warmup = warmup super () . __init__ ( optimizer )","title":"WarmupScheduler"},{"location":"reference/transformer/train/#instanovo.transformer.train.WarmupScheduler.get_lr","text":"Get the learning rate at the current step. Source code in instanovo/transformer/train.py 413 414 415 416 def get_lr ( self ) -> list [ float ]: \"\"\"Get the learning rate at the current step.\"\"\" lr_factor = self . get_lr_factor ( epoch = self . last_epoch ) return [ base_lr * lr_factor for base_lr in self . base_lrs ]","title":"get_lr()"},{"location":"reference/transformer/train/#instanovo.transformer.train.WarmupScheduler.get_lr_factor","text":"Get the LR factor at the current step. Source code in instanovo/transformer/train.py 418 419 420 421 422 423 def get_lr_factor ( self , epoch : int ) -> float : \"\"\"Get the LR factor at the current step.\"\"\" lr_factor = 1.0 if epoch <= self . warmup : lr_factor *= epoch / self . warmup return lr_factor","title":"get_lr_factor()"},{"location":"reference/transformer/train/#instanovo.transformer.train.main","text":"Train the model. Source code in instanovo/transformer/train.py 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 def main () -> None : \"\"\"Train the model.\"\"\" logging . info ( \"Initializing training.\" ) parser = argparse . ArgumentParser () parser . add_argument ( \"train_path\" ) parser . add_argument ( \"valid_path\" ) parser . add_argument ( \"--config\" , default = \"base.yaml\" ) parser . add_argument ( \"--n_gpu\" , default = 1 ) parser . add_argument ( \"--n_workers\" , default = 8 ) args = parser . parse_args () config_path = os . path . join ( os . path . dirname ( os . path . realpath ( __file__ )), f \"../../configs/instanovo/ { args . config } \" ) with open ( config_path ) as f_in : config = yaml . safe_load ( f_in ) # config[\"residues\"] = {str(aa): float(mass) for aa, mass in config[\"residues\"].items()} config [ \"n_gpu\" ] = int ( args . n_gpu ) config [ \"n_workers\" ] = int ( args . n_workers ) if config [ \"n_gpu\" ] > 1 : raise Exception ( \"n_gpu > 1 currently not supported.\" ) if not config [ \"train_from_scratch\" ]: model_path = config [ \"resume_checkpoint\" ] else : model_path = None train ( args . train_path , args . valid_path , config , model_path )","title":"main()"},{"location":"reference/transformer/train/#instanovo.transformer.train.train","text":"Training function. Source code in instanovo/transformer/train.py 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 def train ( train_path : str , valid_path : str , config : dict , model_path : str | None = None , ) -> None : \"\"\"Training function.\"\"\" config [ \"tb_summarywriter\" ] = config [ \"tb_summarywriter\" ] + datetime . datetime . now () . strftime ( \"_%y_%m_ %d _%H_%M\" ) sw = SummaryWriter ( config [ \"tb_summarywriter\" ]) # Transformer vocabulary, should we include an UNK token? if config [ \"dec_type\" ] != \"depthcharge\" : vocab = [ \"PAD\" , \"<s>\" , \"</s>\" ] + list ( config [ \"residues\" ] . keys ()) else : vocab = list ( config [ \"residues\" ] . keys ()) config [ \"vocab\" ] = vocab s2i = { v : i for i , v in enumerate ( vocab )} i2s = { i : v for i , v in enumerate ( vocab )} logging . info ( f \"Vocab: { i2s } \" ) logging . info ( \"Loading data\" ) if train_path . endswith ( \".ipc\" ): train_df = pl . read_ipc ( train_path ) train_df = train_df . sample ( fraction = config [ \"train_subset\" ], seed = 0 ) valid_df = pl . read_ipc ( valid_path ) valid_df = valid_df . sample ( fraction = config [ \"valid_subset\" ], seed = 0 ) elif train_path . endswith ( \".csv\" ): train_df = pd . read_csv ( train_path ) train_df = train_df . sample ( frac = config [ \"train_subset\" ], random_state = 0 ) valid_df = pd . read_csv ( valid_path ) valid_df = valid_df . sample ( frac = config [ \"valid_subset\" ], random_state = 0 ) train_ds = SpectrumDataset ( train_df , s2i , config [ \"n_peaks\" ], return_str = True ) valid_ds = SpectrumDataset ( valid_df , s2i , config [ \"n_peaks\" ], return_str = True ) logging . info ( f \"Data loaded: { len ( train_ds ) : , } training samples; { len ( valid_ds ) : , } validation samples\" ) train_dl = DataLoader ( train_ds , batch_size = config [ \"train_batch_size\" ], num_workers = config [ \"n_workers\" ], shuffle = True , collate_fn = collate_batch , ) valid_dl = DataLoader ( valid_ds , batch_size = config [ \"predict_batch_size\" ], num_workers = config [ \"n_workers\" ], shuffle = False , collate_fn = collate_batch , ) # Update rates based on bs=32 step_scale = 32 / config [ \"train_batch_size\" ] logging . info ( f \"Updates per epoch: { len ( train_dl ) : , } , step_scale= { step_scale } \" ) batch = next ( iter ( train_dl )) spectra , precursors , spectra_mask , peptides , peptides_mask = batch logging . info ( \"Sample batch:\" ) logging . info ( f \" - spectra.shape= { spectra . shape } \" ) logging . info ( f \" - precursors.shape= { precursors . shape } \" ) logging . info ( f \" - spectra_mask.shape= { spectra_mask . shape } \" ) logging . info ( f \" - len(peptides)= { len ( peptides ) } \" ) logging . info ( f \" - peptides_mask= { peptides_mask } \" ) # init model model = InstaNovo ( i2s = i2s , residues = config [ \"residues\" ], dim_model = config [ \"dim_model\" ], n_head = config [ \"n_head\" ], dim_feedforward = config [ \"dim_feedforward\" ], n_layers = config [ \"n_layers\" ], dropout = config [ \"dropout\" ], max_length = config [ \"max_length\" ], max_charge = config [ \"max_charge\" ], use_depthcharge = config [ \"use_depthcharge\" ], enc_type = config [ \"enc_type\" ], dec_type = config [ \"dec_type\" ], dec_precursor_sos = config [ \"dec_precursor_sos\" ], ) if model_path is not None : logging . info ( f \"Loading model checkpoint from ' { model_path } '\" ) model_state = torch . load ( model_path , map_location = \"cpu\" ) # check if PTL checkpoint if \"state_dict\" in model_state : model_state = { k . replace ( \"model.\" , \"\" ): v for k , v in model_state [ \"state_dict\" ] . items ()} k_missing = np . sum ( [ x not in list ( model_state . keys ()) for x in list ( model . state_dict () . keys ())] ) if k_missing > 0 : logging . warning ( f \"Model checkpoint is missing { k_missing } keys!\" ) k_missing = np . sum ( [ x not in list ( model . state_dict () . keys ()) for x in list ( model_state . keys ())] ) if k_missing > 0 : logging . warning ( f \"Model state is missing { k_missing } keys!\" ) model . load_state_dict ( model_state , strict = False ) logging . info ( f \"Model loaded with { np . sum ([ p . numel () for p in model . parameters ()]) : ,d } parameters\" ) logging . info ( \"Test forward pass:\" ) with torch . no_grad (): y , _ = model ( spectra , precursors , peptides , spectra_mask , peptides_mask ) logging . info ( f \" - y.shape= { y . shape } \" ) # Train on GPU device = \"cuda:0\" if torch . cuda . is_available () else \"cpu\" model = model . to ( device ) # decoder = GreedyDecoder(model, i2s, max_length=config[\"max_length\"]) decoder = BeamSearchDecoder ( model = model ) metrics = Metrics ( config [ \"residues\" ], config [ \"isotope_error_range\" ]) # init optim # assert s2i[\"PAD\"] == 0 # require PAD token to be index 0, all padding should use zeros # loss_fn = nn.CrossEntropyLoss(ignore_index=0) optim = torch . optim . Adam ( model . parameters (), lr = float ( config [ \"learning_rate\" ]), weight_decay = float ( config [ \"weight_decay\" ]), ) scheduler = WarmupScheduler ( optim , config [ \"warmup_iters\" ]) strategy = _get_strategy () ptmodel = PTModule ( config , model , decoder , metrics , sw , optim , scheduler ) if config [ \"save_model\" ]: callbacks = [ ptl . callbacks . ModelCheckpoint ( dirpath = config [ \"model_save_folder_path\" ], save_top_k =- 1 , save_weights_only = config [ \"save_weights_only\" ], every_n_train_steps = config [ \"ckpt_interval\" ], ) ] else : callbacks = None logging . info ( \"Initializing PL trainer.\" ) trainer = ptl . Trainer ( accelerator = \"auto\" , auto_select_gpus = True , callbacks = callbacks , devices = \"auto\" , logger = config [ \"logger\" ], max_epochs = config [ \"epochs\" ], num_sanity_val_steps = config [ \"num_sanity_val_steps\" ], accumulate_grad_batches = config [ \"grad_accumulation\" ], gradient_clip_val = config [ \"gradient_clip_val\" ], strategy = strategy , ) # Train the model. trainer . fit ( ptmodel , train_dl , valid_dl ) logging . info ( \"Training complete.\" )","title":"train()"},{"location":"reference/utils/","text":"","title":"Index"},{"location":"reference/utils/convert_to_ipc/","text":"convert_mgf_ipc ( source , target , max_charge = 10 , use_old_schema = False , verbose = True ) Convert .mgf file to Polars .ipc. Source code in instanovo/utils/convert_to_ipc.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def convert_mgf_ipc ( source : Path , target : Path , max_charge : int = 10 , use_old_schema : bool = False , verbose : bool = True , ) -> pl . DataFrame : \"\"\"Convert .mgf file to Polars .ipc.\"\"\" schema = { \"experiment_name\" : str , \"evidence_index\" : int , \"scan_number\" : int , \"sequence\" : str , \"modified_sequence\" : str , \"precursor_mass\" : float , \"precursor_mz\" : pl . Float64 , \"precursor_charge\" : int , \"retention_time\" : pl . Float64 , \"mz_array\" : pl . List ( pl . Float64 ), \"intensity_array\" : pl . List ( pl . Float64 ), } if use_old_schema : schema = { \"experiment_name\" : str , \"evidence_index\" : int , \"scan_number\" : int , \"Sequence\" : str , \"Modified sequence\" : str , \"Mass\" : float , \"MS/MS m/z\" : pl . Float64 , \"Charge\" : int , \"retention_time\" : pl . Float64 , \"Mass values\" : pl . List ( pl . Float64 ), \"Intensity\" : pl . List ( pl . Float64 ), } df = pl . DataFrame ( schema = schema ) if source . is_file (): filenames = [ source ] else : filenames = list ( source . iterdir ()) for filepath in filenames : if not filepath . suffix . lower () . endswith ( \"mgf\" ): logger . info ( f \"Skipping { filepath } ... Not a mgf file...\" ) continue if verbose : logger . info ( f \"Processing { filepath } ...\" ) if not filepath . is_file (): if verbose : logger . warning ( \"File not found, skipping...\" ) continue exp = load_from_mgf ( str ( filepath )) data = [] metadata = [] evidence_index = 1 scan_number = 0 for spectrum in exp : scan_number += 1 meta = spectrum . metadata peptide = \"\" unmod_peptide = \"\" if \"peptide_sequence\" in meta : peptide = meta [ \"peptide_sequence\" ] unmod_peptide = \"\" . join ([ x [ 0 ] for x in re . split ( r \"(?<=.)(?=[A-Z])\" , peptide )]) if \"charge\" not in meta or meta [ \"charge\" ] > max_charge : continue data . append ( [ source . stem , evidence_index , scan_number , unmod_peptide , peptide if not use_old_schema else f \"_ { peptide } _\" , meta [ \"pepmass\" ][ 0 ], meta [ \"precursor_mz\" ], meta [ \"charge\" ], meta [ \"retention_time\" ], list ( spectrum . mz ), list ( spectrum . intensities ), ] ) metadata . append ( { k : v for k , v in meta . items () if k not in { \"pepmass\" , \"precursor_mz\" , \"charge\" , \"retention_time\" , \"peptide_sequence\" , } } ) evidence_index += 1 data_df = pl . from_pandas ( pd . DataFrame . from_records ( metadata )) data_df = pl . concat ([ data_df , pl . DataFrame ( data , schema = schema )], how = \"horizontal\" ) df = pl . concat ([ df , data_df ], how = \"diagonal\" ) Path ( target ) . parent . mkdir ( parents = True , exist_ok = True ) df . write_ipc ( target ) return df convert_mzml_ipc ( source , target , max_charge = 10 , use_old_schema = False , verbose = True ) Convert mzml to polars ipc. Source code in instanovo/utils/convert_to_ipc.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 def convert_mzml_ipc ( source : Path , target : Path , max_charge : int = 10 , use_old_schema : bool = False , verbose : bool = True , ) -> None : \"\"\"Convert mzml to polars ipc.\"\"\" schema = { \"experiment_name\" : str , \"evidence_index\" : int , \"scan_number\" : int , \"sequence\" : str , \"modified_sequence\" : str , \"precursor_mass\" : float , \"precursor_mz\" : pl . Float64 , \"precursor_charge\" : int , \"precursor_intensity\" : pl . Float64 , \"retention_time\" : pl . Float64 , \"mz_array\" : pl . List ( pl . Float64 ), \"intensity_array\" : pl . List ( pl . Float64 ), } if use_old_schema : schema = { \"experiment_name\" : str , \"evidence_index\" : int , \"scan_number\" : int , \"Sequence\" : str , \"Modified sequence\" : str , \"Mass\" : float , \"MS/MS m/z\" : pl . Float64 , \"Charge\" : int , \"precursor_intensity\" : pl . Float64 , \"retention_time\" : pl . Float64 , \"Mass values\" : pl . List ( pl . Float64 ), \"Intensity\" : pl . List ( pl . Float64 ), } df = pl . DataFrame ( schema = schema ) if source . is_file (): filenames = [ source ] else : filenames = list ( source . iterdir ()) for filepath in filenames : if not filepath . suffix . lower () . endswith ( \"mzml\" ): logger . info ( f \"Skipping { filepath } ... Not a mzml file...\" ) continue if verbose : logger . info ( f \"Processing { filepath } ...\" ) if not filepath . is_file (): if verbose : logger . warning ( \"File not found, skipping...\" ) continue exp = pyopenms . MSExperiment () pyopenms . MzMLFile () . load ( str ( filepath ), exp ) evidence_index = 0 exp_iter = iter ( exp ) if verbose : exp_iter = tqdm ( exp_iter , total = len ( exp . getSpectra ())) data = [] for spectrum in exp_iter : if spectrum . getMSLevel () != 2 : continue mz_array , int_array = spectrum . get_peaks () precursor = spectrum . getPrecursors ()[ 0 ] if precursor . getCharge () > max_charge : continue scan_id = int ( re . findall ( r \"=(\\d+)\" , spectrum . getNativeID ())[ - 1 ]) data . append ( [ filepath . stem , evidence_index , scan_id , \"\" , \"\" if not use_old_schema else \"__\" , precursor . getUnchargedMass (), precursor . getMZ (), precursor . getCharge (), precursor . getIntensity (), spectrum . getRT (), list ( mz_array ), list ( int_array ), ] ) evidence_index += 1 df = pl . concat ([ df , pl . DataFrame ( data , schema = schema )]) Path ( target ) . parent . mkdir ( parents = True , exist_ok = True ) df . write_ipc ( target ) main () Convert data to ipc. Source code in instanovo/utils/convert_to_ipc.py 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 def main () -> None : \"\"\"Convert data to ipc.\"\"\" logging . basicConfig ( level = logging . INFO ) parser = argparse . ArgumentParser () parser . add_argument ( \"source\" , help = \"source file or folder\" ) parser . add_argument ( \"target\" , help = \"target ipc file to be saved\" ) parser . add_argument ( \"--source_type\" , default = None , choices = [ \"mgf\" , \"mzml\" , \"csv\" ], help = \"type of input data\" ) parser . add_argument ( \"--max_charge\" , default = 10 , help = \"maximum charge to filter out\" ) parser . add_argument ( \"--verbose\" , action = \"store_true\" ) parser . add_argument ( \"--use_old_schema\" , action = \"store_true\" ) args = parser . parse_args () source = Path ( args . source ) target = Path ( args . target ) source_type = args . source_type if source_type is None : # Attempt to infer type from file if source . is_dir (): raise ValueError ( f \"Cannot infer source type from a directory. Please specify with --source_type\" ) source_type = source . suffix [ 1 :] . lower () else : source_type = source_type . lower () if source_type == \"mgf\" : convert_mgf_ipc ( source , target , args . max_charge , use_old_schema = args . use_old_schema , verbose = args . verbose , ) elif source_type == \"mzml\" : convert_mzml_ipc ( source , target , args . max_charge , use_old_schema = args . use_old_schema , verbose = args . verbose , ) elif source_type == \"csv\" : df = pd . read_csv ( source ) df = pl . from_pandas ( df ) Path ( target ) . parent . mkdir ( parents = True , exist_ok = True ) df . write_ipc ( target ) else : raise ValueError ( f \"Source type { source_type } not supported.\" )","title":"Convert to ipc"},{"location":"reference/utils/convert_to_ipc/#instanovo.utils.convert_to_ipc.convert_mgf_ipc","text":"Convert .mgf file to Polars .ipc. Source code in instanovo/utils/convert_to_ipc.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def convert_mgf_ipc ( source : Path , target : Path , max_charge : int = 10 , use_old_schema : bool = False , verbose : bool = True , ) -> pl . DataFrame : \"\"\"Convert .mgf file to Polars .ipc.\"\"\" schema = { \"experiment_name\" : str , \"evidence_index\" : int , \"scan_number\" : int , \"sequence\" : str , \"modified_sequence\" : str , \"precursor_mass\" : float , \"precursor_mz\" : pl . Float64 , \"precursor_charge\" : int , \"retention_time\" : pl . Float64 , \"mz_array\" : pl . List ( pl . Float64 ), \"intensity_array\" : pl . List ( pl . Float64 ), } if use_old_schema : schema = { \"experiment_name\" : str , \"evidence_index\" : int , \"scan_number\" : int , \"Sequence\" : str , \"Modified sequence\" : str , \"Mass\" : float , \"MS/MS m/z\" : pl . Float64 , \"Charge\" : int , \"retention_time\" : pl . Float64 , \"Mass values\" : pl . List ( pl . Float64 ), \"Intensity\" : pl . List ( pl . Float64 ), } df = pl . DataFrame ( schema = schema ) if source . is_file (): filenames = [ source ] else : filenames = list ( source . iterdir ()) for filepath in filenames : if not filepath . suffix . lower () . endswith ( \"mgf\" ): logger . info ( f \"Skipping { filepath } ... Not a mgf file...\" ) continue if verbose : logger . info ( f \"Processing { filepath } ...\" ) if not filepath . is_file (): if verbose : logger . warning ( \"File not found, skipping...\" ) continue exp = load_from_mgf ( str ( filepath )) data = [] metadata = [] evidence_index = 1 scan_number = 0 for spectrum in exp : scan_number += 1 meta = spectrum . metadata peptide = \"\" unmod_peptide = \"\" if \"peptide_sequence\" in meta : peptide = meta [ \"peptide_sequence\" ] unmod_peptide = \"\" . join ([ x [ 0 ] for x in re . split ( r \"(?<=.)(?=[A-Z])\" , peptide )]) if \"charge\" not in meta or meta [ \"charge\" ] > max_charge : continue data . append ( [ source . stem , evidence_index , scan_number , unmod_peptide , peptide if not use_old_schema else f \"_ { peptide } _\" , meta [ \"pepmass\" ][ 0 ], meta [ \"precursor_mz\" ], meta [ \"charge\" ], meta [ \"retention_time\" ], list ( spectrum . mz ), list ( spectrum . intensities ), ] ) metadata . append ( { k : v for k , v in meta . items () if k not in { \"pepmass\" , \"precursor_mz\" , \"charge\" , \"retention_time\" , \"peptide_sequence\" , } } ) evidence_index += 1 data_df = pl . from_pandas ( pd . DataFrame . from_records ( metadata )) data_df = pl . concat ([ data_df , pl . DataFrame ( data , schema = schema )], how = \"horizontal\" ) df = pl . concat ([ df , data_df ], how = \"diagonal\" ) Path ( target ) . parent . mkdir ( parents = True , exist_ok = True ) df . write_ipc ( target ) return df","title":"convert_mgf_ipc()"},{"location":"reference/utils/convert_to_ipc/#instanovo.utils.convert_to_ipc.convert_mzml_ipc","text":"Convert mzml to polars ipc. Source code in instanovo/utils/convert_to_ipc.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 def convert_mzml_ipc ( source : Path , target : Path , max_charge : int = 10 , use_old_schema : bool = False , verbose : bool = True , ) -> None : \"\"\"Convert mzml to polars ipc.\"\"\" schema = { \"experiment_name\" : str , \"evidence_index\" : int , \"scan_number\" : int , \"sequence\" : str , \"modified_sequence\" : str , \"precursor_mass\" : float , \"precursor_mz\" : pl . Float64 , \"precursor_charge\" : int , \"precursor_intensity\" : pl . Float64 , \"retention_time\" : pl . Float64 , \"mz_array\" : pl . List ( pl . Float64 ), \"intensity_array\" : pl . List ( pl . Float64 ), } if use_old_schema : schema = { \"experiment_name\" : str , \"evidence_index\" : int , \"scan_number\" : int , \"Sequence\" : str , \"Modified sequence\" : str , \"Mass\" : float , \"MS/MS m/z\" : pl . Float64 , \"Charge\" : int , \"precursor_intensity\" : pl . Float64 , \"retention_time\" : pl . Float64 , \"Mass values\" : pl . List ( pl . Float64 ), \"Intensity\" : pl . List ( pl . Float64 ), } df = pl . DataFrame ( schema = schema ) if source . is_file (): filenames = [ source ] else : filenames = list ( source . iterdir ()) for filepath in filenames : if not filepath . suffix . lower () . endswith ( \"mzml\" ): logger . info ( f \"Skipping { filepath } ... Not a mzml file...\" ) continue if verbose : logger . info ( f \"Processing { filepath } ...\" ) if not filepath . is_file (): if verbose : logger . warning ( \"File not found, skipping...\" ) continue exp = pyopenms . MSExperiment () pyopenms . MzMLFile () . load ( str ( filepath ), exp ) evidence_index = 0 exp_iter = iter ( exp ) if verbose : exp_iter = tqdm ( exp_iter , total = len ( exp . getSpectra ())) data = [] for spectrum in exp_iter : if spectrum . getMSLevel () != 2 : continue mz_array , int_array = spectrum . get_peaks () precursor = spectrum . getPrecursors ()[ 0 ] if precursor . getCharge () > max_charge : continue scan_id = int ( re . findall ( r \"=(\\d+)\" , spectrum . getNativeID ())[ - 1 ]) data . append ( [ filepath . stem , evidence_index , scan_id , \"\" , \"\" if not use_old_schema else \"__\" , precursor . getUnchargedMass (), precursor . getMZ (), precursor . getCharge (), precursor . getIntensity (), spectrum . getRT (), list ( mz_array ), list ( int_array ), ] ) evidence_index += 1 df = pl . concat ([ df , pl . DataFrame ( data , schema = schema )]) Path ( target ) . parent . mkdir ( parents = True , exist_ok = True ) df . write_ipc ( target )","title":"convert_mzml_ipc()"},{"location":"reference/utils/convert_to_ipc/#instanovo.utils.convert_to_ipc.main","text":"Convert data to ipc. Source code in instanovo/utils/convert_to_ipc.py 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 def main () -> None : \"\"\"Convert data to ipc.\"\"\" logging . basicConfig ( level = logging . INFO ) parser = argparse . ArgumentParser () parser . add_argument ( \"source\" , help = \"source file or folder\" ) parser . add_argument ( \"target\" , help = \"target ipc file to be saved\" ) parser . add_argument ( \"--source_type\" , default = None , choices = [ \"mgf\" , \"mzml\" , \"csv\" ], help = \"type of input data\" ) parser . add_argument ( \"--max_charge\" , default = 10 , help = \"maximum charge to filter out\" ) parser . add_argument ( \"--verbose\" , action = \"store_true\" ) parser . add_argument ( \"--use_old_schema\" , action = \"store_true\" ) args = parser . parse_args () source = Path ( args . source ) target = Path ( args . target ) source_type = args . source_type if source_type is None : # Attempt to infer type from file if source . is_dir (): raise ValueError ( f \"Cannot infer source type from a directory. Please specify with --source_type\" ) source_type = source . suffix [ 1 :] . lower () else : source_type = source_type . lower () if source_type == \"mgf\" : convert_mgf_ipc ( source , target , args . max_charge , use_old_schema = args . use_old_schema , verbose = args . verbose , ) elif source_type == \"mzml\" : convert_mzml_ipc ( source , target , args . max_charge , use_old_schema = args . use_old_schema , verbose = args . verbose , ) elif source_type == \"csv\" : df = pd . read_csv ( source ) df = pl . from_pandas ( df ) Path ( target ) . parent . mkdir ( parents = True , exist_ok = True ) df . write_ipc ( target ) else : raise ValueError ( f \"Source type { source_type } not supported.\" )","title":"main()"},{"location":"reference/utils/metrics/","text":"Metrics ( residues , isotope_error_range , cum_mass_threshold = 0.5 , ind_mass_threshold = 0.1 ) Peptide metrics class. Source code in instanovo/utils/metrics.py 16 17 18 19 20 21 22 23 24 25 26 def __init__ ( self , residues : dict [ str , float ], isotope_error_range : list [ int ], cum_mass_threshold : float = 0.5 , ind_mass_threshold : float = 0.1 , ) -> None : self . residues = residues self . isotope_error_range = isotope_error_range self . cum_mass_threshold = cum_mass_threshold self . ind_mass_threshold = ind_mass_threshold calc_auc ( targs , preds , conf ) Calculate the peptide-level AUC. Source code in instanovo/utils/metrics.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 def calc_auc ( self , targs : list [ str ] | list [ list [ str ]], preds : list [ str ] | list [ list [ str ]], conf : list [ float ], ) -> float : \"\"\"Calculate the peptide-level AUC.\"\"\" x , y = self . _get_pr_curve ( targs , preds , conf ) recall , precision = np . array ( x )[:: - 1 ], np . array ( y )[:: - 1 ] width = recall [ 1 :] - recall [: - 1 ] height = np . minimum ( precision [ 1 :], precision [: - 1 ]) top = np . maximum ( precision [ 1 :], precision [: - 1 ]) side = top - height return ( width * height ) . sum () + 0.5 * ( side * width ) . sum () # type: ignore compute_aa_er ( peptides_truth , peptides_predicted ) Compute amino-acid level error-rate. Source code in instanovo/utils/metrics.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def compute_aa_er ( self , peptides_truth : list [ str ] | list [ list [ str ]], peptides_predicted : list [ str ] | list [ list [ str ]], ) -> float : \"\"\"Compute amino-acid level error-rate.\"\"\" # Ensure amino acids are separated peptides_truth = self . _split_sequences ( peptides_truth ) peptides_predicted = self . _split_sequences ( peptides_predicted ) return float ( jiwer . wer ( [ \" \" . join ( x ) for x in peptides_truth ], [ \" \" . join ( x ) for x in peptides_predicted ] ) ) compute_precision_recall ( targets , predictions , confidence = None , threshold = None ) Calculate precision and recall at peptide- and AA-level. Parameters: Name Type Description Default targets list [ str ] | list [ list [ str ]] Target peptides. required predictions list [ str ] | list [ list [ str ]] Model predicted peptides. required confidence list [ float ] | None Optional model confidence. None threshold float | None Optional confidence threshold. None Source code in instanovo/utils/metrics.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def compute_precision_recall ( self , targets : list [ str ] | list [ list [ str ]], predictions : list [ str ] | list [ list [ str ]], confidence : list [ float ] | None = None , threshold : float | None = None , ) -> tuple [ float , float , float , float ]: \"\"\"Calculate precision and recall at peptide- and AA-level. Args: targets (list[str] | list[list[str]]): Target peptides. predictions (list[str] | list[list[str]]): Model predicted peptides. confidence (list[float] | None): Optional model confidence. threshold (float | None): Optional confidence threshold. \"\"\" targets = self . _split_sequences ( targets ) predictions = self . _split_sequences ( predictions ) n_targ_aa , n_pred_aa , n_match_aa = 0 , 0 , 0 n_pred_pep , n_match_pep = 0 , 0 if confidence is None or threshold is None : threshold = 0 confidence = np . ones ( len ( predictions )) for i in range ( len ( targets )): targ = self . _split_peptide ( targets [ i ]) pred = self . _split_peptide ( predictions [ i ]) conf = confidence [ i ] # type: ignore if pred [ 0 ] == \"\" : pred = [] n_targ_aa += len ( targ ) if conf >= threshold and len ( pred ) > 0 : n_pred_aa += len ( pred ) n_pred_pep += 1 # pred = [x.replace('I', 'L') for x in pred] # n_match_aa += np.sum([m[0]==' ' for m in difflib.ndiff(targ,pred)]) n_match = self . _novor_match ( targ , pred ) n_match_aa += n_match if len ( pred ) == len ( targ ) and len ( targ ) == n_match : n_match_pep += 1 pep_recall = n_match_pep / len ( targets ) aa_recall = n_match_aa / n_targ_aa if n_pred_pep == 0 : pep_precision = 1.0 aa_prec = 1.0 else : pep_precision = n_match_pep / n_pred_pep aa_prec = n_match_aa / n_pred_aa return aa_prec , aa_recall , pep_recall , pep_precision matches_precursor ( seq , prec_mass , prec_charge , prec_tol = 50 ) Check if a sequence matches the precursor mass within some tolerance. Source code in instanovo/utils/metrics.py 38 39 40 41 42 43 44 45 46 47 48 49 50 def matches_precursor ( self , seq : str | list [ str ], prec_mass : float , prec_charge : int , prec_tol : int = 50 ) -> tuple [ bool , list [ float ]]: \"\"\"Check if a sequence matches the precursor mass within some tolerance.\"\"\" seq_mass = self . _mass ( seq , charge = prec_charge ) delta_mass_ppm = [ self . _calc_mass_error ( seq_mass , prec_mass , prec_charge , isotope ) for isotope in range ( self . isotope_error_range [ 0 ], self . isotope_error_range [ 1 ] + 1 , ) ] return any ( abs ( d ) < prec_tol for d in delta_mass_ppm ), delta_mass_ppm","title":"Metrics"},{"location":"reference/utils/metrics/#instanovo.utils.metrics.Metrics","text":"Peptide metrics class. Source code in instanovo/utils/metrics.py 16 17 18 19 20 21 22 23 24 25 26 def __init__ ( self , residues : dict [ str , float ], isotope_error_range : list [ int ], cum_mass_threshold : float = 0.5 , ind_mass_threshold : float = 0.1 , ) -> None : self . residues = residues self . isotope_error_range = isotope_error_range self . cum_mass_threshold = cum_mass_threshold self . ind_mass_threshold = ind_mass_threshold","title":"Metrics"},{"location":"reference/utils/metrics/#instanovo.utils.metrics.Metrics.calc_auc","text":"Calculate the peptide-level AUC. Source code in instanovo/utils/metrics.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 def calc_auc ( self , targs : list [ str ] | list [ list [ str ]], preds : list [ str ] | list [ list [ str ]], conf : list [ float ], ) -> float : \"\"\"Calculate the peptide-level AUC.\"\"\" x , y = self . _get_pr_curve ( targs , preds , conf ) recall , precision = np . array ( x )[:: - 1 ], np . array ( y )[:: - 1 ] width = recall [ 1 :] - recall [: - 1 ] height = np . minimum ( precision [ 1 :], precision [: - 1 ]) top = np . maximum ( precision [ 1 :], precision [: - 1 ]) side = top - height return ( width * height ) . sum () + 0.5 * ( side * width ) . sum () # type: ignore","title":"calc_auc()"},{"location":"reference/utils/metrics/#instanovo.utils.metrics.Metrics.compute_aa_er","text":"Compute amino-acid level error-rate. Source code in instanovo/utils/metrics.py 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def compute_aa_er ( self , peptides_truth : list [ str ] | list [ list [ str ]], peptides_predicted : list [ str ] | list [ list [ str ]], ) -> float : \"\"\"Compute amino-acid level error-rate.\"\"\" # Ensure amino acids are separated peptides_truth = self . _split_sequences ( peptides_truth ) peptides_predicted = self . _split_sequences ( peptides_predicted ) return float ( jiwer . wer ( [ \" \" . join ( x ) for x in peptides_truth ], [ \" \" . join ( x ) for x in peptides_predicted ] ) )","title":"compute_aa_er()"},{"location":"reference/utils/metrics/#instanovo.utils.metrics.Metrics.compute_precision_recall","text":"Calculate precision and recall at peptide- and AA-level. Parameters: Name Type Description Default targets list [ str ] | list [ list [ str ]] Target peptides. required predictions list [ str ] | list [ list [ str ]] Model predicted peptides. required confidence list [ float ] | None Optional model confidence. None threshold float | None Optional confidence threshold. None Source code in instanovo/utils/metrics.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def compute_precision_recall ( self , targets : list [ str ] | list [ list [ str ]], predictions : list [ str ] | list [ list [ str ]], confidence : list [ float ] | None = None , threshold : float | None = None , ) -> tuple [ float , float , float , float ]: \"\"\"Calculate precision and recall at peptide- and AA-level. Args: targets (list[str] | list[list[str]]): Target peptides. predictions (list[str] | list[list[str]]): Model predicted peptides. confidence (list[float] | None): Optional model confidence. threshold (float | None): Optional confidence threshold. \"\"\" targets = self . _split_sequences ( targets ) predictions = self . _split_sequences ( predictions ) n_targ_aa , n_pred_aa , n_match_aa = 0 , 0 , 0 n_pred_pep , n_match_pep = 0 , 0 if confidence is None or threshold is None : threshold = 0 confidence = np . ones ( len ( predictions )) for i in range ( len ( targets )): targ = self . _split_peptide ( targets [ i ]) pred = self . _split_peptide ( predictions [ i ]) conf = confidence [ i ] # type: ignore if pred [ 0 ] == \"\" : pred = [] n_targ_aa += len ( targ ) if conf >= threshold and len ( pred ) > 0 : n_pred_aa += len ( pred ) n_pred_pep += 1 # pred = [x.replace('I', 'L') for x in pred] # n_match_aa += np.sum([m[0]==' ' for m in difflib.ndiff(targ,pred)]) n_match = self . _novor_match ( targ , pred ) n_match_aa += n_match if len ( pred ) == len ( targ ) and len ( targ ) == n_match : n_match_pep += 1 pep_recall = n_match_pep / len ( targets ) aa_recall = n_match_aa / n_targ_aa if n_pred_pep == 0 : pep_precision = 1.0 aa_prec = 1.0 else : pep_precision = n_match_pep / n_pred_pep aa_prec = n_match_aa / n_pred_aa return aa_prec , aa_recall , pep_recall , pep_precision","title":"compute_precision_recall()"},{"location":"reference/utils/metrics/#instanovo.utils.metrics.Metrics.matches_precursor","text":"Check if a sequence matches the precursor mass within some tolerance. Source code in instanovo/utils/metrics.py 38 39 40 41 42 43 44 45 46 47 48 49 50 def matches_precursor ( self , seq : str | list [ str ], prec_mass : float , prec_charge : int , prec_tol : int = 50 ) -> tuple [ bool , list [ float ]]: \"\"\"Check if a sequence matches the precursor mass within some tolerance.\"\"\" seq_mass = self . _mass ( seq , charge = prec_charge ) delta_mass_ppm = [ self . _calc_mass_error ( seq_mass , prec_mass , prec_charge , isotope ) for isotope in range ( self . isotope_error_range [ 0 ], self . isotope_error_range [ 1 ] + 1 , ) ] return any ( abs ( d ) < prec_tol for d in delta_mass_ppm ), delta_mass_ppm","title":"matches_precursor()"},{"location":"reference/utils/residues/","text":"ResidueSet ( residue_masses ) A class for managing sets of residues. Source code in instanovo/utils/residues.py 11 12 13 14 15 16 17 18 19 def __init__ ( self , residue_masses : dict [ str , float ]) -> None : self . residue_masses = residue_masses self . residue_to_index = { residue : index for index , residue in enumerate ( self . residue_masses . keys ()) } self . index_to_residue = list ( self . residue_to_index . keys ()) self . tokenizer_regex = r \"(?<=.)(?=[A-Z])\" self . eos_index = self . residue_to_index [ \"$\" ] self . pad_index = self . eos_index decode ( sequence ) Map a sequence of indices to the corresponding sequence of residues. Parameters: Name Type Description Default sequence list [ int ] The sequence of residue indices. required Returns: Type Description list [ str ] list[str]: The corresponding sequence of residue strings. Source code in instanovo/utils/residues.py 78 79 80 81 82 83 84 85 86 87 def decode ( self , sequence : list [ int ]) -> list [ str ]: \"\"\"Map a sequence of indices to the corresponding sequence of residues. Args: sequence (list[int]): The sequence of residue indices. Returns: list[str]: The corresponding sequence of residue strings. \"\"\" return [ self . index_to_residue [ index ] for index in sequence ] detokenize ( sequence ) Joining a list of residues into a string representing the peptide. Parameters: Name Type Description Default sequence list [ str ] The sequence of residues. required Returns: Name Type Description str str The string representing the peptide. Source code in instanovo/utils/residues.py 46 47 48 49 50 51 52 53 54 55 56 57 def detokenize ( self , sequence : list [ str ]) -> str : \"\"\"Joining a list of residues into a string representing the peptide. Args: sequence (list[str]): The sequence of residues. Returns: str: The string representing the peptide. \"\"\" return \"\" . join ( sequence ) encode ( sequence , pad_length = None ) Map a sequence of residues to their indices and optionally pad them to a fixed length. Parameters: Name Type Description Default sequence list [ str ] The sequence of residues. required pad_length int | None An optional fixed length to pad the encoded sequence to. If this is None , no padding is done. None Returns: Type Description LongTensor torch.LongTensor: A tensor with the indices of the residues. Source code in instanovo/utils/residues.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def encode ( self , sequence : list [ str ], pad_length : int | None = None ) -> torch . LongTensor : \"\"\"Map a sequence of residues to their indices and optionally pad them to a fixed length. Args: sequence (list[str]): The sequence of residues. pad_length (int | None, optional): An optional fixed length to pad the encoded sequence to. If this is `None`, no padding is done. Returns: torch.LongTensor: A tensor with the indices of the residues. \"\"\" encoded_list = [ self . residue_to_index [ residue ] for residue in sequence ] if pad_length : encoded_list . extend (( pad_length - len ( encoded_list )) * [ self . pad_index ]) return torch . tensor ( encoded_list ) get_mass ( residue ) Get the mass of a residue. Parameters: Name Type Description Default residue str The residue whose mass to fetch. This residue must be in the residue set or this will raise a KeyError . required Returns: Name Type Description float float The mass of the residue in Daltons. Source code in instanovo/utils/residues.py 21 22 23 24 25 26 27 28 29 30 31 32 33 def get_mass ( self , residue : str ) -> float : \"\"\"Get the mass of a residue. Args: residue (str): The residue whose mass to fetch. This residue must be in the residue set or this will raise a `KeyError`. Returns: float: The mass of the residue in Daltons. \"\"\" return self . residue_masses [ residue ] tokenize ( sequence ) Split a peptide represented as a string into a list of residues. Parameters: Name Type Description Default sequence str The peptide to be split. required Returns: Type Description list [ str ] list[str]: The sequence of residues forming the peptide. Source code in instanovo/utils/residues.py 35 36 37 38 39 40 41 42 43 44 def tokenize ( self , sequence : str ) -> list [ str ]: \"\"\"Split a peptide represented as a string into a list of residues. Args: sequence (str): The peptide to be split. Returns: list[str]: The sequence of residues forming the peptide. \"\"\" return re . split ( self . tokenizer_regex , sequence )","title":"Residues"},{"location":"reference/utils/residues/#instanovo.utils.residues.ResidueSet","text":"A class for managing sets of residues. Source code in instanovo/utils/residues.py 11 12 13 14 15 16 17 18 19 def __init__ ( self , residue_masses : dict [ str , float ]) -> None : self . residue_masses = residue_masses self . residue_to_index = { residue : index for index , residue in enumerate ( self . residue_masses . keys ()) } self . index_to_residue = list ( self . residue_to_index . keys ()) self . tokenizer_regex = r \"(?<=.)(?=[A-Z])\" self . eos_index = self . residue_to_index [ \"$\" ] self . pad_index = self . eos_index","title":"ResidueSet"},{"location":"reference/utils/residues/#instanovo.utils.residues.ResidueSet.decode","text":"Map a sequence of indices to the corresponding sequence of residues. Parameters: Name Type Description Default sequence list [ int ] The sequence of residue indices. required Returns: Type Description list [ str ] list[str]: The corresponding sequence of residue strings. Source code in instanovo/utils/residues.py 78 79 80 81 82 83 84 85 86 87 def decode ( self , sequence : list [ int ]) -> list [ str ]: \"\"\"Map a sequence of indices to the corresponding sequence of residues. Args: sequence (list[int]): The sequence of residue indices. Returns: list[str]: The corresponding sequence of residue strings. \"\"\" return [ self . index_to_residue [ index ] for index in sequence ]","title":"decode()"},{"location":"reference/utils/residues/#instanovo.utils.residues.ResidueSet.detokenize","text":"Joining a list of residues into a string representing the peptide. Parameters: Name Type Description Default sequence list [ str ] The sequence of residues. required Returns: Name Type Description str str The string representing the peptide. Source code in instanovo/utils/residues.py 46 47 48 49 50 51 52 53 54 55 56 57 def detokenize ( self , sequence : list [ str ]) -> str : \"\"\"Joining a list of residues into a string representing the peptide. Args: sequence (list[str]): The sequence of residues. Returns: str: The string representing the peptide. \"\"\" return \"\" . join ( sequence )","title":"detokenize()"},{"location":"reference/utils/residues/#instanovo.utils.residues.ResidueSet.encode","text":"Map a sequence of residues to their indices and optionally pad them to a fixed length. Parameters: Name Type Description Default sequence list [ str ] The sequence of residues. required pad_length int | None An optional fixed length to pad the encoded sequence to. If this is None , no padding is done. None Returns: Type Description LongTensor torch.LongTensor: A tensor with the indices of the residues. Source code in instanovo/utils/residues.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def encode ( self , sequence : list [ str ], pad_length : int | None = None ) -> torch . LongTensor : \"\"\"Map a sequence of residues to their indices and optionally pad them to a fixed length. Args: sequence (list[str]): The sequence of residues. pad_length (int | None, optional): An optional fixed length to pad the encoded sequence to. If this is `None`, no padding is done. Returns: torch.LongTensor: A tensor with the indices of the residues. \"\"\" encoded_list = [ self . residue_to_index [ residue ] for residue in sequence ] if pad_length : encoded_list . extend (( pad_length - len ( encoded_list )) * [ self . pad_index ]) return torch . tensor ( encoded_list )","title":"encode()"},{"location":"reference/utils/residues/#instanovo.utils.residues.ResidueSet.get_mass","text":"Get the mass of a residue. Parameters: Name Type Description Default residue str The residue whose mass to fetch. This residue must be in the residue set or this will raise a KeyError . required Returns: Name Type Description float float The mass of the residue in Daltons. Source code in instanovo/utils/residues.py 21 22 23 24 25 26 27 28 29 30 31 32 33 def get_mass ( self , residue : str ) -> float : \"\"\"Get the mass of a residue. Args: residue (str): The residue whose mass to fetch. This residue must be in the residue set or this will raise a `KeyError`. Returns: float: The mass of the residue in Daltons. \"\"\" return self . residue_masses [ residue ]","title":"get_mass()"},{"location":"reference/utils/residues/#instanovo.utils.residues.ResidueSet.tokenize","text":"Split a peptide represented as a string into a list of residues. Parameters: Name Type Description Default sequence str The peptide to be split. required Returns: Type Description list [ str ] list[str]: The sequence of residues forming the peptide. Source code in instanovo/utils/residues.py 35 36 37 38 39 40 41 42 43 44 def tokenize ( self , sequence : str ) -> list [ str ]: \"\"\"Split a peptide represented as a string into a list of residues. Args: sequence (str): The peptide to be split. Returns: list[str]: The sequence of residues forming the peptide. \"\"\" return re . split ( self . tokenizer_regex , sequence )","title":"tokenize()"}]}